{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxD2kV9daRYOfOA0ZPN3PZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/model_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz79TpKNA6Ej",
        "outputId": "74d42f09-1ce8-49b4-e84f-78237e3ab9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-13 08:11:37--  https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.141.206\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.141.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895321 (874K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 874.34K   966KB/s    in 0.9s    \n",
            "\n",
            "2020-10-13 08:11:38 (966 KB/s) - ‘vocab.txt’ saved [895321/895321]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fFXvKt4Beo",
        "outputId": "b10b1c33-8eb6-4c51-803e-7a2642e96e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "\n",
        "    pairs = set(pairs)\n",
        "    return pairs\n",
        "print(get_pairs('Hôm_nay'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('n', 'a'), ('m', '_'), ('ô', 'm'), ('a', 'y'), ('_', 'n'), ('H', 'ô')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xgZ30tg4wMj",
        "outputId": "fef9017a-1d21-4e66-ac86-c96d87cf3596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('bpe.codes', encoding=\"utf-8\") as merges_handle:\n",
        "  merges = merges_handle.read().split(\"\\n\")[:-1]\n",
        "\n",
        "merges = [tuple(merge.split()[:-1]) for merge in merges]\n",
        "bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "cache = {}\n",
        "\n",
        "def bpe(token):\n",
        "  if token in cache:\n",
        "    return cache[token]\n",
        "  word = tuple(token)\n",
        "  word = tuple(list(word[:-1]) + [word[-1] + \"</w>\"])\n",
        "  pairs = get_pairs(word)\n",
        "\n",
        "  if not pairs:\n",
        "    return token\n",
        "\n",
        "  while True:\n",
        "    bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
        "    if bigram not in bpe_ranks:\n",
        "      break\n",
        "    first, second = bigram\n",
        "    new_word = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "      try:\n",
        "        j = word.index(first, i)\n",
        "      except ValueError:\n",
        "        new_word.extend(word[i:])\n",
        "        break\n",
        "      else:\n",
        "        new_word.extend(word[i:j])\n",
        "        i = j\n",
        "\n",
        "      if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "        new_word.append(first + second)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_word.append(word[i])\n",
        "        i += 1\n",
        "    new_word = tuple(new_word)\n",
        "    word = new_word\n",
        "    if len(word) == 1:\n",
        "      break\n",
        "    else:\n",
        "      pairs = get_pairs(word)\n",
        "  word = \"@@ \".join(word)\n",
        "  word = word[:-4]\n",
        "  cache[token] = word\n",
        "  return word\n",
        "\n",
        "print(bpe('Viblo'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vi@@ b@@ lo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWel1NUh7bxQ",
        "outputId": "7fd5fdd6-e8c2-4ee6-a7c1-fb7c129c478f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "def _tokenize(text):\n",
        "    \"\"\"Tokenize a string.\"\"\"\n",
        "    split_tokens = []\n",
        "\n",
        "    words = re.findall(r\"\\S+\\n?\", text)\n",
        "\n",
        "    for token in words:\n",
        "        split_tokens.extend([t for t in bpe(token).split(\" \")])\n",
        "    return split_tokens\n",
        "\n",
        "print(_tokenize('Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hôm_nay', 'trời', 'nóng', 'quá', 'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6acVhfz9AZrF"
      },
      "source": [
        "encoder = {}\n",
        "bos_token=\"<s>\"\n",
        "eos_token=\"</s>\"\n",
        "sep_token=\"</s>\"\n",
        "cls_token=\"<s>\"\n",
        "unk_token=\"<unk>\"\n",
        "pad_token=\"<pad>\"\n",
        "mask_token=\"<mask>\"\n",
        "\n",
        "\n",
        "encoder[bos_token] = 0\n",
        "encoder[pad_token] = 1\n",
        "encoder[eos_token] = 2\n",
        "encoder[unk_token] = 3\n",
        "\n",
        "def add_from_file(f):\n",
        "  if isinstance(f, str):\n",
        "      try:\n",
        "          with open(f, \"r\", encoding=\"utf-8\") as fd:\n",
        "              add_from_file(fd)\n",
        "      except FileNotFoundError as fnfe:\n",
        "          raise fnfe\n",
        "      except UnicodeError:\n",
        "          raise Exception(\"Incorrect encoding detected in {}, please \" \"rebuild the dataset\".format(f))\n",
        "      return\n",
        "\n",
        "  lines = f.readlines()\n",
        "  for lineTmp in lines:\n",
        "      line = lineTmp.strip()\n",
        "      idx = line.rfind(\" \")\n",
        "      if idx == -1:\n",
        "          raise ValueError(\"Incorrect dictionary format, expected '<token> <cnt>'\")\n",
        "      word = line[:idx]\n",
        "      encoder[word] = len(encoder)\n",
        "\n",
        "add_from_file('vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcz9F29_Sgz",
        "outputId": "01213f3f-7b26-448a-c224-615aa34ab9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def _convert_token_to_id(token):\n",
        "    \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "    return encoder.get(token, encoder.get(unk_token))\n",
        "print(_convert_token_to_id('Hôm_nay'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59YJ4MJF3xu",
        "outputId": "cc5b51c1-a994-4691-8924-4bbd31f36368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = {v: k for k, v in encoder.items()}\n",
        "def _convert_id_to_token(index):\n",
        "    return decoder.get(index, unk_token)\n",
        "\n",
        "print(_convert_id_to_token(7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "của\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSccuIU7rn3",
        "outputId": "c47204ad-4001-4c85-9cfc-4a07bb27f884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def convert_tokens_to_string(tokens):\n",
        "    out_string = \" \".join(tokens)\n",
        "    out_string = out_string.replace(\"@@ \", \"\").strip()\n",
        "    out_string = out_string.replace('@@','').strip()\n",
        "    return out_string\n",
        "  \n",
        "print(convert_tokens_to_string(['Hôm_nay', 'trời', 'nóng', 'quá' ,'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGKL3T4d0l8J"
      },
      "source": [
        "# Requirement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZTb_7oezVdQ",
        "outputId": "12f48d71-7d95-40e6-e785-1337645ca936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install pandas\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av0uUbDt0tmE",
        "outputId": "95898b13-6b00-4635-978e-e46dc0d6acc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! pip install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52mxnz7V4nMG",
        "outputId": "66185570-fdfc-41d2-e321-f09ee10b11c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "! pip3 install vncorenlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645934 sha256=59ac30fc83307e1ffbe03f25630bc6d7eba6ac0a4e507fe2ebf92169d669acab\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niPCrVSQ4xWY"
      },
      "source": [
        "# ! rm -rf "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhN1qnIx08Og",
        "outputId": "5166ebbe-ad59-455d-e436-1ebe0bb916ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "! mkdir -p vncorenlp/models/wordsegmenter\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "! mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "! mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "! mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-15 01:38:40--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   108MB/s    in 0.2s    \n",
            "\n",
            "2020-10-15 01:38:41 (108 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2020-10-15 01:38:41--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-10-15 01:38:41 (5.94 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2020-10-15 01:38:41--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-15 01:38:42 (2.91 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjjCGJ5I1Y41"
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "import re\n",
        "\n",
        "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "def document_2_list_token(documents):\n",
        "  vocab = {}\n",
        "  for document in documents:\n",
        "    tokenizer = rdrsegmenter.tokenize(document)\n",
        "    for sentence in tokenizer:\n",
        "      for token in sentence:\n",
        "        token = token.lower()\n",
        "        if not re.search('[a-zA-Z]',token):\n",
        "          continue\n",
        "        if token != '':\n",
        "          if token not in vocab:\n",
        "            vocab[token] = 1\n",
        "          else:\n",
        "            vocab[token] += 1\n",
        "  return vocab\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iy2De0536h3"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cbSUDkxzZdM",
        "outputId": "40aba8ae-663a-4cfd-889e-73cecbaecc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from itertools import islice\n",
        "\n",
        "dfs = pd.read_excel('data_classifier.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "data_land = list(dfs['land'])\n",
        "data_house = list(dfs['house'])\n",
        "\n",
        "for index1 in range(len(data_land)):\n",
        "  if isinstance(data_land[index1],float):\n",
        "    break\n",
        "for index2 in range(len(data_house)):\n",
        "  if isinstance(data_house[index2],float):\n",
        "    break\n",
        "\n",
        "data_land = data_land[:index1]\n",
        "count_feature_land = document_2_list_token(data_land)\n",
        "count_feature_land = {k: v for k, v in sorted(count_feature_land.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "data_house = data_house[:index2]\n",
        "count_feature_house = document_2_list_token(data_house)\n",
        "count_feature_house = {k: v for k, v in sorted(count_feature_house.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "print(take(20,count_feature_land.items()))\n",
        "print(take(20,count_feature_house.items()))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('có', 132), ('đất', 129), ('nhà', 56), ('cách', 49), ('đường', 46), ('bán', 44), ('hà_nội', 33), ('giá', 33), ('rất', 33), ('liên_hệ', 31), ('cấp', 30), ('trong', 30), ('và', 28), ('lô', 27), ('lại', 27), ('là', 27), ('khu', 26), ('sổ_đỏ', 26), ('nhà_đất', 24), ('nhu_cầu', 24)]\n",
            "[('nhà', 246), ('tầng', 212), ('phòng', 199), ('có', 95), ('gần', 80), ('giá', 78), ('ngủ', 78), ('và', 77), ('tỷ', 77), ('chính', 75), ('cách', 75), ('bán', 74), ('sổ_đỏ', 73), ('bếp', 68), ('chủ', 67), ('khu', 60), ('khách', 58), ('sân', 56), ('đường', 56), ('xây', 51)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFERuDc77qfk"
      },
      "source": [
        "vocabulary_land = ['đất','nhà','lô','nhà_đất','vuông_vắn','vườn','mảnh','xưởng','kho']\n",
        "vocabulary_house = ['tầng','phòng','ngủ','bếp','khách','sân','cửa','thiết_kế','phơi','thờ','nội_thất','wc','hướng',',mặt_tiền','vệ_sinh']"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBRMeH-9GJK3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = []\n",
        "  document = rdrsegmenter.tokenize(text)\n",
        "  for senten in document:\n",
        "    for token in senten:\n",
        "      tokens.append(token.lower())\n",
        "  return tokens\n",
        "\n",
        "def transform_document_to_vec(text):\n",
        "  vocab = vocabulary_land\n",
        "  for item in vocabulary_house:\n",
        "    if item not in vocab:\n",
        "      vocab.append(item)\n",
        "  \n",
        "  text = text.lower()\n",
        "  vectorizer = CountVectorizer(tokenizer=tokenize,vocabulary=vocab,binary=True)\n",
        "  # print(vectorizer.build_tokenizer())\n",
        "  # 1/0\n",
        "  X = vectorizer.fit_transform([text]).toarray()\n",
        "  return X\n",
        "\n",
        "\n",
        "def convert_labels(y, C = 2):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y.T\n",
        "\n",
        "# a = transform_document_to_vec('Bán nhà 40m2x 3 tầng mới đường oto đỗ cửa giá 1.38 tỷ có TL ... Hướng : Đông Nam ... Diện tích : 40m2 nở hậu ... Vị trí : Nhà có vị trí cực thuận tiện gần trường học các cấp , công an phường , quân đội , nhà văn hóa . Nhà gần bãi gửi xe ngày đêm , đường trước nhà rộng 3 cực thoáng trong dãy phân lô cao tầng ... + Nhà gần trục đường 6 tuyến đi Hà Đông , Ngã Tư Sở , Mỹ Đình hay tuyến Chương Mỹ , Hòa Bình ... + Nhà gần nhiều dự án trọng đểm của quận Hà Đông là khu đang phát triển rất mạnh .. ... + Khu dân cư đông , an ninh tốt , hàng xóm thân thiện .. ... + Nhà gần điểm xe bus cách đường quốc lộ 6 chỉ 500m ... + Với tài chính vừa phải bạn sở hữu căn nhà lý tưởng ... Thiết kế : Theo kiến trúc tân cổ điển , rộng , thoáng , tầng 2 phòng ngủ ... + Tâng 1 : Phòng khách , để xe , bếp ăn riêng , có sân sau ... + Tầng 2 : 2 phòng ngủ rộng , 1 wc ... + Tầng 3 : 1 phòng ngủ , 1 phòng thờ , sân phơi rộng ... + Nhà có sân trước 3m , sân sau 1,5m các phòng đều có cửa sổ rộng , không khí lưu thông ... Nội thất : Hoàn thiện đầy đủ nội thất cơ bản , nhận nhà ngay ... Pháp lý : Giấy phép xây dựng riêng ... Sổ đỏ chính chủ ... Giá : 1.38 tỷ làm việc trực tiếp , bao sang tên ... Liên hệ : Tuấn Anh 0974322298 đi xem miễn phí ... Hỗ trợ thủ tục vay vốn ngân hàng 70% giá trị với lãi xuất thấp , thủ tục nhanh ... Thông tin pháp lý : Sổ đỏ chính chủ.')\n",
        "# print(a.shape)\n",
        "Feature_1 = []\n",
        "label = []\n",
        "for document in data_house:\n",
        "  Feature_1.extend(transform_document_to_vec(document))\n",
        "  label.append(0)\n",
        "for document in data_land:\n",
        "  Feature_1.extend(transform_document_to_vec(document))\n",
        "  label.append(1)\n",
        "Feature_1 = np.asarray(Feature_1)\n",
        "Label     = np.asarray(label)\n",
        "\n",
        "Feature1_train, Feature1_test, y_train, y_test = train_test_split(\n",
        "     Feature_1, Label, test_size=0.33, random_state=42)\n",
        "y_train =  convert_labels(y_train)\n",
        "y_test  =  convert_labels(y_test)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sUkwYqEHm0",
        "outputId": "573cfb70-8ed1-4e72-e9b9-095331108460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGxmYPvHQee",
        "outputId": "736090e4-cdcc-4391-abc2-bc10c5c90b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Feature1_train.shape)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktjm5dwVTF7r"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Parameter\n",
        "learning_rate = 0.001\n",
        "epoch = 100\n",
        "batch_size = 128\n",
        "display_step =5\n",
        "\n",
        "#Network Parameters\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 64\n",
        "dimention_1 = 24 # dimention of vocabulary\n",
        "dimention_2 = 8 # dimention of entities\n",
        "connect_dimention = 16 # dimention connect\n",
        "num_class = 2\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "X_1 = tf.placeholder('float',[None,dimention_1],name='X_1')\n",
        "X_2 = tf.placeholder('float',[None,dimention_2],name='X_2')\n",
        "Y   = tf.placeholder('int64',[None,num_class],name='Y')"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vzDYHFE4HON"
      },
      "source": [
        "# Store layers weight and bias\n",
        "\n",
        "weights = {\n",
        "    'concat_1': tf.Variable(tf.random_normal([dimention_1,connect_dimention]),name='concat_1'),\n",
        "    'concat_2': tf.Variable(tf.random_normal([dimention_2,connect_dimention]),name='concat_2'),\n",
        "    'h_1'     : tf.Variable(tf.random_normal([connect_dimention,n_hidden_1]),name='h_1'),\n",
        "    'h_2'     : tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2]),name='h_2'),\n",
        "    'out'     : tf.Variable(tf.random_normal([n_hidden_2,num_class]),name='out')\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b_1' : tf.Variable(tf.random_normal([n_hidden_1]),name='b_1'),\n",
        "    'b_2' : tf.Variable(tf.random_normal([n_hidden_2]),name='b_2'),\n",
        "    'out' : tf.Variable(tf.random_normal([num_class]),name='out')\n",
        "}"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okTxSF0s-JRE"
      },
      "source": [
        "# Create model\n",
        "def neural_net(x1,x2):\n",
        "  # Concat Feature\n",
        "  concat  = tf.add(tf.matmul(x1,weights['concat_1']),tf.matmul(x2,weights['concat_2']))\n",
        "  # hidden layer 1\n",
        "  layer_1 = tf.add(tf.matmul(concat,weights['h_1']),biases['b_1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  # hidden layer 2\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h_2']),biases['b_2'])\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  # out layer\n",
        "  out_layer = tf.matmul(layer_2,weights['out'])\n",
        "  out_layer = tf.nn.bias_add(out_layer,biases['out'])\n",
        "  # probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "  return out_layer"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TSyMOgp_v-i"
      },
      "source": [
        "logit = neural_net(X_1, X_2)\n",
        "\n",
        "# predict = tf.argmax(logit, axis=-1, output_type=tf.int32)\n",
        "# log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "# one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "# per_example_loss = -tf.reduce_sum(Y * log_probs, axis=-1)\n",
        "# loss_op = tf.reduce_mean(per_example_loss)\n",
        "# with tf.variable_scope(\"loss\"):\n",
        "    # I.e., 0.1 dropout\n",
        "    # output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    # logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    # logits = tf.nn.bias_add(logits, output_bias)\n",
        "# probabilities = tf.nn.softmax(logit, axis=-1)\n",
        "# log_probs = tf.nn.log_softmax(logit, axis=-1)\n",
        "\n",
        "    # one_hot_labels = tf.one_hot(Y, depth=num_class, dtype=tf.float32)\n",
        "\n",
        "# per_example_loss = -tf.reduce_sum(Y * log_probs, axis=-1)\n",
        "# loss_op = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "\n",
        "# total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n",
        "# loss \n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits = logit, labels = Y))"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9qDsefjAuLC",
        "outputId": "646404d8-3339-4736-cea3-edf3c19e11da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "def training(op):\n",
        "  if op == 'Adam':\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  elif op == 'RMSprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "  elif op == 'Momentum':\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "  predict = tf.argmax(logit, 1)\n",
        "  correct_pred = tf.equal(tf.argmax(logit, 1),tf.argmax(Y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pred , tf.float32))\n",
        "  # TP = tf.count_nonzero(tf.matmul(predict , Y))\n",
        "  # TN = tf.count_nonzero((predict - 1) * (Y - 1))\n",
        "  # FP = tf.count_nonzero(predict * (Y - 1))\n",
        "  # FN = tf.count_nonzero((predict - 1) * Y)\n",
        "  # precision = TP / (TP + FP)\n",
        "  # recall = TP / (TP + FN)\n",
        "  # f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  loss_list = np.zeros(epoch)\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for step in range(epoch):\n",
        "      batch_x, batch_y = Feature1_train , y_train\n",
        "      sess.run(train_op, feed_dict={X_1: batch_x,\n",
        "                                    X_2:np.zeros([batch_x.shape[0],dimention_2]),\n",
        "                                    Y :batch_y})\n",
        "      loss , acc , pred,log = sess.run([loss_op, accuracy,predict,logit],feed_dict={X_1: batch_x,\n",
        "                                                           X_2: np.zeros([batch_x.shape[0],dimention_2]),\n",
        "                                                                              Y : batch_y})\n",
        "      # prop = sess.run([probabilities],feed_dict ={X_1: batch_x,\n",
        "                                                  # X_2: np.zeros([batch_x.shape[0],dimention_2])} )\n",
        "      loss_list[step] = loss\n",
        "      if step % display_step == 0 or step == 1:\n",
        "        print('Step '+str(step)+' batch loss = '+ '{:.4f}'.format(loss) + ' trainning acc ='+\n",
        "              '{:.3f}'.format(acc))\n",
        "        save_path = saver.save(sess, \"/content/model/model\" + str(step)+ \".ckpt\")\n",
        "        print(\"Model saved in path: %s\" % save_path)\n",
        "        batch_y_true = np.argmax(batch_y, 1)\n",
        "        print(batch_y_true.shape)\n",
        "        print(f1_score(batch_y_true, pred, average='macro'))\n",
        "        print(pred.shape)\n",
        "    print('Testing Acc:',sess.run(accuracy,feed_dict={X_1: Feature1_test,\n",
        "                                                      X_2:np.zeros([Feature1_test.shape[0],dimention_2]),\n",
        "                                                      Y  :y_test}))\n",
        "    test_result = sess.run(predict,feed_dict={X_1: Feature1_test,\n",
        "                                              X_2:np.zeros([Feature1_test.shape[0],dimention_2]),\n",
        "                                              Y  :y_test})\n",
        "    test_true = np.argmax(y_test, 1)\n",
        "    print(f1_score(test_true,test_result, average='macro'))\n",
        "    print(test_result)\n",
        "    print(test_true)\n",
        "  return loss_list\n",
        "\n",
        "adam = training('Adam') "
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 batch loss = 54.0846 trainning acc =0.718\n",
            "Model saved in path: /content/model/model0.ckpt\n",
            "(78,)\n",
            "0.6689814814814815\n",
            "(78,)\n",
            "Step 1 batch loss = 42.6935 trainning acc =0.756\n",
            "Model saved in path: /content/model/model1.ckpt\n",
            "(78,)\n",
            "0.723249299719888\n",
            "(78,)\n",
            "Step 5 batch loss = 16.2686 trainning acc =0.885\n",
            "Model saved in path: /content/model/model5.ckpt\n",
            "(78,)\n",
            "0.8813186813186813\n",
            "(78,)\n",
            "Step 10 batch loss = 8.9914 trainning acc =0.910\n",
            "Model saved in path: /content/model/model10.ckpt\n",
            "(78,)\n",
            "0.9084353513332215\n",
            "(78,)\n",
            "Step 15 batch loss = 2.7981 trainning acc =0.974\n",
            "Model saved in path: /content/model/model15.ckpt\n",
            "(78,)\n",
            "0.9739304812834224\n",
            "(78,)\n",
            "Step 20 batch loss = 1.0700 trainning acc =0.987\n",
            "Model saved in path: /content/model/model20.ckpt\n",
            "(78,)\n",
            "0.9869193359047459\n",
            "(78,)\n",
            "Step 25 batch loss = 0.0028 trainning acc =1.000\n",
            "Model saved in path: /content/model/model25.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 30 batch loss = 0.0679 trainning acc =0.974\n",
            "Model saved in path: /content/model/model30.ckpt\n",
            "(78,)\n",
            "0.9735054347826086\n",
            "(78,)\n",
            "Step 35 batch loss = 0.0009 trainning acc =1.000\n",
            "Model saved in path: /content/model/model35.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 40 batch loss = 0.0001 trainning acc =1.000\n",
            "Model saved in path: /content/model/model40.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 45 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model45.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 50 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model50.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 55 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model55.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 60 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model60.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 65 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model65.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 70 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model70.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 75 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model75.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 80 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model80.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 85 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model85.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 90 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model90.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Step 95 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model95.ckpt\n",
            "(78,)\n",
            "1.0\n",
            "(78,)\n",
            "Testing Acc: 0.8717949\n",
            "0.8583877995642701\n",
            "[0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0]\n",
            "[0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB27K6cz-bIx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYJsj6EgjCWh",
        "outputId": "23ebc81f-0f94-4c15-ab19-bd22edd1b53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "tf.reset_default_graph()\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 64\n",
        "dimention_1 = 24 # dimention of vocabulary\n",
        "dimention_2 = 8 # dimention of entities\n",
        "connect_dimention = 16 # dimention connect\n",
        "num_class = 2\n",
        "\n",
        "\n",
        "  # tf Graph input\n",
        "X_11 = tf.placeholder('float',[None,dimention_1],name='X_11')\n",
        "X_21 = tf.placeholder('float',[None,dimention_2],name='X_21')\n",
        "Y1   = tf.placeholder('float',[None,num_class],name='Y1')\n",
        "\n",
        "weights = {\n",
        "    'concat_1': tf.Variable(tf.random_normal([dimention_1,connect_dimention]),name='concat_1'),\n",
        "    'concat_2': tf.Variable(tf.random_normal([dimention_2,connect_dimention]),name='concat_2'),\n",
        "    'h_1'     : tf.Variable(tf.random_normal([connect_dimention,n_hidden_1]),name='h_1'),\n",
        "    'h_2'     : tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2]),name='h_2'),\n",
        "    'out'     : tf.Variable(tf.random_normal([n_hidden_2,num_class]),name='out')\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b_1' : tf.Variable(tf.random_normal([n_hidden_1]),name='b_1'),\n",
        "    'b_2' : tf.Variable(tf.random_normal([n_hidden_2]),name='b_2'),\n",
        "    'out' : tf.Variable(tf.random_normal([num_class]),name='out')\n",
        "}\n",
        "def neural_net(x1,x2):\n",
        "  # Concat Feature\n",
        "  concat  = tf.add(tf.matmul(x1,weights['concat_1']),tf.matmul(x2,weights['concat_2']))\n",
        "  # hidden layer 1\n",
        "  layer_1 = tf.add(tf.matmul(concat,weights['h_1']),biases['b_1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  # hidden layer 2\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h_2']),biases['b_2'])\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  # out layer\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weights['out']),biases['out'])\n",
        "  return out_layer\n",
        "\n",
        "def predict_raw_text(text):\n",
        "  # graph_meta = tf.train.import_meta_graph('/content/model/model95.ckpt.meta')\n",
        "  logits = neural_net(X_11, X_21)\n",
        "  probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "  predict = tf.argmax(logits, 1)\n",
        "  vector = transform_document_to_vec(text)\n",
        "  with tf.Session() as sess:\n",
        "    # graph_meta.restore(sess , tf.train.latest_checkpoint('/content/model'))\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, '/content/model/model95.ckpt')\n",
        "    pred , prop ,log = sess.run([predict,probabilities,logits],feed_dict={X_11: vector,\n",
        "                                       X_21: np.zeros([vector.shape[0],dimention_2])})\n",
        "    print(log)\n",
        "  return pred[0]\n",
        "\n",
        "# print(predict_raw_text('Bán gấp nhà Văn Điển 5 tầng ô tô đỗ gần chỉ 2,7 tỷ ... + Nhà mới đẹp Văn Điển ở ngay , ô tô đỗ gần , công năng đầy đủ cho 1 hộ gia đình ở : 1 khách , bếp , 2 ngủ , 3wc , phòng thờ , sân phơi . Có thể lên thêm 2 tầng thoải mái ... + Nhà Văn Điển gần rất nhiều tiện ích : chợ , trung tâm thương mại , bệnh viện , trung tâm thể dục thể thao , huyện ủy thanh trì ... + Khu vực Thanh Trì sắp lên Quận giá trị bất động sản ngày một tang , khách mua năm sbawts tình hình mua đợt này dịch giá có bớt cho ai thiện chí ... + Khu vực Thanh trì lên trung tâm thành phố rất gần chỉ 15 phút , đi các quận rất thuận tiện , về quê qua bến xe nước ngầm chỉ 2 phút ... + Nhà đẹp sổ vuông nở hậu ... + Liên hệ Mrs Thúy : 0965 - 2535 - 83'))\n",
        "# 1/0\n",
        "import json\n",
        "with open('/content/result4700.json','r',encoding='utf-8') as file:\n",
        "  data = json.load(file)\n",
        "result = []\n",
        "for index in range(1000,1100):\n",
        "  text = data[index]['text']\n",
        "  result.append(predict_raw_text(text))\n",
        "\n",
        "print(result)\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 911.8693  -348.10632]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-782.77136 -919.6132 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -397.48618 -1145.1324 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 383.4993  -234.26576]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-132.71846 -574.9512 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -87.02093 -163.83817]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 214.82092 -624.1751 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  91.65172 -151.45755]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-152.24344 -884.0349 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -82.16578 -264.1356 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-653.37134 -974.3844 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -82.16578 -264.1356 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  47.194595 -484.6993  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  -14.901515 -1256.7627  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 416.87466 -396.94388]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -397.48618 -1145.1324 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -36.530968 -402.73788 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-474.6422 -487.9503]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 166.43306 -674.9104 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -308.47403 -1050.441  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 409.5629  -124.42804]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -632.94824 -1038.0663 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-132.68398 -433.56406]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-162.769   -399.95135]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-151.3811  -265.25595]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-102.66214 -267.6214 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-782.77136 -919.6132 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-132.68398 -433.56406]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -129.55112 -1174.1248 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-595.00433 -721.6591 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-218.91203 -476.91013]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-132.68398 -433.56406]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -98.59869 -167.21872]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-451.54816 -160.74857]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 384.82242 -336.1652 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-460.51584 -941.1069 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 384.82242 -336.1652 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -397.48618 -1145.1324 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-653.37134 -974.3844 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-162.769   -399.95135]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-460.51584 -941.1069 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[309.74072  -17.908714]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 148.38844 -469.1348 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 186.539   -368.20404]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  25.13604 -675.76044]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-460.51584 -941.1069 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-292.22803 -397.47028]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  90.59795 -181.7743 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 384.82242 -336.1652 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-292.22803 -397.47028]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-699.6592 -650.5006]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -27.270498 -467.03726 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-782.77136 -919.6132 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-132.68398 -433.56406]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 141.19841 -388.992  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 474.31573 -265.43912]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-538.5847 -419.28  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-484.24408 -848.16565]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-292.22803 -397.47028]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-336.48752 -314.70703]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-460.51584 -941.1069 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-111.67886 -183.32869]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 911.8693  -348.10632]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 177.61852 -485.053  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-232.28963 -658.25684]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  24.96134 -216.11835]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[   0.50910944 -441.72452   ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 195.1216 -586.8568]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-251.57037 -277.52438]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-339.34622 -114.5324 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 141.19841 -388.992  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-460.51584 -941.1069 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-799.2082  -254.61713]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 905.69556 -421.83884]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-611.4203 -414.6276]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-302.21875 -228.63855]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 733.4612 -253.1302]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-220.66515   -44.446075]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 141.19841 -388.992  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[  25.13604 -675.76044]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -306.14825 -1049.2203 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -409.96323 -1103.737  ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-102.66214 -267.6214 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-213.34012 -274.26392]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ 455.4228  -207.43254]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-200.7448 -644.1116]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-674.64874 -561.4302 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-273.79807 -327.20053]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-182.08305 -634.9853 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-694.3095     20.730408]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -200.88498 -1020.4452 ]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-261.2362  -462.65402]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[-328.62192 -610.38586]]\n",
            "INFO:tensorflow:Restoring parameters from /content/model/model95.ckpt\n",
            "[[ -98.485794 -523.6679  ]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYodTGT5VbkZ",
        "outputId": "026524a1-1b64-425c-dbf0-354c175f7ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for item in result:\n",
        "  if item == 1 :\n",
        "    print(result.index(item))\n",
        "    break"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmcbp_Rl-aly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}