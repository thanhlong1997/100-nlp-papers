{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrP7b9kuw9Eh5r3fiP/DIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/phobert_inside.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz79TpKNA6Ej",
        "outputId": "74d42f09-1ce8-49b4-e84f-78237e3ab9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-13 08:11:37--  https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.141.206\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.141.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895321 (874K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 874.34K   966KB/s    in 0.9s    \n",
            "\n",
            "2020-10-13 08:11:38 (966 KB/s) - ‘vocab.txt’ saved [895321/895321]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fFXvKt4Beo",
        "outputId": "b10b1c33-8eb6-4c51-803e-7a2642e96e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "\n",
        "    pairs = set(pairs)\n",
        "    return pairs\n",
        "print(get_pairs('Hôm_nay'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('n', 'a'), ('m', '_'), ('ô', 'm'), ('a', 'y'), ('_', 'n'), ('H', 'ô')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xgZ30tg4wMj",
        "outputId": "fef9017a-1d21-4e66-ac86-c96d87cf3596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('bpe.codes', encoding=\"utf-8\") as merges_handle:\n",
        "  merges = merges_handle.read().split(\"\\n\")[:-1]\n",
        "\n",
        "merges = [tuple(merge.split()[:-1]) for merge in merges]\n",
        "bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "cache = {}\n",
        "\n",
        "def bpe(token):\n",
        "  if token in cache:\n",
        "    return cache[token]\n",
        "  word = tuple(token)\n",
        "  word = tuple(list(word[:-1]) + [word[-1] + \"</w>\"])\n",
        "  pairs = get_pairs(word)\n",
        "\n",
        "  if not pairs:\n",
        "    return token\n",
        "\n",
        "  while True:\n",
        "    bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
        "    if bigram not in bpe_ranks:\n",
        "      break\n",
        "    first, second = bigram\n",
        "    new_word = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "      try:\n",
        "        j = word.index(first, i)\n",
        "      except ValueError:\n",
        "        new_word.extend(word[i:])\n",
        "        break\n",
        "      else:\n",
        "        new_word.extend(word[i:j])\n",
        "        i = j\n",
        "\n",
        "      if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "        new_word.append(first + second)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_word.append(word[i])\n",
        "        i += 1\n",
        "    new_word = tuple(new_word)\n",
        "    word = new_word\n",
        "    if len(word) == 1:\n",
        "      break\n",
        "    else:\n",
        "      pairs = get_pairs(word)\n",
        "  word = \"@@ \".join(word)\n",
        "  word = word[:-4]\n",
        "  cache[token] = word\n",
        "  return word\n",
        "\n",
        "print(bpe('Viblo'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vi@@ b@@ lo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWel1NUh7bxQ",
        "outputId": "7fd5fdd6-e8c2-4ee6-a7c1-fb7c129c478f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "def _tokenize(text):\n",
        "    \"\"\"Tokenize a string.\"\"\"\n",
        "    split_tokens = []\n",
        "\n",
        "    words = re.findall(r\"\\S+\\n?\", text)\n",
        "\n",
        "    for token in words:\n",
        "        split_tokens.extend([t for t in bpe(token).split(\" \")])\n",
        "    return split_tokens\n",
        "\n",
        "print(_tokenize('Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hôm_nay', 'trời', 'nóng', 'quá', 'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6acVhfz9AZrF"
      },
      "source": [
        "encoder = {}\n",
        "bos_token=\"<s>\"\n",
        "eos_token=\"</s>\"\n",
        "sep_token=\"</s>\"\n",
        "cls_token=\"<s>\"\n",
        "unk_token=\"<unk>\"\n",
        "pad_token=\"<pad>\"\n",
        "mask_token=\"<mask>\"\n",
        "\n",
        "\n",
        "encoder[bos_token] = 0\n",
        "encoder[pad_token] = 1\n",
        "encoder[eos_token] = 2\n",
        "encoder[unk_token] = 3\n",
        "\n",
        "def add_from_file(f):\n",
        "  if isinstance(f, str):\n",
        "      try:\n",
        "          with open(f, \"r\", encoding=\"utf-8\") as fd:\n",
        "              add_from_file(fd)\n",
        "      except FileNotFoundError as fnfe:\n",
        "          raise fnfe\n",
        "      except UnicodeError:\n",
        "          raise Exception(\"Incorrect encoding detected in {}, please \" \"rebuild the dataset\".format(f))\n",
        "      return\n",
        "\n",
        "  lines = f.readlines()\n",
        "  for lineTmp in lines:\n",
        "      line = lineTmp.strip()\n",
        "      idx = line.rfind(\" \")\n",
        "      if idx == -1:\n",
        "          raise ValueError(\"Incorrect dictionary format, expected '<token> <cnt>'\")\n",
        "      word = line[:idx]\n",
        "      encoder[word] = len(encoder)\n",
        "\n",
        "add_from_file('vocab.txt')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcz9F29_Sgz",
        "outputId": "01213f3f-7b26-448a-c224-615aa34ab9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def _convert_token_to_id(token):\n",
        "    \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "    return encoder.get(token, encoder.get(unk_token))\n",
        "print(_convert_token_to_id('Hôm_nay'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59YJ4MJF3xu",
        "outputId": "cc5b51c1-a994-4691-8924-4bbd31f36368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = {v: k for k, v in encoder.items()}\n",
        "def _convert_id_to_token(index):\n",
        "    return decoder.get(index, unk_token)\n",
        "\n",
        "print(_convert_id_to_token(7))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "của\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSccuIU7rn3",
        "outputId": "c47204ad-4001-4c85-9cfc-4a07bb27f884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def convert_tokens_to_string(tokens):\n",
        "    out_string = \" \".join(tokens)\n",
        "    out_string = out_string.replace(\"@@ \", \"\").strip()\n",
        "    out_string = out_string.replace('@@','').strip()\n",
        "    return out_string\n",
        "  \n",
        "print(convert_tokens_to_string(['Hôm_nay', 'trời', 'nóng', 'quá' ,'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@']))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}