{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_classifer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIstT67ANKe8fIY6oHtxlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/lstm_classifer_v27-10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGtF0TRJgmBh",
        "outputId": "027d9b7b-85dc-4258-eb9e-cab50872baca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lWKjexCSd_z",
        "outputId": "ca198b86-532b-430a-a072-192cd8f183d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "with open('/content/drive/My Drive/AI_COLAB/Viettel/BDS_pro/word2vec_withpretrain_withlowercase.txt',encoding='utf-8') as file:\n",
        "  data = file.read().split('\\n')\n",
        "\n",
        "print(data[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ", 1.7550116 0.6403701 1.0106261 -0.100027084 -0.6259266 -2.0654218 -0.4674754 0.7536621 0.7067965 0.103890195 0.017494626 -2.143243 -1.5075748 -0.3049835 1.9675244 -2.7327125 -1.4566739 -0.3218536 -1.0424958 2.5058665 2.642143 0.35143217 1.9613054 -0.29678753 -2.0407095 1.7218971 -0.24872133 0.6826157 -0.7374534 1.1005011 -0.04818977 0.24478696 1.373891 1.7141589 -0.9877517 1.9262325 -0.030063858 -0.97013927 -0.4087678 -0.43211883 1.8703843 -0.0016462572 -0.4711411 -0.5984844 -1.1348495 0.9535509 1.2096301 0.7467564 -0.21234949 0.31430206 0.3561503 -0.39802393 -1.8856277 0.13919505 -0.18506627 -1.0821388 -2.0494413 -1.3256433 -1.7380134 -1.6475899 -1.5248672 1.4808004 -0.5824671 -0.5303667 0.7237124 -0.063007385 0.35951892 -0.4867316 -0.10153411 1.3696415 -2.1791265 1.6781235 0.7922007 -1.3253648 1.4471008 -0.9240042 -1.6856568 -0.19649033 0.2433787 0.43179432 0.66614825 2.1448116 -0.9609792 -0.8539397 1.2117928 -0.96543866 -0.06373902 -0.074353166 0.48525536 -0.92491305 0.3164538 1.8058876 0.8908069 -1.5699443 -1.4392716 1.0693709 0.73478657 0.5904834 0.701518 0.7459039 1.2939279 1.0914614 -0.99206984 0.56940764 0.27156314 2.0947864 -1.7346069 -0.74027 1.409363 -1.2845217 -0.72323793 -0.009721156 -0.79254127 1.2182271 0.7475091 1.1495407 -1.2054998 1.191915 -2.3635533 -1.5102687 1.1995298 -1.8043752 0.72947234 0.75300705 -0.7663483 -0.42624095 1.2994199 1.1390623 -0.8228938 0.32609063 0.10789208 -0.15264218 -0.19799045 -0.5952523 -1.6218204 0.15815644 -0.8609962 -0.2877423 -0.4146084 0.4419531 -0.87333906 -0.27359 0.32582247 -0.7769107 1.2509866 -0.8436436 -0.43750158 -0.86701393 -0.9636339 -0.6347726 1.4878589 0.32480785 -0.22040775 -0.25402763 -1.798902 -2.5463614 -0.649345 1.7820138 1.1404341 -0.24962552 1.3412468 2.0955966 0.89193064 2.3466966 -0.36383194 0.07856036 -1.2054771 -1.1049448 0.6103725 2.5612118 2.128137 -1.4744958 -1.3745439 1.4230038 1.6146762 -0.32283404 0.3141648 1.0644466 1.3325661 -1.335426 0.164175 -1.2922089 2.4165874 0.22013041 -0.35431144 0.36370778 1.157707 0.43753758 -0.89103574 -0.7956191 -1.0846916 0.70299447 -0.6289723 -0.62499684 -1.3409833 1.7705324 -0.60538775 -1.4248197 1.6135474 -0.17004894 1.1346663 -1.0313538 0.4553051 -0.17390166 -0.63079417 -1.0624063 -0.06876055 -1.184807 0.4252982 0.36381173 1.2183205 -1.063615 -0.9850326 0.9905539 0.2009045 -1.04627 0.7448949 1.0340936 -1.1034076 0.8336197 -0.6237434 -0.2620814 0.5954487 -1.0893803 0.3229224 0.0912265 -1.6030387 0.5590743 0.20815489 -1.6725966 0.58143914 0.35686484 1.6851702 -0.23722297 1.9269096 -0.07256975 1.5891399 0.86427766 -0.4208024 -0.513506 -0.48546442 -0.70951784 -0.945591 0.8691277 -0.4982258 -0.017357584 -0.083632946 -0.0041373754 -1.0613902 -0.5926539 0.54278374 -0.05682483 -0.35912868 0.9072851 -0.9222031 -1.1905285 -1.0573783 1.7868034 0.05646774 2.451792 0.098001994 0.8701292 -0.8255047 0.3237562 1.4170852 2.0318232 -1.5641598 -0.8834582 -0.17847563 -0.7615278 -1.3274454 -1.1326954 -0.48099157 0.86708575 2.2408843 -0.008555642 1.1609641 -0.043754082 -2.0590827 0.58759403 -0.8469439 -1.5436649 -0.19801761 -0.6672606 -0.03855742 0.60208637 -0.24212125 0.22303592 -1.3985647 0.2467839 -3.2534196 -1.0515915 1.8690231 -1.8708354 -0.88527834 0.82272863 1.2747678 0.19543347 -1.2012836 1.2533436 0.9061283 -0.4504427 -0.6469059 0.104492016 0.3837023 1.118246 -0.45534775 3.6351893 -1.1606438 -1.0528153 -0.7643061 -1.2363462 -1.0773673 -0.09491805 -0.979375 1.0998079 1.3614346 2.9000628 1.3103777 0.5585594 -0.92892295 0.7847857 0.88504165 1.3267691 1.8887378 1.2662809 -1.1620258 0.24887061 1.3105725 0.4047601 2.3592126 -0.8647795 -2.2793436 0.7375716 -1.5241854 -2.5040617 0.29278138 1.197605 -0.3404394 0.45219746 0.39699295 -1.2114182 -2.1355958 -0.045289762 -1.4824197 0.65318817 -0.29569307 0.836787 -0.44514343 -1.629825 -0.023247674 -1.2265925 -1.6405151 -2.2509363 -2.190397 -1.1019635 -0.49758384 -2.207213 -0.28450954 -0.72343373 0.6858946 1.7912862 1.0185753 0.64342856 0.032370113 1.8830339 -0.3837112 0.18248715 0.22201805 -1.1273226 0.27079692 0.45068637 1.0400112 1.3166641 0.4078465 -0.049826805 0.99749714 1.2506874 -0.10224689 -0.3943552 3.5239518 -2.2943013 -0.16796754 0.7283012 1.4123409 -0.43279755 2.3249047 1.0929499 3.7711365 -1.0498942 1.9762741 -0.3269554 -0.64303315 0.2510985 0.14025351 -0.6657521 -1.1535951 0.8127371 -1.3174772 -1.6441507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evVIOIqUhc4i"
      },
      "source": [
        "dictionary = {}\n",
        "for item in data[1:]:\n",
        "  item = item.split(' ')\n",
        "  if item[0] not in dictionary:\n",
        "    dictionary.update({item[0]:[float(x) for x in item[1:]]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtOEOLiKig_F",
        "outputId": "1ab0e74f-3351-4b6c-df37-ab3a5c9256ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(dictionary['đất'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.40574467, 0.844268, 0.74620414, 0.9889268, 1.37418, -0.71994126, -0.8223847, 0.5473631, -1.8178779, -0.8376753, 1.6344262, 2.729997, -0.19258766, 1.111818, 0.126012, -0.9965063, 2.5260065, -2.8010004, 1.1261895, -0.47051498, 3.3321054, -0.5482283, -2.1206062, 0.05738081, -1.3058552, -1.2468925, -0.865543, 0.48357227, 0.4675219, -1.1356575, -0.78893685, -0.69343674, -1.5698817, -1.731314, 1.3843918, -2.7145302, -0.45180565, 1.0547185, -0.6273778, -1.3626263, -0.37286052, 2.3618376, 1.5099602, 1.5730988, 0.9491139, -0.46997324, 0.57679176, -3.1692562, -1.4218695, 2.0438955, 2.227205, 2.675415, 2.807365, 2.0079217, 0.18793765, 2.1892152, -0.17387079, -0.07008807, -0.6112559, -2.050454, -1.3962607, -0.2587054, 0.9292434, 1.2262254, -0.9643388, 2.355654, -0.19742665, -2.5144622, -1.026666, 0.76425666, -0.11973483, -0.8137908, -0.19382918, 1.1736418, -0.26152638, -1.2861019, -3.3264437, 2.4360938, 2.0495498, -2.9874947, 0.38942838, -1.8482554, 2.6172075, -1.8258398, -0.6963065, 0.11221918, -0.30209807, 0.568825, 0.020235144, 1.9160913, 0.011250019, 1.7396511, -0.9608525, 1.5284173, -2.9699519, -1.5589194, 1.0910159, 2.0281773, -1.4187083, -0.12619302, -0.14171988, 3.440084, 0.9763913, -0.5105944, 1.986462, -0.265833, -4.2626944, 0.18067548, -0.16432144, -2.133374, 1.9967959, -2.994703, 1.3454356, -0.73856395, -3.024875, -1.4618057, -0.5730563, 1.8226918, 3.12037, 2.8822298, -1.2777743, -2.9231167, 0.014781687, 1.6565084, 0.3605166, 3.2325163, -2.0139344, -1.6050684, -0.6173292, -2.7982872, 0.150255, 1.4313369, 1.0157425, 0.21345231, -2.3340259, 0.6869019, 3.3245978, 0.33148476, -0.7013767, -2.2446501, -0.7959013, 0.47121263, -2.8569372, 1.3858901, -1.6190209, -0.094571434, -1.007764, -1.0979693, 1.0816764, 0.6853738, 0.4249247, -1.3046576, 0.36347857, 1.5823462, -1.1745609, -0.96271557, -0.50308233, 1.6864765, -0.031972695, -0.20230865, -1.1737533, 2.7103434, -0.47111514, 3.3743253, 0.25802305, -0.5747795, 0.02156206, 0.4092896, 1.1886886, 2.5793173, 1.554995, 1.5343885, 2.0693254, -0.7247577, 0.2527443, -0.88586044, -2.9854329, -2.7152405, 0.7371937, 0.122680314, -2.0704284, -2.7637124, 0.20244378, -1.8577237, 0.31440163, -0.68839866, -0.89905137, -0.9459041, -2.6021886, -1.0730917, 1.2532412, 0.17847943, -1.1446625, 0.024210323, -0.51835245, -2.6616304, 0.67693657, 0.39797258, -0.8647158, -0.78019124, -2.4452252, 1.5799391, 0.5028224, 1.7362297, 0.092202716, 0.17256136, -2.0460896, 1.8358305, 1.3577011, 0.6963571, 1.390497, 0.4969885, 2.2424493, -0.50902134, 0.18113516, 0.76640403, -0.4916123, 0.3908622, 0.6785925, -0.22724156, 1.1277648, -2.9262016, -0.6630608, -0.0848336, -1.7202419, -3.0255857, 0.9512318, 1.0093066, -1.5714768, -1.8129181, -1.876082, 1.1727245, 1.9391917, 1.3706414, 4.1206098, -0.3142553, 0.25275138, 0.59526294, 0.2991936, -1.1656328, -2.0419014, 0.67708707, 0.8195378, -0.679851, 1.220201, -1.615487, 3.6359444, 3.4965239, 2.0472772, 1.0179237, -2.3047009, -1.309863, 0.5189003, 0.26019827, 0.68219143, -1.3462372, -0.6043033, -1.9865087, -1.2658716, -0.46521416, -0.23281042, 1.2363739, 0.9782797, -1.6681933, -0.650404, 0.76699746, 0.85637915, -0.5583223, -0.5143691, -2.845763, -0.8904555, 1.6999234, 1.4293605, 1.8950962, -0.57415146, 0.56140065, -0.774268, 0.9625712, -0.02870264, -1.3498592, -0.17378545, -0.33956608, -2.187619, 0.8580801, 1.306889, -1.0884205, 1.410728, -1.622806, 2.1031911, -1.8691114, -0.41945684, -0.3645365, 0.4264207, 0.7963123, 1.3556485, 0.009874499, -0.056841288, -0.27954608, 0.39166787, 1.2503822, 0.15485318, 0.3420379, 2.673928, -0.48224646, -0.44336465, 0.99534816, 1.5221748, 1.1920465, -0.5225459, -0.09837687, 1.7179992, -2.3374808, 2.0988128, -2.276536, -0.3088329, -2.0364273, -1.7430192, -1.6855772, 1.9363294, 3.087022, 0.11199785, 0.50288194, -1.0406879, -1.8218248, -2.9447074, -1.5060273, -0.5828871, -0.3010461, -0.41696763, -0.16207373, 0.8979187, 1.0807177, 0.08489306, -2.0865896, 1.8892167, 1.4778986, 0.5131326, -2.641467, -1.6784307, -0.35203528, 1.0416356, -0.270309, 1.0344476, -1.2896543, -3.3764763, 0.5612571, -1.7636344, -4.3194184, -1.3295995, -0.286394, -1.7965863, 1.3532466, 0.76654035, -0.82890844, -1.752399, 0.6821056, -0.3705866, -1.6669011, 0.25245282, -0.40452012, -0.4628348, 1.0685028, 2.4439566, 1.343809, 0.4633358, -1.3903416, 4.070603, 0.4349844, -1.6563747, -0.61100996, -1.3543943, -0.124525994, -0.61257815, 0.95103484, 1.4031807, -1.6520553, -1.3848199, 1.2092254, 2.6264114, 0.17193389, 0.8974111, 0.25966376, 1.1251919, -1.7051616, -0.40525764, 1.9682089, -0.17410176, -0.98252344, -0.6777622, -1.4155161, 1.8919463, -2.6675017, -1.9269394, 2.939056, 0.85108143, 2.1068957, -0.9723553, 1.5377561, 0.24612653, 1.5649945]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtungLWmul7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1rfBgR9mvpq",
        "outputId": "c23fa302-992a-48f5-f53a-95d77457fd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/thanhlong1997/NLP-Models-Tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP-Models-Tensorflow'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 1988 (delta 45), reused 46 (delta 23), pack-reused 1916\u001b[K\n",
            "Receiving objects: 100% (1988/1988), 46.41 MiB | 27.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1339/1339), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmNh7NOvov7M",
        "outputId": "f0d8f8de-9d19-404e-bb0f-fc3069d46314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ZfMk4-n61C",
        "outputId": "2402579b-0c0b-4b50-f662-60d7491a9e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from NLP_Models_Tensorflow.text_classification.utils import *\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "trainset = sklearn.datasets.load_files(container_path = '/content/NLP_Models_Tensorflow/text_classification/data', encoding = 'UTF-8')\n",
        "trainset.data, trainset.target = separate_dataset(trainset,1.0)\n",
        "print (trainset.target_names)\n",
        "print (len(trainset.data))\n",
        "print (len(trainset.target))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', 'positive']\n",
            "10662\n",
            "10662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8POktRCvzaQ",
        "outputId": "563c48a0-6f13-4071-c3ec-167916647ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import json\n",
        "\n",
        "with open('a.json','r',encoding='utf-8') as file:\n",
        "  data_index = json.load(file)\n",
        "print(len(data_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAsHMKwZtL9q"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from itertools import islice\n",
        "\n",
        "dfs = pd.read_excel('data_classifier.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "check_nan_land = dfs['land'].isnull()\n",
        "colum_land = list(dfs['land'])\n",
        "check_nan_house = dfs['house'].isnull()\n",
        "colum_house = list(dfs['house'])\n",
        "\n",
        "index_land = []\n",
        "index_house = []\n",
        "\n",
        "for index1 in range(len(colum_land)):\n",
        "  if not check_nan_land[index1]:\n",
        "    index_land.append(int(colum_land[index1]))\n",
        "\n",
        "for index1 in range(len(colum_house)):\n",
        "  if not check_nan_house[index1]:\n",
        "    index_house.append(int(colum_house[index1]))\n",
        "\n",
        "data_land = []\n",
        "for index in range(len(index_land)):\n",
        "  data_land.append(data_index[index]['text'])\n",
        "data_house = []\n",
        "for index in range(len(index_house)):\n",
        "  data_house.append(data_index[index]['text'])\n",
        "# data_house = [data[index]['text'] for index in index_house]\n",
        "label_land = np.ones(len(data_land),dtype=int)\n",
        "label_house = np.zeros(len(data_house),dtype=int)\n",
        "\n",
        "train_set = []\n",
        "train_set.extend(data_land)\n",
        "train_set.extend(data_house)\n",
        "\n",
        "label = []\n",
        "label.extend(list(label_land))\n",
        "label.extend(list(label_house))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWVSkG-KByCx"
      },
      "source": [
        "from scipy import sparse\n",
        "def convert_labels(y, C = 2):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-DOCog_v_tP"
      },
      "source": [
        "print(convert_labels(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S6dvt18pHmX"
      },
      "source": [
        "# ONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\n",
        "# ONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\n",
        "labels = convert_labels(label)\n",
        "train_X, test_X,  train_onehot, test_onehot = train_test_split(train_set, \n",
        "                                                               labels, \n",
        "                                                               test_size=0.33, random_state=42)\n",
        "# concat = ' '.join(train_set).split()\n",
        "# vocabulary_size = len(list(set(concat)))\n",
        "# data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
        "word2vec_path = '/content/drive/My Drive/AI_COLAB/Viettel/BDS_pro/word2vec_withpretrain_withlowercase.txt'\n",
        "dictionary = dict()\n",
        "dictionary['GO'] = 0\n",
        "dictionary['PAD'] = 1\n",
        "dictionary['EOS'] = 2\n",
        "dictionary['UNK'] = 3\n",
        "matrix = []\n",
        "matrix.append([0]*400)\n",
        "matrix.append([0]*400)\n",
        "matrix.append([0]*400)\n",
        "matrix.append([0]*400)\n",
        "with open(word2vec_path,'r',encoding ='utf-8') as file:\n",
        "  wordvec = file.read().split('\\n')\n",
        "for index in range(1,len(wordvec)):\n",
        "  line = wordvec[index].split(' ')\n",
        "  if len(line) > 2:\n",
        "    dictionary[line[0]] = len(dictionary)\n",
        "    matrix.append([float(value) for value in line[1:]])\n",
        "# matrix = np.asarray(matrix)\n",
        "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEwcO2dm0IG9"
      },
      "source": [
        "matrix1 = matrix\n",
        "for item in matrix1:\n",
        "  try:\n",
        "    item = np.asarray(item).reshape([1,400])\n",
        "  except:\n",
        "    print(item)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7XGNVUfBpCQ"
      },
      "source": [
        "matrixs1 = np.matrix(matrix1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPZtmxbGy5uv",
        "outputId": "b9fe0d88-cd57-4eaf-a41e-b60e78ef051e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(matrixs1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175837, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtd6TcCSBfaU",
        "outputId": "7a6d1a65-f0a9-417a-d626-27e143d1f4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(matrix1[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.7550116, 0.6403701, 1.0106261, -0.100027084, -0.6259266, -2.0654218, -0.4674754, 0.7536621, 0.7067965, 0.103890195, 0.017494626, -2.143243, -1.5075748, -0.3049835, 1.9675244, -2.7327125, -1.4566739, -0.3218536, -1.0424958, 2.5058665, 2.642143, 0.35143217, 1.9613054, -0.29678753, -2.0407095, 1.7218971, -0.24872133, 0.6826157, -0.7374534, 1.1005011, -0.04818977, 0.24478696, 1.373891, 1.7141589, -0.9877517, 1.9262325, -0.030063858, -0.97013927, -0.4087678, -0.43211883, 1.8703843, -0.0016462572, -0.4711411, -0.5984844, -1.1348495, 0.9535509, 1.2096301, 0.7467564, -0.21234949, 0.31430206, 0.3561503, -0.39802393, -1.8856277, 0.13919505, -0.18506627, -1.0821388, -2.0494413, -1.3256433, -1.7380134, -1.6475899, -1.5248672, 1.4808004, -0.5824671, -0.5303667, 0.7237124, -0.063007385, 0.35951892, -0.4867316, -0.10153411, 1.3696415, -2.1791265, 1.6781235, 0.7922007, -1.3253648, 1.4471008, -0.9240042, -1.6856568, -0.19649033, 0.2433787, 0.43179432, 0.66614825, 2.1448116, -0.9609792, -0.8539397, 1.2117928, -0.96543866, -0.06373902, -0.074353166, 0.48525536, -0.92491305, 0.3164538, 1.8058876, 0.8908069, -1.5699443, -1.4392716, 1.0693709, 0.73478657, 0.5904834, 0.701518, 0.7459039, 1.2939279, 1.0914614, -0.99206984, 0.56940764, 0.27156314, 2.0947864, -1.7346069, -0.74027, 1.409363, -1.2845217, -0.72323793, -0.009721156, -0.79254127, 1.2182271, 0.7475091, 1.1495407, -1.2054998, 1.191915, -2.3635533, -1.5102687, 1.1995298, -1.8043752, 0.72947234, 0.75300705, -0.7663483, -0.42624095, 1.2994199, 1.1390623, -0.8228938, 0.32609063, 0.10789208, -0.15264218, -0.19799045, -0.5952523, -1.6218204, 0.15815644, -0.8609962, -0.2877423, -0.4146084, 0.4419531, -0.87333906, -0.27359, 0.32582247, -0.7769107, 1.2509866, -0.8436436, -0.43750158, -0.86701393, -0.9636339, -0.6347726, 1.4878589, 0.32480785, -0.22040775, -0.25402763, -1.798902, -2.5463614, -0.649345, 1.7820138, 1.1404341, -0.24962552, 1.3412468, 2.0955966, 0.89193064, 2.3466966, -0.36383194, 0.07856036, -1.2054771, -1.1049448, 0.6103725, 2.5612118, 2.128137, -1.4744958, -1.3745439, 1.4230038, 1.6146762, -0.32283404, 0.3141648, 1.0644466, 1.3325661, -1.335426, 0.164175, -1.2922089, 2.4165874, 0.22013041, -0.35431144, 0.36370778, 1.157707, 0.43753758, -0.89103574, -0.7956191, -1.0846916, 0.70299447, -0.6289723, -0.62499684, -1.3409833, 1.7705324, -0.60538775, -1.4248197, 1.6135474, -0.17004894, 1.1346663, -1.0313538, 0.4553051, -0.17390166, -0.63079417, -1.0624063, -0.06876055, -1.184807, 0.4252982, 0.36381173, 1.2183205, -1.063615, -0.9850326, 0.9905539, 0.2009045, -1.04627, 0.7448949, 1.0340936, -1.1034076, 0.8336197, -0.6237434, -0.2620814, 0.5954487, -1.0893803, 0.3229224, 0.0912265, -1.6030387, 0.5590743, 0.20815489, -1.6725966, 0.58143914, 0.35686484, 1.6851702, -0.23722297, 1.9269096, -0.07256975, 1.5891399, 0.86427766, -0.4208024, -0.513506, -0.48546442, -0.70951784, -0.945591, 0.8691277, -0.4982258, -0.017357584, -0.083632946, -0.0041373754, -1.0613902, -0.5926539, 0.54278374, -0.05682483, -0.35912868, 0.9072851, -0.9222031, -1.1905285, -1.0573783, 1.7868034, 0.05646774, 2.451792, 0.098001994, 0.8701292, -0.8255047, 0.3237562, 1.4170852, 2.0318232, -1.5641598, -0.8834582, -0.17847563, -0.7615278, -1.3274454, -1.1326954, -0.48099157, 0.86708575, 2.2408843, -0.008555642, 1.1609641, -0.043754082, -2.0590827, 0.58759403, -0.8469439, -1.5436649, -0.19801761, -0.6672606, -0.03855742, 0.60208637, -0.24212125, 0.22303592, -1.3985647, 0.2467839, -3.2534196, -1.0515915, 1.8690231, -1.8708354, -0.88527834, 0.82272863, 1.2747678, 0.19543347, -1.2012836, 1.2533436, 0.9061283, -0.4504427, -0.6469059, 0.104492016, 0.3837023, 1.118246, -0.45534775, 3.6351893, -1.1606438, -1.0528153, -0.7643061, -1.2363462, -1.0773673, -0.09491805, -0.979375, 1.0998079, 1.3614346, 2.9000628, 1.3103777, 0.5585594, -0.92892295, 0.7847857, 0.88504165, 1.3267691, 1.8887378, 1.2662809, -1.1620258, 0.24887061, 1.3105725, 0.4047601, 2.3592126, -0.8647795, -2.2793436, 0.7375716, -1.5241854, -2.5040617, 0.29278138, 1.197605, -0.3404394, 0.45219746, 0.39699295, -1.2114182, -2.1355958, -0.045289762, -1.4824197, 0.65318817, -0.29569307, 0.836787, -0.44514343, -1.629825, -0.023247674, -1.2265925, -1.6405151, -2.2509363, -2.190397, -1.1019635, -0.49758384, -2.207213, -0.28450954, -0.72343373, 0.6858946, 1.7912862, 1.0185753, 0.64342856, 0.032370113, 1.8830339, -0.3837112, 0.18248715, 0.22201805, -1.1273226, 0.27079692, 0.45068637, 1.0400112, 1.3166641, 0.4078465, -0.049826805, 0.99749714, 1.2506874, -0.10224689, -0.3943552, 3.5239518, -2.2943013, -0.16796754, 0.7283012, 1.4123409, -0.43279755, 2.3249047, 1.0929499, 3.7711365, -1.0498942, 1.9762741, -0.3269554, -0.64303315, 0.2510985, 0.14025351, -0.6657521, -1.1535951, 0.8127371, -1.3174772, -1.6441507]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yALlQy7C63Qz",
        "outputId": "c62dfd78-9bff-450d-e03c-de301954cc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "l = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "c = np.matrix(l[0])\n",
        "print(c.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXnJlIfGqqk6"
      },
      "source": [
        "print(train_onehot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOLaScA6EfsM"
      },
      "source": [
        "GO = dictionary['GO']\n",
        "PAD = dictionary['PAD']\n",
        "EOS = dictionary['EOS']\n",
        "UNK = dictionary['UNK']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KudpRGREjmR",
        "outputId": "bf4ef1be-0c03-4731-f0f0-82afc1766d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "class Model:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size,\n",
        "                 dict_size, dimension_output, learning_rate):\n",
        "        \n",
        "        def cells(size, reuse=False):\n",
        "            return tf.nn.rnn_cell.LSTMCell(size,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.float64, [None, dimension_output])\n",
        "        encoder_embeddings = tf.constant(matrixs1, name=\"embeding\")\n",
        "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "        \n",
        "        for n in range(num_layers):\n",
        "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = cells(size_layer // 2),\n",
        "                cell_bw = cells(size_layer // 2),\n",
        "                inputs = encoder_embedded,\n",
        "                dtype = tf.float64,\n",
        "                scope = 'bidirectional_rnn_%d'%(n))\n",
        "            encoder = tf.concat((out_fw, out_bw), 2)\n",
        "        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer(),dtype=tf.float64)\n",
        "        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer(),dtype=tf.float64)\n",
        "        self.logits = tf.matmul(encoder[:, -1], W) + b\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYAkxHTaE7Mu"
      },
      "source": [
        "size_layer = 128\n",
        "num_layers = 1\n",
        "embedded_size = 400\n",
        "dimension_output = 2\n",
        "learning_rate = 0.0001\n",
        "maxlen = 200\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWG4Ul94E8a1",
        "outputId": "850f5929-9ce2-4fe1-a7be-873f20225daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Model(size_layer,num_layers,embedded_size,matrixs1.shape[0],dimension_output,learning_rate)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-6faec57a72a3>:8: LSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-11-6faec57a72a3>:21: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrOr5xCzE_9R",
        "outputId": "417ea4cf-4173-49c8-e1e2-a3001fd51bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\n",
        "saver = tf.train.Saver()\n",
        "for epoch in range(epochs):\n",
        "        \n",
        "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
        "    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
        "        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n",
        "        loss, _ = sess.run([model.cost, model.optimizer], \n",
        "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
        "        train_loss += loss\n",
        "    # for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
        "    #     batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n",
        "    #     acc, loss = sess.run([model.accuracy, model.cost], \n",
        "    #                        feed_dict = {model.X : batch_x, model.Y : test_onehot[i:i+batch_size]})\n",
        "        # test_loss += loss\n",
        "        # test_acc += acc\n",
        "    acc_train = sess.run([model.accuracy],feed_dict={model.X:str_idx(train_X,dictionary,maxlen), model.Y:train_onehot})\n",
        "    acc_test = sess.run([model.accuracy],feed_dict={model.X:str_idx(test_X,dictionary,maxlen), model.Y:test_onehot})\n",
        "    save_path = saver.save(sess, \"/content/model/model\" + str(epoch)+ \".ckpt\")\n",
        "    # train_loss /= (len(train_X) // batch_size)\n",
        "    # train_acc /= (len(train_X) // batch_size)\n",
        "    # test_loss /= (len(test_X) // batch_size)\n",
        "    # test_acc /= (len(test_X) // batch_size)\n",
        "    \n",
        "    # if test_acc > CURRENT_ACC:\n",
        "    #     print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n",
        "    #     CURRENT_ACC = test_acc\n",
        "    #     CURRENT_CHECKPOINT = 0\n",
        "    # else:\n",
        "    #     CURRENT_CHECKPOINT += 1\n",
        "        \n",
        "    # print('time taken:', time.time()-lasttime)\n",
        "    print('epoch:',epoch, 'training loss:',train_loss ,' training acc: ',acc_train,  'valid acc:',acc_test)\n",
        "    # epoch += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 training loss: 2.0777018590571568  training acc:  [0.5284552845528455] valid acc: %f\n",
            " [0.4918032786885246]\n",
            "epoch: 1 training loss: 2.0574362549239273  training acc:  [0.5447154471544715] valid acc: %f\n",
            " [0.45901639344262296]\n",
            "epoch: 2 training loss: 2.039476054539797  training acc:  [0.5609756097560976] valid acc: %f\n",
            " [0.4426229508196721]\n",
            "epoch: 3 training loss: 2.0230713657015373  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.4262295081967213]\n",
            "epoch: 4 training loss: 2.007856380344106  training acc:  [0.5609756097560976] valid acc: %f\n",
            " [0.4426229508196721]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "epoch: 5 training loss: 1.9936527539894575  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.4262295081967213]\n",
            "epoch: 6 training loss: 1.9803698585794038  training acc:  [0.5853658536585366] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 7 training loss: 1.9679472688993762  training acc:  [0.5853658536585366] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 8 training loss: 1.9563243030712618  training acc:  [0.5772357723577236] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 9 training loss: 1.9454285792699912  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 10 training loss: 1.9351768178041846  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 11 training loss: 1.9254817358473653  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 12 training loss: 1.916260043114426  training acc:  [0.5609756097560976] valid acc: %f\n",
            " [0.4098360655737705]\n",
            "epoch: 13 training loss: 1.9074384260320918  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 14 training loss: 1.8989562327703964  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 15 training loss: 1.8907651275881059  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 16 training loss: 1.882826880576975  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 17 training loss: 1.875110536164814  training acc:  [0.5609756097560976] valid acc: %f\n",
            " [0.4098360655737705]\n",
            "epoch: 18 training loss: 1.8675899412463788  training acc:  [0.5609756097560976] valid acc: %f\n",
            " [0.4098360655737705]\n",
            "epoch: 19 training loss: 1.8602420343169985  training acc:  [0.5691056910569106] valid acc: %f\n",
            " [0.4098360655737705]\n",
            "epoch: 20 training loss: 1.8530459429506245  training acc:  [0.5772357723577236] valid acc: %f\n",
            " [0.39344262295081966]\n",
            "epoch: 21 training loss: 1.8459825826737268  training acc:  [0.5934959349593496] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 22 training loss: 1.8390344827776925  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 23 training loss: 1.8321856053658387  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 24 training loss: 1.8254210812668277  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 25 training loss: 1.8187269032115225  training acc:  [0.6016260162601627] valid acc: %f\n",
            " [0.3770491803278688]\n",
            "epoch: 26 training loss: 1.8120896520222587  training acc:  [0.6016260162601627] valid acc: %f\n",
            " [0.3770491803278688]\n",
            "epoch: 27 training loss: 1.8054963274363922  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 28 training loss: 1.7989343104330402  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 29 training loss: 1.792391438004223  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 30 training loss: 1.7858561404079547  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 31 training loss: 1.7793175762639388  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 32 training loss: 1.7727657255278424  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 33 training loss: 1.7661914187612489  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 34 training loss: 1.7595862948576875  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 35 training loss: 1.752942712412099  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 36 training loss: 1.7462536267012554  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 37 training loss: 1.7395124481459128  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 38 training loss: 1.7327128970098788  training acc:  [0.6097560975609756] valid acc: %f\n",
            " [0.36065573770491804]\n",
            "epoch: 39 training loss: 1.725848864800021  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 40 training loss: 1.7189142884915949  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 41 training loss: 1.711903052790965  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 42 training loss: 1.7048089193087645  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 43 training loss: 1.6976254895208014  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 44 training loss: 1.6903461970505835  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 45 training loss: 1.6829643291869802  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 46 training loss: 1.6754730630674066  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 47 training loss: 1.6678655147628252  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 48 training loss: 1.6601347819016326  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 49 training loss: 1.6522739803766038  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 50 training loss: 1.6442762804919608  training acc:  [0.6178861788617886] valid acc: %f\n",
            " [0.3442622950819672]\n",
            "epoch: 51 training loss: 1.6361349401796108  training acc:  [0.6260162601626016] valid acc: %f\n",
            " [0.32786885245901637]\n",
            "epoch: 52 training loss: 1.6278433546563704  training acc:  [0.6260162601626016] valid acc: %f\n",
            " [0.32786885245901637]\n",
            "epoch: 53 training loss: 1.6193951343301443  training acc:  [0.6341463414634146] valid acc: %f\n",
            " [0.3114754098360656]\n",
            "epoch: 54 training loss: 1.610784219714389  training acc:  [0.6341463414634146] valid acc: %f\n",
            " [0.3114754098360656]\n",
            "epoch: 55 training loss: 1.6020050417294707  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 56 training loss: 1.5930527043821994  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 57 training loss: 1.583923180299247  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 58 training loss: 1.5746134823422595  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 59 training loss: 1.5651218189346774  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 60 training loss: 1.5554477383947227  training acc:  [0.6341463414634146] valid acc: %f\n",
            " [0.3114754098360656]\n",
            "epoch: 61 training loss: 1.545592276495705  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.29508196721311475]\n",
            "epoch: 62 training loss: 1.5355581092565416  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 63 training loss: 1.5253497021465614  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 64 training loss: 1.5149734270905317  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 65 training loss: 1.5044376279861078  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 66 training loss: 1.4937526341871785  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 67 training loss: 1.4829307077712603  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 68 training loss: 1.4719858916243638  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 69 training loss: 1.4609337026657943  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.2786885245901639]\n",
            "epoch: 70 training loss: 1.449790627244537  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 71 training loss: 1.438573449539648  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 72 training loss: 1.4272985298237668  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 73 training loss: 1.4159811647081222  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 74 training loss: 1.4046350899329492  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 75 training loss: 1.393272084019888  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 76 training loss: 1.381901621580889  training acc:  [0.6422764227642277] valid acc: %f\n",
            " [0.26229508196721313]\n",
            "epoch: 77 training loss: 1.3705305401131358  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.2459016393442623]\n",
            "epoch: 78 training loss: 1.3591627148618486  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.2459016393442623]\n",
            "epoch: 79 training loss: 1.3477987319200502  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.2459016393442623]\n",
            "epoch: 80 training loss: 1.3364355339932321  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.2459016393442623]\n",
            "epoch: 81 training loss: 1.3250659870940023  training acc:  [0.6504065040650406] valid acc: %f\n",
            " [0.2459016393442623]\n",
            "epoch: 82 training loss: 1.3136784061376077  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 83 training loss: 1.3022561580115508  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 84 training loss: 1.2907776739141168  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 85 training loss: 1.2792173330386063  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 86 training loss: 1.2675476993009627  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 87 training loss: 1.2557434664148706  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 88 training loss: 1.2437873210089  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 89 training loss: 1.231677749660376  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 90 training loss: 1.219438584613182  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 91 training loss: 1.2071290876100451  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 92 training loss: 1.1948513027303826  training acc:  [0.6666666666666666] valid acc: %f\n",
            " [0.21311475409836064]\n",
            "epoch: 93 training loss: 1.1827486522055781  training acc:  [0.6666666666666666] valid acc: %f\n",
            " [0.21311475409836064]\n",
            "epoch: 94 training loss: 1.1709891896872362  training acc:  [0.6666666666666666] valid acc: %f\n",
            " [0.21311475409836064]\n",
            "epoch: 95 training loss: 1.159732453363419  training acc:  [0.6585365853658537] valid acc: %f\n",
            " [0.22950819672131148]\n",
            "epoch: 96 training loss: 1.149090493398079  training acc:  [0.6666666666666666] valid acc: %f\n",
            " [0.21311475409836064]\n",
            "epoch: 97 training loss: 1.1391028300516197  training acc:  [0.6666666666666666] valid acc: %f\n",
            " [0.19672131147540983]\n",
            "epoch: 98 training loss: 1.129741158577978  training acc:  [0.6747967479674797] valid acc: %f\n",
            " [0.19672131147540983]\n",
            "epoch: 99 training loss: 1.1209424060454993  training acc:  [0.6747967479674797] valid acc: %f\n",
            " [0.19672131147540983]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}