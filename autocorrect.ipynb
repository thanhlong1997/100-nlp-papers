{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4LnVd73GUpvXRdt+gVqtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/autocorrect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He6c59ylCBS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524ea32a-be78-4113-8c1b-e45da299afad"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FMb-o-mD7H1",
        "outputId": "e346e5a2-21f4-4d55-9271-4a9e3e891f62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AueOdJqVFpFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf2967f-6ef4-41ab-9118-f4a140fbadca"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiKrMu26KyH1"
      },
      "source": [
        "with open('/content/drive/MyDrive/AI_COLAB/Viettel/data_noise.txt','r',encoding='utf-8') as file:\n",
        "  data_noise = file.read().split('\\n')\n",
        "with open('/content/drive/MyDrive/AI_COLAB/Viettel/data_true.txt','r',encoding='utf-8') as file:\n",
        "  data_true = file.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzNFcmRzFX_p"
      },
      "source": [
        "def unfold_vnese(token):\n",
        "    accent_dictionary = {\n",
        "    'á':'as', 'à':'af', 'ả':'ar', 'ã':'ax', 'ạ':'aj', 'â':'aa', 'ấ':'aas', 'ầ':'aaf', 'ẩ':'aar', 'ẫ':'aax', 'ậ':'aaj', 'ă':'aw', 'ắ':'aws', 'ằ':'awf', 'ẳ':'awr', 'ẵ':'awx', 'ặ':'awj',\n",
        "    'ó':'os', 'ò':'of', 'ỏ':'or', 'õ':'ox', 'ọ':'oj', 'ô':'oo', 'ố':'oos', 'ồ':'oof', 'ổ':'oor', 'ỗ':'oox', 'ộ':'ooj', 'ơ':'ow', 'ớ':'ows', 'ờ':'owf', 'ở':'owr', 'ỡ':'owx', 'ợ':'owj',\n",
        "    'é':'es', 'è':'ef', 'ẻ':'er', 'ẽ':'ex', 'ẹ':'ej', 'ê':'ee', 'ế':'ees', 'ề':'eef', 'ể':'eer', 'ễ':'eex', 'ệ':'eej',\n",
        "    'ú':'us', 'ù':'uf', 'ủ':'ur', 'ũ':'ux', 'ụ':'uj', 'ư':'uw', 'ứ':'uws', 'ừ':'uwf', 'ử':'uwr', 'ữ':'uwx', 'ự':'uwj',\n",
        "    'í':'is', 'ì':'if', 'ỉ':'ir', 'ĩ':'ix', 'ị':'ij',\n",
        "    'ý':'ys', 'ỳ':'yf', 'ỷ':'yr', 'ỹ':'yx', 'ỵ':'yj',\n",
        "    'đ':'dd'\n",
        "    }\n",
        "    unfold = ''\n",
        "    for character in token :\n",
        "        if character in accent_dictionary.keys():\n",
        "            unfold += accent_dictionary[character]\n",
        "        else:\n",
        "            unfold += character\n",
        "    return unfold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNfaweKdLEa7"
      },
      "source": [
        "with open('combine_dataaaaaaaaaa.txt','w',encoding='utf-8') as file:\n",
        "  for index in range(len(data_noise)):\n",
        "    file.write(data_noise[index] +'\\t' + data_true[index]+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5SccVjiCMsp"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5XNQMcqLxRq"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/AI_COLAB/Viettel/combine_data.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLOfs-CjZUQc"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/AI_COLAB/Viettel/combine_data.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA9PP3DjTL6Q"
      },
      "source": [
        "with open(path_to_file,'r',encoding ='utf-8',buffering=100000) as file:\n",
        "    data = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFQEQF0oTOQu"
      },
      "source": [
        "data = data.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Lntb43TQXz",
        "outputId": "0d65507e-bfd2-4f6a-e0d5-04eddf07c95d"
      },
      "source": [
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5946867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjM8ooFxCRoM"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    # w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    # w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZykxJ_TCThC"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    lines = lines[:-1]\n",
        "    print(len(lines))\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gSE2W9CVZx"
      },
      "source": [
        "class LanguageSourceIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    # self.type_tokenizer = type_tokenizer\n",
        "    \n",
        "    self.create_index()\n",
        "    # print(self.vocab)\n",
        "  def create_index(self):\n",
        "    # if self.type_tokenizer == 0:\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase)\n",
        "    \n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    self.word2idx['<start>'] = 1\n",
        "    self.word2idx['<end>'] = 2\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = len(self.word2idx)\n",
        "    \n",
        "    self.idx2word = {v: k for k, v in self.word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlpmDAsJAWpZ"
      },
      "source": [
        "class LanguageTargetIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    # self.vocab.append('<pad>')\n",
        "    # self.vocab.append('<start>')\n",
        "    # self.vocab.append('<end>')\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print(self.vocab)\n",
        "    self.word2idx['<pad>'] = 0\n",
        "    self.word2idx['<start>'] = 1\n",
        "    self.word2idx['<end>'] = 2\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = len(self.word2idx)\n",
        "    \n",
        "    self.idx2word = {v: k for k, v in self.word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7CGziudKxAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e888aff-67de-4d70-bf4e-f2ad8339c81d"
      },
      "source": [
        "a = LanguageTargetIndex('đường 1 bình chánh hồ chí minh')\n",
        "a.word2idx\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '1', 'b', 'c', 'g', 'h', 'i', 'm', 'n', 'á', 'ì', 'í', 'đ', 'ư', 'ồ', 'ờ']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 3,\n",
              " '1': 4,\n",
              " '<end>': 2,\n",
              " '<pad>': 0,\n",
              " '<start>': 1,\n",
              " 'b': 5,\n",
              " 'c': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'm': 10,\n",
              " 'n': 11,\n",
              " 'á': 12,\n",
              " 'ì': 13,\n",
              " 'í': 14,\n",
              " 'đ': 15,\n",
              " 'ư': 16,\n",
              " 'ồ': 17,\n",
              " 'ờ': 18}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGu7fNZXCXmc"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "    print(pairs[0])\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageSourceIndex(noise for noise, true in pairs)\n",
        "    targ_lang = LanguageTargetIndex(true for noise, true in pairs)\n",
        "    print(targ_lang.word2idx)\n",
        "    \n",
        "    \n",
        "    \n",
        "    input_tensor = []\n",
        "    target_tensor = []\n",
        "    for noise, true in pairs:\n",
        "      noise_senten_tensor = [inp_lang.word2idx['<start>']]\n",
        "      for s in noise:\n",
        "        # if s in inp_lang.word2idx:\n",
        "          noise_senten_tensor.append(inp_lang.word2idx[s])\n",
        "      noise_senten_tensor.append(inp_lang.word2idx['<end>'])\n",
        "\n",
        "      true_senten_tensor = [targ_lang.word2idx['<start>']]\n",
        "      for s in true.split(' '):\n",
        "        # if s in targ_lang.word2idx:\n",
        "        true_senten_tensor.append(targ_lang.word2idx[s])\n",
        "      true_senten_tensor.append(targ_lang.word2idx['<end>'])\n",
        "\n",
        "      input_tensor.append(noise_senten_tensor)\n",
        "      target_tensor.append(true_senten_tensor)\n",
        "    \n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    print(max_length_inp,max_length_tar)\n",
        "    print(len(input_tensor),len(target_tensor))\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Rm5UOyCZtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a4544a-f5c7-4a11-ebe8-a6f0672be86f"
      },
      "source": [
        "num_examples = -1\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5946865\n",
            "['duongdwf 1 binhf chanhs hoof chis minh', 'đường 1 bình chánh hồ chí minh']\n",
            "['', '-', '01-39', '03', '04', '05', '06', '07', '09', '1', '1-1', '1-10', '1-12', '1-19', '1-2', '1-2-3', '1-22', '1-3', '1-4', '1-42', '1-5', '1-6', '1-7', '1-8', '1-9', '10', '10-11', '10-14', '10-22', '100', '100a', '101', '1011', '102', '1021', '1022', '1025', '103', '104', '1041', '104a', '106', '107', '108', '1088', '109', '10a', '10b', '10c', '10d', '10e', '10f', '10g', '11', '11-6', '11-7', '110', '1106', '1107', '111', '112', '114', '115', '116', '117', '118', '119', '11a', '11b', '11c', '11d', '11e', '11l', '11n', '12', '12-52', '120', '121', '122', '123', '1234', '123a', '124', '126', '127d', '128', '128c', '129', '12a', '12ab', '12b', '12c', '12d', '12e', '13', '130', '131', '1315', '132', '135', '137', '138', '13a', '13b', '13c', '14', '14-1', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149/33', '14a', '14b', '14c', '14d', '14e', '14f', '15', '15-23', '150', '152', '152a', '152b', '153', '154', '156', '157', '15a', '15b', '15c', '15d', '15e', '15n', '16', '16-1', '16-10', '16-18', '16-21', '16-25', '16-37', '16-7', '160', '161', '162', '163', '164', '167', '168', '16a', '16b', '16d', '17', '171', '172', '173', '175', '176', '177', '179', '17a', '17b', '17c', '17d', '18', '180', '181', '182', '185', '187', '188', '189', '18a', '18b', '18c', '18d', '18e', '19', '19-10', '19-16', '19-32', '19/1', '19/12', '19/5', '19/5a', '19/5b', '19/8', '191', '192', '197', '198', '19a', '19b', '19c', '19d', '19e', '19f', '19r', '1a', '1b', '1c', '1d', '1e', '1f', '1k', '2', '2,5', '2-1', '2-10', '2-11', '2-2', '2-22', '2-3', '2-3-1', '2-3-4', '2-4', '2-46', '2-5', '2-6', '2-6-7', '2-64', '2-7', '2-8', '2.5a', '2/9', '20', '200', '201', '202', '204', '204b', '205a', '206', '207', '208', '20a', '21', '21-19', '21-25', '210', '210a', '211', '212', '214', '215', '217', '218', '21a', '21b', '21c', '21d', '21e', '22', '22-3', '22/12', '220', '221', '222', '223', '224', '225', '226', '22a', '22b', '23', '23/10', '23/9', '230', '231', '232', '233', '234', '236', '238', '239', '23a', '23b', '24', '244', '245', '247', '248/1', '24a', '24b', '25', '251', '25a', '25b', '26', '26-1', '26-1a', '26-3', '26/3', '26/5', '261', '263', '265', '266', '267', '267a', '267b', '26a', '27', '27-2', '27/3', '27/7', '270', '270a', '270b', '274', '275', '278', '279', '27a', '27c', '28', '281', '284', '286', '288', '28a', '29', '290', '295', '296', '297', '299', '2a', '2b', '2c', '2d', '2e', '2f', '2g', '2k', '3', '3-1', '3-10', '3-19', '3-2-1', '3-26', '3-3', '3-31', '3-33', '3-34', '3-36', '3-4', '3-4-5', '3-4a', '3-7', '3.5', '3/2', '30', '30-1', '30/4', '300', '301', '304', '3052a', '308', '30a', '30b', '30c', '31', '31-4', '311', '312', '3158b', '316', '318', '319', '31a', '31b', '31c', '31d', '31e', '31f', '32', '320', '320a', '321', '32a', '32b', '32c', '32d', '33', '330', '332', '339', '33a', '33b', '33c', '34', '34a', '34b', '34c', '35', '353', '355', '359', '35a', '36', '360', '363', '364', '3643c', '365', '367', '368', '37', '372', '374', '378', '379', '37a', '38', '385', '387', '389', '38a', '38b', '39', '397', '397b', '39b', '3a', '3b', '3c', '3d', '3j', '4', '4-1', '4-2', '4-2-1', '4-22', '4-23', '4-24', '4-3', '4-37', '4-38', '4-4', '4-5', '4-6', '4-9', '40', '400', '406', '409', '40b', '41', '411', '411c', '414', '416', '418', '419', '41a', '41b', '42', '420', '421b', '422', '422a', '422b', '423', '424', '426', '427', '429', '42a', '43', '430', '435', '436', '43b', '44', '440', '441', '442', '446', '447', '447b', '448', '449', '45', '454', '455', '456', '457', '458', '46', '463', '464', '46a', '46b', '47', '473', '475', '479', '48', '487', '489', '48a', '48b', '49', '490', '491', '494', '495', '499', '49a', '49b', '49c', '4a', '4b', '4c', '4d', '4e', '4f', '5', '5-1', '5-10-11', '5-11-12', '5-12', '5-2', '5-26', '5-27', '5-3', '5-31', '5-6', '5-8', '50', '50/27', '500', '506', '508', '50a', '50b', '50c', '50d', '51', '511', '512', '517', '519', '52', '520', '52a', '52b', '53', '530', '531', '538', '539', '53a', '53b', '53c', '54', '540', '545', '546', '54a', '55', '556', '55a', '55b', '56', '567', '568', '57', '570', '572', '577', '57a', '57b', '57c', '58', '588', '58c', '59', '591', '595', '597', '59b', '59c', '5a', '5b', '5c', '5d', '5e', '5f', '5k', '5n', '5x', '6', '6-1', '6-17', '6-2', '6-2-5', '6-22', '6-23', '6-27', '6-3', '6-38', '60', '606', '60b', '61', '614', '618', '61b', '62', '622', '623', '624', '626', '628', '63', '635', '64', '640', '643', '647', '65', '652', '659', '66', '668', '67', '671', '672', '68', '680', '688', '69', '6a', '6b', '6c', '6d', '6e', '6f', '6t', '7', '7-', '7-1', '7-13', '7-17', '7-2', '7-25', '7-3', '7-34', '7-4', '7-5', '7-50', '7-50b', '7-6', '7-8', '7-9', '70', '702', '708', '70a', '70b', '71', '711', '715', '717', '719', '71a', '72', '73', '734', '74', '741', '747', '75', '76', '762', '767', '769', '77', '78', '783', '785', '787', '79', '791', '793', '7a', '7b', '7c', '7d', '7e', '7l', '7p', '8', '8-1', '8-11', '8-18', '8-22', '8-28', '8-3', '8-38', '8-4', '8-9', '8/3', '80', '800a', '808', '80b', '81', '811', '816', '817', '817a', '82', '822', '823', '824', '825', '826', '826c', '827', '83', '830', '830b', '833', '833b', '835', '835a', '835b', '835c', '835d', '836', '84', '842', '843', '84b', '85', '86', '865', '866', '87', '87a', '88', '882', '89', '892', '898', '8a', '8b', '8c', '8d', '8g', '8k', '9', '9-1', '9-17', '9-2', '9-34', '9-48', '9-5', '9-55', '902', '904', '907', '909', '91', '911', '915', '916', '92', '93', '933', '94', '95', '96', '963', '97', '970', '976', '98', '980', '99', '990', '9a', '9b', '9f', '9m', 'a', 'a-3', 'a1', 'a10', 'a2', 'a3', 'a35', 'a4', 'a5', 'a6', 'a61', 'a7', 'a8', 'a9', 'alexandre', 'am', 'an', 'anh', 'ao', 'b', 'b1', 'b10', 'b11', 'b12', 'b14', 'b2', 'b3', 'b37', 'b4', 'b5', 'b6', 'b7', 'ba', 'bang', 'bao', 'bà', 'bài', 'bàu', 'bách', 'bạc', 'bạch', 'be', 'bi', 'bia', 'binh', 'biên', 'biều', 'biểu', 'bn1', 'buồm', 'bùi', 'buồm', 'buội', 'bà', 'bài', 'bàn', 'bàng', 'bành', 'bàu', 'bá', 'bác', 'bái', 'bán', 'bánh', 'bát', 'bân', 'bây', 'bãi', 'bè', 'bé', 'bê', 'bến', 'bình', 'bí', 'bích', 'bính', 'bò', 'bông', 'bồ', 'bột', 'bùi', 'bùng', 'bún', 'búp', 'bút', 'bơ', 'bơi', 'bưng', 'bương', 'bươu', 'bường', 'bưởi', 'bạc', 'bạch', 'bạng', 'bạt', 'bản', 'bảng', 'bảo', 'bảy', 'bầu', 'bậc', 'bật', 'bắc', 'bằng', 'bặc', 'bế', 'bến', 'bền', 'bệ', 'bỉnh', 'bị', 'bốc', 'bối', 'bồ', 'bộ', 'bộc', 'bội', 'bột', 'bờ', 'bụt', 'bứa', 'bức', 'bửu', 'c', 'c1', 'c10', 'c11', 'c12', 'c13', 'c18', 'c19', 'c2', 'c22', 'c27', 'c3', 'c4', 'c5', 'c6', 'c65', 'c7', 'c7b', 'c7c', 'c8', 'c9', 'ca', 'cai', 'calmette', 'cam', 'camex', 'can', 'cang', 'canh', 'cao', 'cau', 'cát', 'cả', 'cc2', 'cc3', 'cc5', 'cd2', 'cd3', 'chai', 'chi', 'chinh', 'chiêm', 'chiêu', 'chiếc', 'chiến', 'chiếu', 'chiểu', 'chu', 'chung', 'chuyên', 'chuông', 'chúc', 'chuẩn', 'chuối', 'chàng', 'chác', 'chánh', 'cháo', 'chân', 'châu', 'chè', 'chèo', 'chì', 'chí', 'chích', 'chính', 'chùa', 'chú', 'chúa', 'chúc', 'chĩnh', 'chợ', 'chương', 'chưởng', 'chạo', 'chả', 'chấn', 'chất', 'chẩm', 'chế', 'chỉ', 'chợ', 'chủ', 'chử', 'cn1', 'cn10', 'cn11', 'cn12', 'cn13', 'cn2', 'cn3', 'cn4', 'cn5', 'cn6', 'cn7', 'cn8', 'cn9', 'con', 'cò', 'ct.', 'ct01', 'cu', 'cung', 'cá', 'các', 'cách', 'cái', 'cám', 'cánh', 'cáp', 'cát', 'cân', 'câu', 'cây', 'cầu', 'cấn', 'cận', 'có', 'cót', 'cô', 'côi', 'côn', 'công', 'cống', 'cù', 'cùng', 'cúc', 'căn', 'cơ', 'cư', 'cương', 'cửa', 'cường', 'cả', 'cảm', 'cảng', 'cảnh', 'cấm', 'cấn', 'cầm', 'cần', 'cầu', 'cẩm', 'cẩn', 'cọ', 'cố', 'cốc', 'cốm', 'cống', 'cồ', 'cổ', 'cổng', 'cội', 'cộng', 'cột', 'cờ', 'củ', 'của', 'củi', 'củng', 'cừ', 'cử', 'cửa', 'cửu', 'cự', 'd', 'd1', 'd10', 'd100', 'd11', 'd11b', 'd12', 'd13', 'd14', 'd14a', 'd14b', 'd15', 'd16', 'd17', 'd18', 'd19', 'd1a', 'd2', 'd20', 'd21', 'd22', 'd23', 'd24', 'd26', 'd2a', 'd2d', 'd3', 'd35', 'd3a', 'd3b', 'd4', 'd400', 'd436', 'd44', 'd4a', 'd5', 'd52', 'd5a', 'd5b', 'd6', 'd64', 'd6a', 'd7', 'd8', 'd9', 'd990', 'd9a', 'da', 'dal', 'danh', 'dc1', 'dc11', 'dc13', 'dc2', 'dc3', 'dc4', 'dc5', 'dc6', 'dc7', 'dc8', 'dc9', 'dct1', 'dct5', 'dct7', 'dct9', 'dd10', 'dd11', 'dd12', 'dd2', 'dd4', 'dd4-1', 'dd4-2', 'dd5', 'dd6', 'dd6-1', 'dd7', 'dd7-1', 'dd9', 'dh5', 'di', 'diên', 'diêu', 'diều', 'diễn', 'diện', 'diệp', 'diệu', 'dk4', 'dn', 'dn10', 'dn11', 'dn12', 'dn13', 'dn2', 'dn4', 'dn5', 'dn6', 'dn7', 'dn8', 'dn9', 'do', 'doanh', 'doãn', 'dòng', 'dtc', 'dtc2', 'du', 'dung', 'duy', 'duyên', 'duyện', 'duẩn', 'duật', 'dx', 'dài', 'dân', 'dây', 'dã', 'dền', 'dùng', 'dĩ', 'dĩa', 'dũng', 'dư', 'dưa', 'dương', 'dừa', 'dưỡng', 'dược', 'dạ', 'dạng', 'dầu', 'dật', 'dậu', 'dậy', 'dế', 'dền', 'dị', 'dịch', 'dốc', 'dục', 'dừa', 'dữ', 'dực', 'e', 'e12', 'e14', 'e2', 'e32', 'e38', 'e5', 'e50', 'ec', 'einstein', 'f', 'f1', 'f11', 'g', 'g1', 'g10', 'g11', 'g14', 'g2', 'g3', 'g4', 'g5', 'g6', 'g7', 'g8', 'g9', 'ga', 'gai', 'ghe', 'gia', 'giai', 'giang', 'giao', 'giám', 'giảng', 'giuộc', 'già', 'giàu', 'giác', 'giám', 'giáng', 'giáo', 'giáp', 'giây', 'giã', 'gióng', 'giót', 'giải', 'giản', 'giảng', 'giấy', 'giầy', 'giếng', 'giỏi', 'giống', 'giồng', 'giờ', 'giữ', 'giữa', 'go', 'gò', 'guột', 'gà', 'gánh', 'gò', 'gót', 'gõ', 'gạch', 'gạo', 'gấm', 'gầm', 'gốc', 'gốm', 'gỗ', 'h', 'h1', 'h10', 'h2', 'hai', 'hang', 'hà', 'hàn', 'hàng', 'hành', 'hải', 'hạ', 'hb2', 'hi', 'him', 'hiến', 'hiếu', 'hiền', 'hiển', 'hiểu', 'hiện', 'hiệp', 'hiệu', 'hn1', 'hoa', 'hoan', 'hom', 'hoà', 'hoài', 'hoàn', 'hoàng', 'hoành', 'hoạch', 'hu', 'huy', 'huyên', 'huyền', 'huyền', 'huyện', 'huân', 'huê', 'huế', 'huấn', 'huế', 'huề', 'huệ', 'huống', 'huỳnh', 'hy', 'hà', 'hài', 'hàm', 'hàn', 'hàng', 'hành', 'hào', 'hán', 'háo', 'hát', 'hân', 'hãn', 'hè', 'hên', 'hòa', 'hòe', 'hòm', 'hóa', 'hóc', 'hổ', 'hội', 'hùng', 'húc', 'hưng', 'hưu', 'hương', 'hướng', 'hướng', 'hưởng', 'hạ', 'hạc', 'hạnh', 'hạo', 'hạt', 'hải', 'hảo', 'hầu', 'hậu', 'hắc', 'hằng', 'hẹ', 'hẻm', 'họa', 'học', 'hỏa', 'hố', 'hối', 'hồ', 'hồi', 'hồng', 'hổ', 'hộ', 'hội', 'hộp', 'hới', 'hớn', 'hợp', 'hụ', 'hữu', 'hỷ', 'i', 'iii', 'ích', 'jd12', 'k', 'k1', 'k1b', 'k2', 'k22', 'k3', 'k300', 'k32', 'k4', 'k800a', 'kao', 'kdc', 'keo', 'kha', 'khai', 'khang', 'khanh', 'khay', 'khánh', 'khi', 'khiêm', 'khiêu', 'khiết', 'khiếu', 'kho', 'khoa', 'khoai', 'khoan', 'khoang', 'khoách', 'khoái', 'khoát', 'khu', 'khuyê', 'khuyến', 'khuê', 'khuông', 'khuất', 'khái', 'kháng', 'khánh', 'khát', 'khâm', 'khéo', 'khê', 'khôi', 'khổng', 'khúc', 'khơi', 'khương', 'khạ', 'khả', 'khải', 'khảm', 'khẩu', 'khắc', 'khỏe', 'khối', 'khổng', 'khởi', 'kim', 'kinh', 'kiên', 'kiêu', 'kiếm', 'kiến', 'kiếp', 'kiều', 'kiển', 'kiểng', 'kiểu', 'kiệm', 'kiện', 'kiệp', 'kiệt', 'kiệu', 'kon', 'kontum', 'kp', 'kp3', 'kp5', 'kt', 'kt19', 'kt20', 'kt21', 'kt4', 'kt8', 'ky', 'kỳ', 'kênh', 'kính', 'ký', 'kẻ', 'kế', 'kết', 'kỉnh', 'kỳ', 'kỵ', 'kỷ', 'l25', 'l6', 'la', 'lai', 'lam', 'lan', 'lang', 'lao', 'láng', 'lãng', 'le', 'linh', 'liêm', 'liên', 'liêu', 'liễu', 'liệt', 'liền', 'liễu', 'liệp', 'liệt', 'liệu', 'loa', 'long', 'lu', 'lung', 'luyện', 'luông', 'luật', 'ly', 'lý', 'là', 'lài', 'làng', 'lành', 'lá', 'lách', 'lái', 'láng', 'lâm', 'lân', 'lâu', 'lã', 'lãm', 'lãn', 'lãng', 'lãnh', 'lão', 'lê', 'lên', 'lệnh', 'lò', 'lô', 'lôi', 'lỗi', 'lộ', 'lùng', 'lý', 'lăng', 'lĩnh', 'lũ', 'lũng', 'lũy', 'lơ', 'lơn', 'lư', 'lưu', 'lương', 'lường', 'lược', 'lượng', 'lạc', 'lại', 'lấp', 'lầu', 'lập', 'lắm', 'lắng', 'lễ', 'lệ', 'lịch', 'lỗ', 'lỗi', 'lộ', 'lộc', 'lội', 'lộng', 'lớn', 'lở', 'lợi', 'lụa', 'lục', 'lủ', 'lừ', 'lừa', 'lửa', 'lữ', 'lực', 'lựu', 'm', 'm1', 'm10', 'm12', 'm16', 'm3', 'm34', 'm4', 'm5', 'm50', 'm6', 'm7', 'm8', 'mai', 'man', 'mác', 'mã', 'me', 'mi', 'minh', 'miên', 'miêu', 'miếu', 'miến', 'miếng', 'miếu', 'miễn', 'miễu', 'muối', 'muồng', 'mài', 'mành', 'mác', 'mân', 'mây', 'mã', 'mãn', 'mèo', 'mê', 'mô', 'môi', 'môn', 'mũi', 'mơ', 'mương', 'mười', 'mạc', 'mạch', 'mại', 'mạng', 'mạnh', 'mảng', 'mầu', 'mẫn', 'mẫu', 'mật', 'mậu', 'mắm', 'mặc', 'mễ', 'mỗ', 'mộ', 'mộc', 'mộng', 'một', 'mới', 'mỡ', 'mứt', 'mỹ', 'n', 'n1', 'n10', 'n11', 'n12', 'n13', 'n17', 'n18', 'n2', 'n21', 'n3', 'n4', 'n5', 'n6', 'n7', 'n77', 'n8', 'n9', 'n9-2', 'na', 'nai', 'nam', 'nả', 'nb3', 'ne', 'nga', 'ngang', 'ngã', 'nghi', 'nghiêm', 'nghiệp', 'nghè', 'nghé', 'nghĩ', 'nghĩa', 'nghệ', 'nghỉ', 'nghị', 'ngọc', 'ngoại', 'ngung', 'nguyên', 'nguyễn', 'nguyễn', 'nguyệt', 'ngà', 'ngàn', 'ngào', 'ngân', 'ngâu', 'ngã', 'ngô', 'ngõ', 'ngũ', 'ngư', 'ngưu', 'ngạc', 'ngạn', 'ngải', 'ngần', 'ngọ', 'ngọa', 'ngọc', 'ngọn', 'ngụy', 'ngữ', 'ngựa', 'nh3', 'nha', 'nhai', 'nhan', 'nhàn', 'nhi', 'nhiên', 'nhiêu', 'nhiếp', 'nhiếp', 'nhiễu', 'nhiệm', 'nho', 'nhu', 'nhum', 'nhung', 'nhuận', 'nhuế', 'nhuệ', 'nhuộm', 'nhà', 'nhàn', 'nhân', 'nhậm', 'nhì', 'nhơn', 'như', 'nhược', 'nhạ', 'nhạc', 'nhạn', 'nhất', 'nhậm', 'nhật', 'nhị', 'nhỏ', 'nhổn', 'nhữ', 'nhựt', 'ni', 'ninh', 'niên', 'niệm', 'noi', 'nuôi', 'nàn', 'não', 'nê', 'nón', 'nông', 'nội', 'núi', 'năm', 'năng', 'nơ', 'nước', 'nại', 'nậu', 'nền', 'nị', 'nổi', 'nỗ', 'nội', 'nộn', 'nở', 'nửa', 'nữ', 'nữa', 'o', 'oai', 'oanh', 'on', 'oánh', 'p', 'p11', 'p2', 'p4', 'paris', 'park', 'pasteur', 'pha', 'phan', 'phạm', 'phi', 'phiên', 'phiến', 'phong', 'phú', 'phúc', 'phụng', 'phách', 'phái', 'pháo', 'pháp', 'phát', 'phèn', 'phép', 'phó', 'phóng', 'phố', 'phù', 'phùng', 'phú', 'phúc', 'phương', 'phước', 'phường', 'phượng', 'phạm', 'phải', 'phấn', 'phần', 'phố', 'phổ', 'phụ', 'phục', 'phụng', 'phủ', 'phức', 'qh', 'qh2', 'quai', 'quan', 'quang', 'quán', 'que', 'quy', 'quyên', 'quỳnh', 'quyến', 'quyết', 'quyền', 'quá', 'quách', 'quán', 'quát', 'quân', 'quí', 'quốc', 'quý', 'quơ', 'quạt', 'quản', 'quảng', 'quất', 'quần', 'quận', 'quế', 'quốc', 'quới', 'quờn', 'quỳ', 'quỳnh', 'qúy', 'r', 'r2', 'ra', 'rada', 'ranh', 'rau', 'raymondienne', 'rađa', 'riêng', 'rành', 'rây', 'rèn', 'rõ', 'rùa', 'răm', 'rư', 'rươi', 'rạch', 'rạng', 'rần', 'rồng', 'rỗng', 'rừng', 'rực', 's', 's1', 's11', 's2', 's3', 's4', 's5', 's7', 's8', 's9', 'sa', 'samco', 'sang', 'sanh', 'sao', 'se', 'sen', 'si', 'sinco', 'sinh', 'siêng', 'siêu', 'song', 'soạn', 'sung', 'suối', 'sài', 'sào', 'sác', 'sách', 'sáng', 'sáp', 'sát', 'sáu', 'sâm', 'sóc', 'sông', 'số', 'súc', 'súy', 'săng', 'sĩ', 'sũ', 'sơn', 'sở', 'sư', 'sưa', 'sương', 'sảo', 'sầm', 'sậy', 'sắc', 'sắt', 'sến', 'số', 'sộp', 'sở', 'sứ', 'sử', 'sửu', 'sữa', 'sự', 'sỹ', 't', 't1', 't10', 't11', 't12', 't14', 't15', 't2', 't3', 't30', 't4', 't4a', 't4b', 't5', 't6', 't7', 't8', 't9', 'tam', 'tc12', 'tc2', 'tech', 'thai', 'tham', 'than', 'thanh', 'thao', 'thau', 'thành', 'thái', 'thánh', 'thạch', 'thạnh', 'the', 'thi', 'thiêm', 'thiên', 'thì', 'thị', 'thịnh', 'thiếc', 'thiếp', 'thiết', 'thiếu', 'thiền', 'thiều', 'thiện', 'thiệp', 'thiệt', 'thiệu', 'thoàn', 'thoại', 'thu', 'thuyên', 'thuyết', 'thuê', 'thuấn', 'thuần', 'thuận', 'thuật', 'thuốc', 'thuỷ', 'thành', 'thái', 'thám', 'thán', 'tháng', 'thánh', 'tháp', 'thân', 'thâu', 'thép', 'thê', 'thêm', 'thì', 'thích', 'thôn', 'thông', 'thổ', 'thùng', 'thùy', 'thúc', 'thúy', 'thăng', 'thơ', 'thơi', 'thơm', 'thới', 'thư', 'thương', 'thượng', 'thước', 'thường', 'thưởng', 'thược', 'thượng', 'thạch', 'thạnh', 'thảnh', 'thảo', 'thất', 'thần', 'thầy', 'thẩm', 'thận', 'thập', 'thắng', 'thế', 'thể', 'thỉnh', 'thị', 'thịnh', 'thọ', 'thỏa', 'thống', 'thổ', 'thới', 'thờ', 'thời', 'thợ', 'thụ', 'thục', 'thụy', 'thủ', 'thủy', 'thứ', 'thức', 'thừa', 'thử', 'thực', 'tinh', 'tiên', 'tiêu', 'tiền', 'tiến', 'tiếp', 'tiết', 'tiền', 'tiểu', 'tiệp', 'tk', 'tk1', 'tk10', 'tk11', 'tk12', 'tk13', 'tk14', 'tk16', 'tk2', 'tk20', 'tk21', 'tk27', 'tk4', 'tk7', 'tk8', 'tka11', 'tn02', 'tn1', 'tn13', 'toàn', 'toái', 'toại', 'toản', 'tr3', 'trai', 'trang', 'tranh', 'tràng', 'tre', 'tri', 'trinh', 'trì', 'triết', 'triều', 'triển', 'triệu', 'trong', 'trung', 'truyền', 'truyện', 'truông', 'trúc', 'trà', 'tràm', 'tràng', 'trào', 'trác', 'tráng', 'trâm', 'trân', 'trâu', 'trần', 'trãi', 'trì', 'trình', 'trí', 'trích', 'trò', 'trôi', 'trôm', 'trống', 'trùng', 'trú', 'trúc', 'trưng', 'trương', 'trực', 'trường', 'trượng', 'trạc', 'trạch', 'trại', 'trạm', 'trảng', 'trấn', 'trấu', 'trầm', 'trần', 'trắc', 'trẻ', 'trị', 'trịnh', 'trọng', 'trống', 'trỗi', 'trụ', 'trục', 'trứ', 'trừng', 'trực', 'tu', 'tulip', 'tum', 'tung', 'tuy', 'tuyến', 'tuyết', 'tuyền', 'tuyển', 'tuân', 'tuấn', 'tuệ', 'tuổi', 'tx22', 'tài', 'tàm', 'tàng', 'tá', 'tác', 'tách', 'tái', 'tám', 'tánh', 'tâm', 'tân', 'tây', 'tấn', 'tèn', 'tên', 'tình', 'tía', 'tích', 'tím', 'tín', 'tòng', 'tó', 'tô', 'tôn', 'tông', 'tổ', 'tùng', 'tú', 'túc', 'túy', 'tăng', 'tĩnh', 'tơ', 'tư', 'tương', 'tứ', 'tử', 'tự', 'tước', 'tướng', 'tường', 'tưởng', 'tượng', 'tạ', 'tạm', 'tạnh', 'tạo', 'tả', 'tản', 'tảo', 'tấn', 'tất', 'tầm', 'tần', 'tập', 'tắc', 'tắng', 'tẻ', 'tế', 'tỉnh', 'tịch', 'tịnh', 'tố', 'tốc', 'tốn', 'tống', 'tốt', 'tồn', 'tổ', 'tổng', 'tộ', 'tộc', 'tụ', 'tứ', 'tức', 'từ', 'tử', 'tự', 'tựu', 'tỵ', 'tỷ', 'u', 'ung', 'uy', 'uyên', 'v', 'ven', 'vi', 'vinh', 'viêm', 'viên', 'vĩnh', 'vị', 'viết', 'viễn', 'viện', 'việt', 'voi', 'võ', 'vu', 'vua', 'vui', 'vuông', 'vũ', 'vàng', 'vành', 'ván', 'vân', 'vì', 'vòng', 'vóc', 'vôi', 'võ', 'võng', 'vùng', 'văn', 'vĩ', 'vĩnh', 'vũ', 'vương', 'vườn', 'vượng', 'vượt', 'vạn', 'vải', 'vấn', 'vấp', 'vận', 'vật', 'vẽ', 'vệ', 'vị', 'vịnh', 'vọng', 'vỏ', 'vồng', 'vỡ', 'vực', 'vỹ', 'x4', 'xa', 'xanh', 'xá', 'xe', 'xiển', 'xiểu', 'xong', 'xoài', 'xu', 'xung', 'xuy', 'xuyên', 'xuyến', 'xuân', 'xuất', 'xxt', 'xá', 'xán', 'xây', 'xã', 'xén', 'xéo', 'xí', 'xích', 'xóm', 'xô', 'xơ', 'xưa', 'xương', 'xảo', 'xốm', 'y', 'y.e.c', 'yersin', 'yêm', 'yên', 'yến', 'yết', 'z133', 'z153', 'á', 'ái', 'án', 'áng', 'ánh', 'ân', 'âu', 'ấp', 'ích', 'ô', 'ôn', 'ông', 'úc', 'út', 'ý', 'đa', 'đai', 'đan', 'đang', 'đào', 'đại', 'đạo', 'đen', 'đinh', 'điêu', 'điện', 'đình', 'điếu', 'điềm', 'điền', 'điều', 'điểm', 'điển', 'điện', 'điệp', 'điệt', 'điệu', 'đoàn', 'đò', 'đt', 'đuống', 'đà', 'đài', 'đàm', 'đàn', 'đành', 'đào', 'đá', 'đán', 'đáng', 'đáo', 'đáy', 'đèn', 'đê', 'đìa-nam', 'đình', 'đò', 'đô', 'đôi', 'đôn', 'đông', 'đồng', 'đống', 'độ', 'đội', 'đúc', 'đăm', 'đăng', 'đằng', 'đĩnh', 'đơ', 'đường', 'đại', 'đạm', 'đạo', 'đạt', 'đản', 'đảo', 'đất', 'đầm', 'đầy', 'đẩu', 'đẫy', 'đậu', 'đắc', 'đằng', 'đặc', 'đặng', 'đế', 'đề', 'đền', 'đệm', 'đỉnh', 'địa', 'định', 'đốc', 'đối', 'đống', 'đồ', 'đồi', 'đồn', 'đồng', 'đổ', 'đổng', 'đỗ', 'độ', 'độc', 'đội', 'động', 'đục', 'đức', 'đừng', 'ơn', 'ư', 'ưu', 'ương', 'ước', 'ảnh', 'ấm', 'ấp', 'ấu', 'ống', 'ụ', 'ứng', 'ỷ']\n",
            "{'<pad>': 0, '<start>': 1, '<end>': 2, '': 3, '-': 4, '01-39': 5, '03': 6, '04': 7, '05': 8, '06': 9, '07': 10, '09': 11, '1': 12, '1-1': 13, '1-10': 14, '1-12': 15, '1-19': 16, '1-2': 17, '1-2-3': 18, '1-22': 19, '1-3': 20, '1-4': 21, '1-42': 22, '1-5': 23, '1-6': 24, '1-7': 25, '1-8': 26, '1-9': 27, '10': 28, '10-11': 29, '10-14': 30, '10-22': 31, '100': 32, '100a': 33, '101': 34, '1011': 35, '102': 36, '1021': 37, '1022': 38, '1025': 39, '103': 40, '104': 41, '1041': 42, '104a': 43, '106': 44, '107': 45, '108': 46, '1088': 47, '109': 48, '10a': 49, '10b': 50, '10c': 51, '10d': 52, '10e': 53, '10f': 54, '10g': 55, '11': 56, '11-6': 57, '11-7': 58, '110': 59, '1106': 60, '1107': 61, '111': 62, '112': 63, '114': 64, '115': 65, '116': 66, '117': 67, '118': 68, '119': 69, '11a': 70, '11b': 71, '11c': 72, '11d': 73, '11e': 74, '11l': 75, '11n': 76, '12': 77, '12-52': 78, '120': 79, '121': 80, '122': 81, '123': 82, '1234': 83, '123a': 84, '124': 85, '126': 86, '127d': 87, '128': 88, '128c': 89, '129': 90, '12a': 91, '12ab': 92, '12b': 93, '12c': 94, '12d': 95, '12e': 96, '13': 97, '130': 98, '131': 99, '1315': 100, '132': 101, '135': 102, '137': 103, '138': 104, '13a': 105, '13b': 106, '13c': 107, '14': 108, '14-1': 109, '140': 110, '141': 111, '142': 112, '143': 113, '144': 114, '145': 115, '146': 116, '147': 117, '148': 118, '149/33': 119, '14a': 120, '14b': 121, '14c': 122, '14d': 123, '14e': 124, '14f': 125, '15': 126, '15-23': 127, '150': 128, '152': 129, '152a': 130, '152b': 131, '153': 132, '154': 133, '156': 134, '157': 135, '15a': 136, '15b': 137, '15c': 138, '15d': 139, '15e': 140, '15n': 141, '16': 142, '16-1': 143, '16-10': 144, '16-18': 145, '16-21': 146, '16-25': 147, '16-37': 148, '16-7': 149, '160': 150, '161': 151, '162': 152, '163': 153, '164': 154, '167': 155, '168': 156, '16a': 157, '16b': 158, '16d': 159, '17': 160, '171': 161, '172': 162, '173': 163, '175': 164, '176': 165, '177': 166, '179': 167, '17a': 168, '17b': 169, '17c': 170, '17d': 171, '18': 172, '180': 173, '181': 174, '182': 175, '185': 176, '187': 177, '188': 178, '189': 179, '18a': 180, '18b': 181, '18c': 182, '18d': 183, '18e': 184, '19': 185, '19-10': 186, '19-16': 187, '19-32': 188, '19/1': 189, '19/12': 190, '19/5': 191, '19/5a': 192, '19/5b': 193, '19/8': 194, '191': 195, '192': 196, '197': 197, '198': 198, '19a': 199, '19b': 200, '19c': 201, '19d': 202, '19e': 203, '19f': 204, '19r': 205, '1a': 206, '1b': 207, '1c': 208, '1d': 209, '1e': 210, '1f': 211, '1k': 212, '2': 213, '2,5': 214, '2-1': 215, '2-10': 216, '2-11': 217, '2-2': 218, '2-22': 219, '2-3': 220, '2-3-1': 221, '2-3-4': 222, '2-4': 223, '2-46': 224, '2-5': 225, '2-6': 226, '2-6-7': 227, '2-64': 228, '2-7': 229, '2-8': 230, '2.5a': 231, '2/9': 232, '20': 233, '200': 234, '201': 235, '202': 236, '204': 237, '204b': 238, '205a': 239, '206': 240, '207': 241, '208': 242, '20a': 243, '21': 244, '21-19': 245, '21-25': 246, '210': 247, '210a': 248, '211': 249, '212': 250, '214': 251, '215': 252, '217': 253, '218': 254, '21a': 255, '21b': 256, '21c': 257, '21d': 258, '21e': 259, '22': 260, '22-3': 261, '22/12': 262, '220': 263, '221': 264, '222': 265, '223': 266, '224': 267, '225': 268, '226': 269, '22a': 270, '22b': 271, '23': 272, '23/10': 273, '23/9': 274, '230': 275, '231': 276, '232': 277, '233': 278, '234': 279, '236': 280, '238': 281, '239': 282, '23a': 283, '23b': 284, '24': 285, '244': 286, '245': 287, '247': 288, '248/1': 289, '24a': 290, '24b': 291, '25': 292, '251': 293, '25a': 294, '25b': 295, '26': 296, '26-1': 297, '26-1a': 298, '26-3': 299, '26/3': 300, '26/5': 301, '261': 302, '263': 303, '265': 304, '266': 305, '267': 306, '267a': 307, '267b': 308, '26a': 309, '27': 310, '27-2': 311, '27/3': 312, '27/7': 313, '270': 314, '270a': 315, '270b': 316, '274': 317, '275': 318, '278': 319, '279': 320, '27a': 321, '27c': 322, '28': 323, '281': 324, '284': 325, '286': 326, '288': 327, '28a': 328, '29': 329, '290': 330, '295': 331, '296': 332, '297': 333, '299': 334, '2a': 335, '2b': 336, '2c': 337, '2d': 338, '2e': 339, '2f': 340, '2g': 341, '2k': 342, '3': 343, '3-1': 344, '3-10': 345, '3-19': 346, '3-2-1': 347, '3-26': 348, '3-3': 349, '3-31': 350, '3-33': 351, '3-34': 352, '3-36': 353, '3-4': 354, '3-4-5': 355, '3-4a': 356, '3-7': 357, '3.5': 358, '3/2': 359, '30': 360, '30-1': 361, '30/4': 362, '300': 363, '301': 364, '304': 365, '3052a': 366, '308': 367, '30a': 368, '30b': 369, '30c': 370, '31': 371, '31-4': 372, '311': 373, '312': 374, '3158b': 375, '316': 376, '318': 377, '319': 378, '31a': 379, '31b': 380, '31c': 381, '31d': 382, '31e': 383, '31f': 384, '32': 385, '320': 386, '320a': 387, '321': 388, '32a': 389, '32b': 390, '32c': 391, '32d': 392, '33': 393, '330': 394, '332': 395, '339': 396, '33a': 397, '33b': 398, '33c': 399, '34': 400, '34a': 401, '34b': 402, '34c': 403, '35': 404, '353': 405, '355': 406, '359': 407, '35a': 408, '36': 409, '360': 410, '363': 411, '364': 412, '3643c': 413, '365': 414, '367': 415, '368': 416, '37': 417, '372': 418, '374': 419, '378': 420, '379': 421, '37a': 422, '38': 423, '385': 424, '387': 425, '389': 426, '38a': 427, '38b': 428, '39': 429, '397': 430, '397b': 431, '39b': 432, '3a': 433, '3b': 434, '3c': 435, '3d': 436, '3j': 437, '4': 438, '4-1': 439, '4-2': 440, '4-2-1': 441, '4-22': 442, '4-23': 443, '4-24': 444, '4-3': 445, '4-37': 446, '4-38': 447, '4-4': 448, '4-5': 449, '4-6': 450, '4-9': 451, '40': 452, '400': 453, '406': 454, '409': 455, '40b': 456, '41': 457, '411': 458, '411c': 459, '414': 460, '416': 461, '418': 462, '419': 463, '41a': 464, '41b': 465, '42': 466, '420': 467, '421b': 468, '422': 469, '422a': 470, '422b': 471, '423': 472, '424': 473, '426': 474, '427': 475, '429': 476, '42a': 477, '43': 478, '430': 479, '435': 480, '436': 481, '43b': 482, '44': 483, '440': 484, '441': 485, '442': 486, '446': 487, '447': 488, '447b': 489, '448': 490, '449': 491, '45': 492, '454': 493, '455': 494, '456': 495, '457': 496, '458': 497, '46': 498, '463': 499, '464': 500, '46a': 501, '46b': 502, '47': 503, '473': 504, '475': 505, '479': 506, '48': 507, '487': 508, '489': 509, '48a': 510, '48b': 511, '49': 512, '490': 513, '491': 514, '494': 515, '495': 516, '499': 517, '49a': 518, '49b': 519, '49c': 520, '4a': 521, '4b': 522, '4c': 523, '4d': 524, '4e': 525, '4f': 526, '5': 527, '5-1': 528, '5-10-11': 529, '5-11-12': 530, '5-12': 531, '5-2': 532, '5-26': 533, '5-27': 534, '5-3': 535, '5-31': 536, '5-6': 537, '5-8': 538, '50': 539, '50/27': 540, '500': 541, '506': 542, '508': 543, '50a': 544, '50b': 545, '50c': 546, '50d': 547, '51': 548, '511': 549, '512': 550, '517': 551, '519': 552, '52': 553, '520': 554, '52a': 555, '52b': 556, '53': 557, '530': 558, '531': 559, '538': 560, '539': 561, '53a': 562, '53b': 563, '53c': 564, '54': 565, '540': 566, '545': 567, '546': 568, '54a': 569, '55': 570, '556': 571, '55a': 572, '55b': 573, '56': 574, '567': 575, '568': 576, '57': 577, '570': 578, '572': 579, '577': 580, '57a': 581, '57b': 582, '57c': 583, '58': 584, '588': 585, '58c': 586, '59': 587, '591': 588, '595': 589, '597': 590, '59b': 591, '59c': 592, '5a': 593, '5b': 594, '5c': 595, '5d': 596, '5e': 597, '5f': 598, '5k': 599, '5n': 600, '5x': 601, '6': 602, '6-1': 603, '6-17': 604, '6-2': 605, '6-2-5': 606, '6-22': 607, '6-23': 608, '6-27': 609, '6-3': 610, '6-38': 611, '60': 612, '606': 613, '60b': 614, '61': 615, '614': 616, '618': 617, '61b': 618, '62': 619, '622': 620, '623': 621, '624': 622, '626': 623, '628': 624, '63': 625, '635': 626, '64': 627, '640': 628, '643': 629, '647': 630, '65': 631, '652': 632, '659': 633, '66': 634, '668': 635, '67': 636, '671': 637, '672': 638, '68': 639, '680': 640, '688': 641, '69': 642, '6a': 643, '6b': 644, '6c': 645, '6d': 646, '6e': 647, '6f': 648, '6t': 649, '7': 650, '7-': 651, '7-1': 652, '7-13': 653, '7-17': 654, '7-2': 655, '7-25': 656, '7-3': 657, '7-34': 658, '7-4': 659, '7-5': 660, '7-50': 661, '7-50b': 662, '7-6': 663, '7-8': 664, '7-9': 665, '70': 666, '702': 667, '708': 668, '70a': 669, '70b': 670, '71': 671, '711': 672, '715': 673, '717': 674, '719': 675, '71a': 676, '72': 677, '73': 678, '734': 679, '74': 680, '741': 681, '747': 682, '75': 683, '76': 684, '762': 685, '767': 686, '769': 687, '77': 688, '78': 689, '783': 690, '785': 691, '787': 692, '79': 693, '791': 694, '793': 695, '7a': 696, '7b': 697, '7c': 698, '7d': 699, '7e': 700, '7l': 701, '7p': 702, '8': 703, '8-1': 704, '8-11': 705, '8-18': 706, '8-22': 707, '8-28': 708, '8-3': 709, '8-38': 710, '8-4': 711, '8-9': 712, '8/3': 713, '80': 714, '800a': 715, '808': 716, '80b': 717, '81': 718, '811': 719, '816': 720, '817': 721, '817a': 722, '82': 723, '822': 724, '823': 725, '824': 726, '825': 727, '826': 728, '826c': 729, '827': 730, '83': 731, '830': 732, '830b': 733, '833': 734, '833b': 735, '835': 736, '835a': 737, '835b': 738, '835c': 739, '835d': 740, '836': 741, '84': 742, '842': 743, '843': 744, '84b': 745, '85': 746, '86': 747, '865': 748, '866': 749, '87': 750, '87a': 751, '88': 752, '882': 753, '89': 754, '892': 755, '898': 756, '8a': 757, '8b': 758, '8c': 759, '8d': 760, '8g': 761, '8k': 762, '9': 763, '9-1': 764, '9-17': 765, '9-2': 766, '9-34': 767, '9-48': 768, '9-5': 769, '9-55': 770, '902': 771, '904': 772, '907': 773, '909': 774, '91': 775, '911': 776, '915': 777, '916': 778, '92': 779, '93': 780, '933': 781, '94': 782, '95': 783, '96': 784, '963': 785, '97': 786, '970': 787, '976': 788, '98': 789, '980': 790, '99': 791, '990': 792, '9a': 793, '9b': 794, '9f': 795, '9m': 796, 'a': 797, 'a-3': 798, 'a1': 799, 'a10': 800, 'a2': 801, 'a3': 802, 'a35': 803, 'a4': 804, 'a5': 805, 'a6': 806, 'a61': 807, 'a7': 808, 'a8': 809, 'a9': 810, 'alexandre': 811, 'am': 812, 'an': 813, 'anh': 814, 'ao': 815, 'b': 816, 'b1': 817, 'b10': 818, 'b11': 819, 'b12': 820, 'b14': 821, 'b2': 822, 'b3': 823, 'b37': 824, 'b4': 825, 'b5': 826, 'b6': 827, 'b7': 828, 'ba': 829, 'bang': 830, 'bao': 831, 'bà': 832, 'bài': 833, 'bàu': 834, 'bách': 835, 'bạc': 836, 'bạch': 837, 'be': 838, 'bi': 839, 'bia': 840, 'binh': 841, 'biên': 842, 'biều': 843, 'biểu': 844, 'bn1': 845, 'buồm': 846, 'bùi': 847, 'buồm': 848, 'buội': 849, 'bà': 850, 'bài': 851, 'bàn': 852, 'bàng': 853, 'bành': 854, 'bàu': 855, 'bá': 856, 'bác': 857, 'bái': 858, 'bán': 859, 'bánh': 860, 'bát': 861, 'bân': 862, 'bây': 863, 'bãi': 864, 'bè': 865, 'bé': 866, 'bê': 867, 'bến': 868, 'bình': 869, 'bí': 870, 'bích': 871, 'bính': 872, 'bò': 873, 'bông': 874, 'bồ': 875, 'bột': 876, 'bùi': 877, 'bùng': 878, 'bún': 879, 'búp': 880, 'bút': 881, 'bơ': 882, 'bơi': 883, 'bưng': 884, 'bương': 885, 'bươu': 886, 'bường': 887, 'bưởi': 888, 'bạc': 889, 'bạch': 890, 'bạng': 891, 'bạt': 892, 'bản': 893, 'bảng': 894, 'bảo': 895, 'bảy': 896, 'bầu': 897, 'bậc': 898, 'bật': 899, 'bắc': 900, 'bằng': 901, 'bặc': 902, 'bế': 903, 'bến': 904, 'bền': 905, 'bệ': 906, 'bỉnh': 907, 'bị': 908, 'bốc': 909, 'bối': 910, 'bồ': 911, 'bộ': 912, 'bộc': 913, 'bội': 914, 'bột': 915, 'bờ': 916, 'bụt': 917, 'bứa': 918, 'bức': 919, 'bửu': 920, 'c': 921, 'c1': 922, 'c10': 923, 'c11': 924, 'c12': 925, 'c13': 926, 'c18': 927, 'c19': 928, 'c2': 929, 'c22': 930, 'c27': 931, 'c3': 932, 'c4': 933, 'c5': 934, 'c6': 935, 'c65': 936, 'c7': 937, 'c7b': 938, 'c7c': 939, 'c8': 940, 'c9': 941, 'ca': 942, 'cai': 943, 'calmette': 944, 'cam': 945, 'camex': 946, 'can': 947, 'cang': 948, 'canh': 949, 'cao': 950, 'cau': 951, 'cát': 952, 'cả': 953, 'cc2': 954, 'cc3': 955, 'cc5': 956, 'cd2': 957, 'cd3': 958, 'chai': 959, 'chi': 960, 'chinh': 961, 'chiêm': 962, 'chiêu': 963, 'chiếc': 964, 'chiến': 965, 'chiếu': 966, 'chiểu': 967, 'chu': 968, 'chung': 969, 'chuyên': 970, 'chuông': 971, 'chúc': 972, 'chuẩn': 973, 'chuối': 974, 'chàng': 975, 'chác': 976, 'chánh': 977, 'cháo': 978, 'chân': 979, 'châu': 980, 'chè': 981, 'chèo': 982, 'chì': 983, 'chí': 984, 'chích': 985, 'chính': 986, 'chùa': 987, 'chú': 988, 'chúa': 989, 'chúc': 990, 'chĩnh': 991, 'chợ': 992, 'chương': 993, 'chưởng': 994, 'chạo': 995, 'chả': 996, 'chấn': 997, 'chất': 998, 'chẩm': 999, 'chế': 1000, 'chỉ': 1001, 'chợ': 1002, 'chủ': 1003, 'chử': 1004, 'cn1': 1005, 'cn10': 1006, 'cn11': 1007, 'cn12': 1008, 'cn13': 1009, 'cn2': 1010, 'cn3': 1011, 'cn4': 1012, 'cn5': 1013, 'cn6': 1014, 'cn7': 1015, 'cn8': 1016, 'cn9': 1017, 'con': 1018, 'cò': 1019, 'ct.': 1020, 'ct01': 1021, 'cu': 1022, 'cung': 1023, 'cá': 1024, 'các': 1025, 'cách': 1026, 'cái': 1027, 'cám': 1028, 'cánh': 1029, 'cáp': 1030, 'cát': 1031, 'cân': 1032, 'câu': 1033, 'cây': 1034, 'cầu': 1035, 'cấn': 1036, 'cận': 1037, 'có': 1038, 'cót': 1039, 'cô': 1040, 'côi': 1041, 'côn': 1042, 'công': 1043, 'cống': 1044, 'cù': 1045, 'cùng': 1046, 'cúc': 1047, 'căn': 1048, 'cơ': 1049, 'cư': 1050, 'cương': 1051, 'cửa': 1052, 'cường': 1053, 'cả': 1054, 'cảm': 1055, 'cảng': 1056, 'cảnh': 1057, 'cấm': 1058, 'cấn': 1059, 'cầm': 1060, 'cần': 1061, 'cầu': 1062, 'cẩm': 1063, 'cẩn': 1064, 'cọ': 1065, 'cố': 1066, 'cốc': 1067, 'cốm': 1068, 'cống': 1069, 'cồ': 1070, 'cổ': 1071, 'cổng': 1072, 'cội': 1073, 'cộng': 1074, 'cột': 1075, 'cờ': 1076, 'củ': 1077, 'của': 1078, 'củi': 1079, 'củng': 1080, 'cừ': 1081, 'cử': 1082, 'cửa': 1083, 'cửu': 1084, 'cự': 1085, 'd': 1086, 'd1': 1087, 'd10': 1088, 'd100': 1089, 'd11': 1090, 'd11b': 1091, 'd12': 1092, 'd13': 1093, 'd14': 1094, 'd14a': 1095, 'd14b': 1096, 'd15': 1097, 'd16': 1098, 'd17': 1099, 'd18': 1100, 'd19': 1101, 'd1a': 1102, 'd2': 1103, 'd20': 1104, 'd21': 1105, 'd22': 1106, 'd23': 1107, 'd24': 1108, 'd26': 1109, 'd2a': 1110, 'd2d': 1111, 'd3': 1112, 'd35': 1113, 'd3a': 1114, 'd3b': 1115, 'd4': 1116, 'd400': 1117, 'd436': 1118, 'd44': 1119, 'd4a': 1120, 'd5': 1121, 'd52': 1122, 'd5a': 1123, 'd5b': 1124, 'd6': 1125, 'd64': 1126, 'd6a': 1127, 'd7': 1128, 'd8': 1129, 'd9': 1130, 'd990': 1131, 'd9a': 1132, 'da': 1133, 'dal': 1134, 'danh': 1135, 'dc1': 1136, 'dc11': 1137, 'dc13': 1138, 'dc2': 1139, 'dc3': 1140, 'dc4': 1141, 'dc5': 1142, 'dc6': 1143, 'dc7': 1144, 'dc8': 1145, 'dc9': 1146, 'dct1': 1147, 'dct5': 1148, 'dct7': 1149, 'dct9': 1150, 'dd10': 1151, 'dd11': 1152, 'dd12': 1153, 'dd2': 1154, 'dd4': 1155, 'dd4-1': 1156, 'dd4-2': 1157, 'dd5': 1158, 'dd6': 1159, 'dd6-1': 1160, 'dd7': 1161, 'dd7-1': 1162, 'dd9': 1163, 'dh5': 1164, 'di': 1165, 'diên': 1166, 'diêu': 1167, 'diều': 1168, 'diễn': 1169, 'diện': 1170, 'diệp': 1171, 'diệu': 1172, 'dk4': 1173, 'dn': 1174, 'dn10': 1175, 'dn11': 1176, 'dn12': 1177, 'dn13': 1178, 'dn2': 1179, 'dn4': 1180, 'dn5': 1181, 'dn6': 1182, 'dn7': 1183, 'dn8': 1184, 'dn9': 1185, 'do': 1186, 'doanh': 1187, 'doãn': 1188, 'dòng': 1189, 'dtc': 1190, 'dtc2': 1191, 'du': 1192, 'dung': 1193, 'duy': 1194, 'duyên': 1195, 'duyện': 1196, 'duẩn': 1197, 'duật': 1198, 'dx': 1199, 'dài': 1200, 'dân': 1201, 'dây': 1202, 'dã': 1203, 'dền': 1204, 'dùng': 1205, 'dĩ': 1206, 'dĩa': 1207, 'dũng': 1208, 'dư': 1209, 'dưa': 1210, 'dương': 1211, 'dừa': 1212, 'dưỡng': 1213, 'dược': 1214, 'dạ': 1215, 'dạng': 1216, 'dầu': 1217, 'dật': 1218, 'dậu': 1219, 'dậy': 1220, 'dế': 1221, 'dền': 1222, 'dị': 1223, 'dịch': 1224, 'dốc': 1225, 'dục': 1226, 'dừa': 1227, 'dữ': 1228, 'dực': 1229, 'e': 1230, 'e12': 1231, 'e14': 1232, 'e2': 1233, 'e32': 1234, 'e38': 1235, 'e5': 1236, 'e50': 1237, 'ec': 1238, 'einstein': 1239, 'f': 1240, 'f1': 1241, 'f11': 1242, 'g': 1243, 'g1': 1244, 'g10': 1245, 'g11': 1246, 'g14': 1247, 'g2': 1248, 'g3': 1249, 'g4': 1250, 'g5': 1251, 'g6': 1252, 'g7': 1253, 'g8': 1254, 'g9': 1255, 'ga': 1256, 'gai': 1257, 'ghe': 1258, 'gia': 1259, 'giai': 1260, 'giang': 1261, 'giao': 1262, 'giám': 1263, 'giảng': 1264, 'giuộc': 1265, 'già': 1266, 'giàu': 1267, 'giác': 1268, 'giám': 1269, 'giáng': 1270, 'giáo': 1271, 'giáp': 1272, 'giây': 1273, 'giã': 1274, 'gióng': 1275, 'giót': 1276, 'giải': 1277, 'giản': 1278, 'giảng': 1279, 'giấy': 1280, 'giầy': 1281, 'giếng': 1282, 'giỏi': 1283, 'giống': 1284, 'giồng': 1285, 'giờ': 1286, 'giữ': 1287, 'giữa': 1288, 'go': 1289, 'gò': 1290, 'guột': 1291, 'gà': 1292, 'gánh': 1293, 'gò': 1294, 'gót': 1295, 'gõ': 1296, 'gạch': 1297, 'gạo': 1298, 'gấm': 1299, 'gầm': 1300, 'gốc': 1301, 'gốm': 1302, 'gỗ': 1303, 'h': 1304, 'h1': 1305, 'h10': 1306, 'h2': 1307, 'hai': 1308, 'hang': 1309, 'hà': 1310, 'hàn': 1311, 'hàng': 1312, 'hành': 1313, 'hải': 1314, 'hạ': 1315, 'hb2': 1316, 'hi': 1317, 'him': 1318, 'hiến': 1319, 'hiếu': 1320, 'hiền': 1321, 'hiển': 1322, 'hiểu': 1323, 'hiện': 1324, 'hiệp': 1325, 'hiệu': 1326, 'hn1': 1327, 'hoa': 1328, 'hoan': 1329, 'hom': 1330, 'hoà': 1331, 'hoài': 1332, 'hoàn': 1333, 'hoàng': 1334, 'hoành': 1335, 'hoạch': 1336, 'hu': 1337, 'huy': 1338, 'huyên': 1339, 'huyền': 1340, 'huyền': 1341, 'huyện': 1342, 'huân': 1343, 'huê': 1344, 'huế': 1345, 'huấn': 1346, 'huế': 1347, 'huề': 1348, 'huệ': 1349, 'huống': 1350, 'huỳnh': 1351, 'hy': 1352, 'hà': 1353, 'hài': 1354, 'hàm': 1355, 'hàn': 1356, 'hàng': 1357, 'hành': 1358, 'hào': 1359, 'hán': 1360, 'háo': 1361, 'hát': 1362, 'hân': 1363, 'hãn': 1364, 'hè': 1365, 'hên': 1366, 'hòa': 1367, 'hòe': 1368, 'hòm': 1369, 'hóa': 1370, 'hóc': 1371, 'hổ': 1372, 'hội': 1373, 'hùng': 1374, 'húc': 1375, 'hưng': 1376, 'hưu': 1377, 'hương': 1378, 'hướng': 1379, 'hướng': 1380, 'hưởng': 1381, 'hạ': 1382, 'hạc': 1383, 'hạnh': 1384, 'hạo': 1385, 'hạt': 1386, 'hải': 1387, 'hảo': 1388, 'hầu': 1389, 'hậu': 1390, 'hắc': 1391, 'hằng': 1392, 'hẹ': 1393, 'hẻm': 1394, 'họa': 1395, 'học': 1396, 'hỏa': 1397, 'hố': 1398, 'hối': 1399, 'hồ': 1400, 'hồi': 1401, 'hồng': 1402, 'hổ': 1403, 'hộ': 1404, 'hội': 1405, 'hộp': 1406, 'hới': 1407, 'hớn': 1408, 'hợp': 1409, 'hụ': 1410, 'hữu': 1411, 'hỷ': 1412, 'i': 1413, 'iii': 1414, 'ích': 1415, 'jd12': 1416, 'k': 1417, 'k1': 1418, 'k1b': 1419, 'k2': 1420, 'k22': 1421, 'k3': 1422, 'k300': 1423, 'k32': 1424, 'k4': 1425, 'k800a': 1426, 'kao': 1427, 'kdc': 1428, 'keo': 1429, 'kha': 1430, 'khai': 1431, 'khang': 1432, 'khanh': 1433, 'khay': 1434, 'khánh': 1435, 'khi': 1436, 'khiêm': 1437, 'khiêu': 1438, 'khiết': 1439, 'khiếu': 1440, 'kho': 1441, 'khoa': 1442, 'khoai': 1443, 'khoan': 1444, 'khoang': 1445, 'khoách': 1446, 'khoái': 1447, 'khoát': 1448, 'khu': 1449, 'khuyê': 1450, 'khuyến': 1451, 'khuê': 1452, 'khuông': 1453, 'khuất': 1454, 'khái': 1455, 'kháng': 1456, 'khánh': 1457, 'khát': 1458, 'khâm': 1459, 'khéo': 1460, 'khê': 1461, 'khôi': 1462, 'khổng': 1463, 'khúc': 1464, 'khơi': 1465, 'khương': 1466, 'khạ': 1467, 'khả': 1468, 'khải': 1469, 'khảm': 1470, 'khẩu': 1471, 'khắc': 1472, 'khỏe': 1473, 'khối': 1474, 'khổng': 1475, 'khởi': 1476, 'kim': 1477, 'kinh': 1478, 'kiên': 1479, 'kiêu': 1480, 'kiếm': 1481, 'kiến': 1482, 'kiếp': 1483, 'kiều': 1484, 'kiển': 1485, 'kiểng': 1486, 'kiểu': 1487, 'kiệm': 1488, 'kiện': 1489, 'kiệp': 1490, 'kiệt': 1491, 'kiệu': 1492, 'kon': 1493, 'kontum': 1494, 'kp': 1495, 'kp3': 1496, 'kp5': 1497, 'kt': 1498, 'kt19': 1499, 'kt20': 1500, 'kt21': 1501, 'kt4': 1502, 'kt8': 1503, 'ky': 1504, 'kỳ': 1505, 'kênh': 1506, 'kính': 1507, 'ký': 1508, 'kẻ': 1509, 'kế': 1510, 'kết': 1511, 'kỉnh': 1512, 'kỳ': 1513, 'kỵ': 1514, 'kỷ': 1515, 'l25': 1516, 'l6': 1517, 'la': 1518, 'lai': 1519, 'lam': 1520, 'lan': 1521, 'lang': 1522, 'lao': 1523, 'láng': 1524, 'lãng': 1525, 'le': 1526, 'linh': 1527, 'liêm': 1528, 'liên': 1529, 'liêu': 1530, 'liễu': 1531, 'liệt': 1532, 'liền': 1533, 'liễu': 1534, 'liệp': 1535, 'liệt': 1536, 'liệu': 1537, 'loa': 1538, 'long': 1539, 'lu': 1540, 'lung': 1541, 'luyện': 1542, 'luông': 1543, 'luật': 1544, 'ly': 1545, 'lý': 1546, 'là': 1547, 'lài': 1548, 'làng': 1549, 'lành': 1550, 'lá': 1551, 'lách': 1552, 'lái': 1553, 'láng': 1554, 'lâm': 1555, 'lân': 1556, 'lâu': 1557, 'lã': 1558, 'lãm': 1559, 'lãn': 1560, 'lãng': 1561, 'lãnh': 1562, 'lão': 1563, 'lê': 1564, 'lên': 1565, 'lệnh': 1566, 'lò': 1567, 'lô': 1568, 'lôi': 1569, 'lỗi': 1570, 'lộ': 1571, 'lùng': 1572, 'lý': 1573, 'lăng': 1574, 'lĩnh': 1575, 'lũ': 1576, 'lũng': 1577, 'lũy': 1578, 'lơ': 1579, 'lơn': 1580, 'lư': 1581, 'lưu': 1582, 'lương': 1583, 'lường': 1584, 'lược': 1585, 'lượng': 1586, 'lạc': 1587, 'lại': 1588, 'lấp': 1589, 'lầu': 1590, 'lập': 1591, 'lắm': 1592, 'lắng': 1593, 'lễ': 1594, 'lệ': 1595, 'lịch': 1596, 'lỗ': 1597, 'lỗi': 1598, 'lộ': 1599, 'lộc': 1600, 'lội': 1601, 'lộng': 1602, 'lớn': 1603, 'lở': 1604, 'lợi': 1605, 'lụa': 1606, 'lục': 1607, 'lủ': 1608, 'lừ': 1609, 'lừa': 1610, 'lửa': 1611, 'lữ': 1612, 'lực': 1613, 'lựu': 1614, 'm': 1615, 'm1': 1616, 'm10': 1617, 'm12': 1618, 'm16': 1619, 'm3': 1620, 'm34': 1621, 'm4': 1622, 'm5': 1623, 'm50': 1624, 'm6': 1625, 'm7': 1626, 'm8': 1627, 'mai': 1628, 'man': 1629, 'mác': 1630, 'mã': 1631, 'me': 1632, 'mi': 1633, 'minh': 1634, 'miên': 1635, 'miêu': 1636, 'miếu': 1637, 'miến': 1638, 'miếng': 1639, 'miếu': 1640, 'miễn': 1641, 'miễu': 1642, 'muối': 1643, 'muồng': 1644, 'mài': 1645, 'mành': 1646, 'mác': 1647, 'mân': 1648, 'mây': 1649, 'mã': 1650, 'mãn': 1651, 'mèo': 1652, 'mê': 1653, 'mô': 1654, 'môi': 1655, 'môn': 1656, 'mũi': 1657, 'mơ': 1658, 'mương': 1659, 'mười': 1660, 'mạc': 1661, 'mạch': 1662, 'mại': 1663, 'mạng': 1664, 'mạnh': 1665, 'mảng': 1666, 'mầu': 1667, 'mẫn': 1668, 'mẫu': 1669, 'mật': 1670, 'mậu': 1671, 'mắm': 1672, 'mặc': 1673, 'mễ': 1674, 'mỗ': 1675, 'mộ': 1676, 'mộc': 1677, 'mộng': 1678, 'một': 1679, 'mới': 1680, 'mỡ': 1681, 'mứt': 1682, 'mỹ': 1683, 'n': 1684, 'n1': 1685, 'n10': 1686, 'n11': 1687, 'n12': 1688, 'n13': 1689, 'n17': 1690, 'n18': 1691, 'n2': 1692, 'n21': 1693, 'n3': 1694, 'n4': 1695, 'n5': 1696, 'n6': 1697, 'n7': 1698, 'n77': 1699, 'n8': 1700, 'n9': 1701, 'n9-2': 1702, 'na': 1703, 'nai': 1704, 'nam': 1705, 'nả': 1706, 'nb3': 1707, 'ne': 1708, 'nga': 1709, 'ngang': 1710, 'ngã': 1711, 'nghi': 1712, 'nghiêm': 1713, 'nghiệp': 1714, 'nghè': 1715, 'nghé': 1716, 'nghĩ': 1717, 'nghĩa': 1718, 'nghệ': 1719, 'nghỉ': 1720, 'nghị': 1721, 'ngọc': 1722, 'ngoại': 1723, 'ngung': 1724, 'nguyên': 1725, 'nguyễn': 1726, 'nguyễn': 1727, 'nguyệt': 1728, 'ngà': 1729, 'ngàn': 1730, 'ngào': 1731, 'ngân': 1732, 'ngâu': 1733, 'ngã': 1734, 'ngô': 1735, 'ngõ': 1736, 'ngũ': 1737, 'ngư': 1738, 'ngưu': 1739, 'ngạc': 1740, 'ngạn': 1741, 'ngải': 1742, 'ngần': 1743, 'ngọ': 1744, 'ngọa': 1745, 'ngọc': 1746, 'ngọn': 1747, 'ngụy': 1748, 'ngữ': 1749, 'ngựa': 1750, 'nh3': 1751, 'nha': 1752, 'nhai': 1753, 'nhan': 1754, 'nhàn': 1755, 'nhi': 1756, 'nhiên': 1757, 'nhiêu': 1758, 'nhiếp': 1759, 'nhiếp': 1760, 'nhiễu': 1761, 'nhiệm': 1762, 'nho': 1763, 'nhu': 1764, 'nhum': 1765, 'nhung': 1766, 'nhuận': 1767, 'nhuế': 1768, 'nhuệ': 1769, 'nhuộm': 1770, 'nhà': 1771, 'nhàn': 1772, 'nhân': 1773, 'nhậm': 1774, 'nhì': 1775, 'nhơn': 1776, 'như': 1777, 'nhược': 1778, 'nhạ': 1779, 'nhạc': 1780, 'nhạn': 1781, 'nhất': 1782, 'nhậm': 1783, 'nhật': 1784, 'nhị': 1785, 'nhỏ': 1786, 'nhổn': 1787, 'nhữ': 1788, 'nhựt': 1789, 'ni': 1790, 'ninh': 1791, 'niên': 1792, 'niệm': 1793, 'noi': 1794, 'nuôi': 1795, 'nàn': 1796, 'não': 1797, 'nê': 1798, 'nón': 1799, 'nông': 1800, 'nội': 1801, 'núi': 1802, 'năm': 1803, 'năng': 1804, 'nơ': 1805, 'nước': 1806, 'nại': 1807, 'nậu': 1808, 'nền': 1809, 'nị': 1810, 'nổi': 1811, 'nỗ': 1812, 'nội': 1813, 'nộn': 1814, 'nở': 1815, 'nửa': 1816, 'nữ': 1817, 'nữa': 1818, 'o': 1819, 'oai': 1820, 'oanh': 1821, 'on': 1822, 'oánh': 1823, 'p': 1824, 'p11': 1825, 'p2': 1826, 'p4': 1827, 'paris': 1828, 'park': 1829, 'pasteur': 1830, 'pha': 1831, 'phan': 1832, 'phạm': 1833, 'phi': 1834, 'phiên': 1835, 'phiến': 1836, 'phong': 1837, 'phú': 1838, 'phúc': 1839, 'phụng': 1840, 'phách': 1841, 'phái': 1842, 'pháo': 1843, 'pháp': 1844, 'phát': 1845, 'phèn': 1846, 'phép': 1847, 'phó': 1848, 'phóng': 1849, 'phố': 1850, 'phù': 1851, 'phùng': 1852, 'phú': 1853, 'phúc': 1854, 'phương': 1855, 'phước': 1856, 'phường': 1857, 'phượng': 1858, 'phạm': 1859, 'phải': 1860, 'phấn': 1861, 'phần': 1862, 'phố': 1863, 'phổ': 1864, 'phụ': 1865, 'phục': 1866, 'phụng': 1867, 'phủ': 1868, 'phức': 1869, 'qh': 1870, 'qh2': 1871, 'quai': 1872, 'quan': 1873, 'quang': 1874, 'quán': 1875, 'que': 1876, 'quy': 1877, 'quyên': 1878, 'quỳnh': 1879, 'quyến': 1880, 'quyết': 1881, 'quyền': 1882, 'quá': 1883, 'quách': 1884, 'quán': 1885, 'quát': 1886, 'quân': 1887, 'quí': 1888, 'quốc': 1889, 'quý': 1890, 'quơ': 1891, 'quạt': 1892, 'quản': 1893, 'quảng': 1894, 'quất': 1895, 'quần': 1896, 'quận': 1897, 'quế': 1898, 'quốc': 1899, 'quới': 1900, 'quờn': 1901, 'quỳ': 1902, 'quỳnh': 1903, 'qúy': 1904, 'r': 1905, 'r2': 1906, 'ra': 1907, 'rada': 1908, 'ranh': 1909, 'rau': 1910, 'raymondienne': 1911, 'rađa': 1912, 'riêng': 1913, 'rành': 1914, 'rây': 1915, 'rèn': 1916, 'rõ': 1917, 'rùa': 1918, 'răm': 1919, 'rư': 1920, 'rươi': 1921, 'rạch': 1922, 'rạng': 1923, 'rần': 1924, 'rồng': 1925, 'rỗng': 1926, 'rừng': 1927, 'rực': 1928, 's': 1929, 's1': 1930, 's11': 1931, 's2': 1932, 's3': 1933, 's4': 1934, 's5': 1935, 's7': 1936, 's8': 1937, 's9': 1938, 'sa': 1939, 'samco': 1940, 'sang': 1941, 'sanh': 1942, 'sao': 1943, 'se': 1944, 'sen': 1945, 'si': 1946, 'sinco': 1947, 'sinh': 1948, 'siêng': 1949, 'siêu': 1950, 'song': 1951, 'soạn': 1952, 'sung': 1953, 'suối': 1954, 'sài': 1955, 'sào': 1956, 'sác': 1957, 'sách': 1958, 'sáng': 1959, 'sáp': 1960, 'sát': 1961, 'sáu': 1962, 'sâm': 1963, 'sóc': 1964, 'sông': 1965, 'số': 1966, 'súc': 1967, 'súy': 1968, 'săng': 1969, 'sĩ': 1970, 'sũ': 1971, 'sơn': 1972, 'sở': 1973, 'sư': 1974, 'sưa': 1975, 'sương': 1976, 'sảo': 1977, 'sầm': 1978, 'sậy': 1979, 'sắc': 1980, 'sắt': 1981, 'sến': 1982, 'số': 1983, 'sộp': 1984, 'sở': 1985, 'sứ': 1986, 'sử': 1987, 'sửu': 1988, 'sữa': 1989, 'sự': 1990, 'sỹ': 1991, 't': 1992, 't1': 1993, 't10': 1994, 't11': 1995, 't12': 1996, 't14': 1997, 't15': 1998, 't2': 1999, 't3': 2000, 't30': 2001, 't4': 2002, 't4a': 2003, 't4b': 2004, 't5': 2005, 't6': 2006, 't7': 2007, 't8': 2008, 't9': 2009, 'tam': 2010, 'tc12': 2011, 'tc2': 2012, 'tech': 2013, 'thai': 2014, 'tham': 2015, 'than': 2016, 'thanh': 2017, 'thao': 2018, 'thau': 2019, 'thành': 2020, 'thái': 2021, 'thánh': 2022, 'thạch': 2023, 'thạnh': 2024, 'the': 2025, 'thi': 2026, 'thiêm': 2027, 'thiên': 2028, 'thì': 2029, 'thị': 2030, 'thịnh': 2031, 'thiếc': 2032, 'thiếp': 2033, 'thiết': 2034, 'thiếu': 2035, 'thiền': 2036, 'thiều': 2037, 'thiện': 2038, 'thiệp': 2039, 'thiệt': 2040, 'thiệu': 2041, 'thoàn': 2042, 'thoại': 2043, 'thu': 2044, 'thuyên': 2045, 'thuyết': 2046, 'thuê': 2047, 'thuấn': 2048, 'thuần': 2049, 'thuận': 2050, 'thuật': 2051, 'thuốc': 2052, 'thuỷ': 2053, 'thành': 2054, 'thái': 2055, 'thám': 2056, 'thán': 2057, 'tháng': 2058, 'thánh': 2059, 'tháp': 2060, 'thân': 2061, 'thâu': 2062, 'thép': 2063, 'thê': 2064, 'thêm': 2065, 'thì': 2066, 'thích': 2067, 'thôn': 2068, 'thông': 2069, 'thổ': 2070, 'thùng': 2071, 'thùy': 2072, 'thúc': 2073, 'thúy': 2074, 'thăng': 2075, 'thơ': 2076, 'thơi': 2077, 'thơm': 2078, 'thới': 2079, 'thư': 2080, 'thương': 2081, 'thượng': 2082, 'thước': 2083, 'thường': 2084, 'thưởng': 2085, 'thược': 2086, 'thượng': 2087, 'thạch': 2088, 'thạnh': 2089, 'thảnh': 2090, 'thảo': 2091, 'thất': 2092, 'thần': 2093, 'thầy': 2094, 'thẩm': 2095, 'thận': 2096, 'thập': 2097, 'thắng': 2098, 'thế': 2099, 'thể': 2100, 'thỉnh': 2101, 'thị': 2102, 'thịnh': 2103, 'thọ': 2104, 'thỏa': 2105, 'thống': 2106, 'thổ': 2107, 'thới': 2108, 'thờ': 2109, 'thời': 2110, 'thợ': 2111, 'thụ': 2112, 'thục': 2113, 'thụy': 2114, 'thủ': 2115, 'thủy': 2116, 'thứ': 2117, 'thức': 2118, 'thừa': 2119, 'thử': 2120, 'thực': 2121, 'tinh': 2122, 'tiên': 2123, 'tiêu': 2124, 'tiền': 2125, 'tiến': 2126, 'tiếp': 2127, 'tiết': 2128, 'tiền': 2129, 'tiểu': 2130, 'tiệp': 2131, 'tk': 2132, 'tk1': 2133, 'tk10': 2134, 'tk11': 2135, 'tk12': 2136, 'tk13': 2137, 'tk14': 2138, 'tk16': 2139, 'tk2': 2140, 'tk20': 2141, 'tk21': 2142, 'tk27': 2143, 'tk4': 2144, 'tk7': 2145, 'tk8': 2146, 'tka11': 2147, 'tn02': 2148, 'tn1': 2149, 'tn13': 2150, 'toàn': 2151, 'toái': 2152, 'toại': 2153, 'toản': 2154, 'tr3': 2155, 'trai': 2156, 'trang': 2157, 'tranh': 2158, 'tràng': 2159, 'tre': 2160, 'tri': 2161, 'trinh': 2162, 'trì': 2163, 'triết': 2164, 'triều': 2165, 'triển': 2166, 'triệu': 2167, 'trong': 2168, 'trung': 2169, 'truyền': 2170, 'truyện': 2171, 'truông': 2172, 'trúc': 2173, 'trà': 2174, 'tràm': 2175, 'tràng': 2176, 'trào': 2177, 'trác': 2178, 'tráng': 2179, 'trâm': 2180, 'trân': 2181, 'trâu': 2182, 'trần': 2183, 'trãi': 2184, 'trì': 2185, 'trình': 2186, 'trí': 2187, 'trích': 2188, 'trò': 2189, 'trôi': 2190, 'trôm': 2191, 'trống': 2192, 'trùng': 2193, 'trú': 2194, 'trúc': 2195, 'trưng': 2196, 'trương': 2197, 'trực': 2198, 'trường': 2199, 'trượng': 2200, 'trạc': 2201, 'trạch': 2202, 'trại': 2203, 'trạm': 2204, 'trảng': 2205, 'trấn': 2206, 'trấu': 2207, 'trầm': 2208, 'trần': 2209, 'trắc': 2210, 'trẻ': 2211, 'trị': 2212, 'trịnh': 2213, 'trọng': 2214, 'trống': 2215, 'trỗi': 2216, 'trụ': 2217, 'trục': 2218, 'trứ': 2219, 'trừng': 2220, 'trực': 2221, 'tu': 2222, 'tulip': 2223, 'tum': 2224, 'tung': 2225, 'tuy': 2226, 'tuyến': 2227, 'tuyết': 2228, 'tuyền': 2229, 'tuyển': 2230, 'tuân': 2231, 'tuấn': 2232, 'tuệ': 2233, 'tuổi': 2234, 'tx22': 2235, 'tài': 2236, 'tàm': 2237, 'tàng': 2238, 'tá': 2239, 'tác': 2240, 'tách': 2241, 'tái': 2242, 'tám': 2243, 'tánh': 2244, 'tâm': 2245, 'tân': 2246, 'tây': 2247, 'tấn': 2248, 'tèn': 2249, 'tên': 2250, 'tình': 2251, 'tía': 2252, 'tích': 2253, 'tím': 2254, 'tín': 2255, 'tòng': 2256, 'tó': 2257, 'tô': 2258, 'tôn': 2259, 'tông': 2260, 'tổ': 2261, 'tùng': 2262, 'tú': 2263, 'túc': 2264, 'túy': 2265, 'tăng': 2266, 'tĩnh': 2267, 'tơ': 2268, 'tư': 2269, 'tương': 2270, 'tứ': 2271, 'tử': 2272, 'tự': 2273, 'tước': 2274, 'tướng': 2275, 'tường': 2276, 'tưởng': 2277, 'tượng': 2278, 'tạ': 2279, 'tạm': 2280, 'tạnh': 2281, 'tạo': 2282, 'tả': 2283, 'tản': 2284, 'tảo': 2285, 'tấn': 2286, 'tất': 2287, 'tầm': 2288, 'tần': 2289, 'tập': 2290, 'tắc': 2291, 'tắng': 2292, 'tẻ': 2293, 'tế': 2294, 'tỉnh': 2295, 'tịch': 2296, 'tịnh': 2297, 'tố': 2298, 'tốc': 2299, 'tốn': 2300, 'tống': 2301, 'tốt': 2302, 'tồn': 2303, 'tổ': 2304, 'tổng': 2305, 'tộ': 2306, 'tộc': 2307, 'tụ': 2308, 'tứ': 2309, 'tức': 2310, 'từ': 2311, 'tử': 2312, 'tự': 2313, 'tựu': 2314, 'tỵ': 2315, 'tỷ': 2316, 'u': 2317, 'ung': 2318, 'uy': 2319, 'uyên': 2320, 'v': 2321, 'ven': 2322, 'vi': 2323, 'vinh': 2324, 'viêm': 2325, 'viên': 2326, 'vĩnh': 2327, 'vị': 2328, 'viết': 2329, 'viễn': 2330, 'viện': 2331, 'việt': 2332, 'voi': 2333, 'võ': 2334, 'vu': 2335, 'vua': 2336, 'vui': 2337, 'vuông': 2338, 'vũ': 2339, 'vàng': 2340, 'vành': 2341, 'ván': 2342, 'vân': 2343, 'vì': 2344, 'vòng': 2345, 'vóc': 2346, 'vôi': 2347, 'võ': 2348, 'võng': 2349, 'vùng': 2350, 'văn': 2351, 'vĩ': 2352, 'vĩnh': 2353, 'vũ': 2354, 'vương': 2355, 'vườn': 2356, 'vượng': 2357, 'vượt': 2358, 'vạn': 2359, 'vải': 2360, 'vấn': 2361, 'vấp': 2362, 'vận': 2363, 'vật': 2364, 'vẽ': 2365, 'vệ': 2366, 'vị': 2367, 'vịnh': 2368, 'vọng': 2369, 'vỏ': 2370, 'vồng': 2371, 'vỡ': 2372, 'vực': 2373, 'vỹ': 2374, 'x4': 2375, 'xa': 2376, 'xanh': 2377, 'xá': 2378, 'xe': 2379, 'xiển': 2380, 'xiểu': 2381, 'xong': 2382, 'xoài': 2383, 'xu': 2384, 'xung': 2385, 'xuy': 2386, 'xuyên': 2387, 'xuyến': 2388, 'xuân': 2389, 'xuất': 2390, 'xxt': 2391, 'xá': 2392, 'xán': 2393, 'xây': 2394, 'xã': 2395, 'xén': 2396, 'xéo': 2397, 'xí': 2398, 'xích': 2399, 'xóm': 2400, 'xô': 2401, 'xơ': 2402, 'xưa': 2403, 'xương': 2404, 'xảo': 2405, 'xốm': 2406, 'y': 2407, 'y.e.c': 2408, 'yersin': 2409, 'yêm': 2410, 'yên': 2411, 'yến': 2412, 'yết': 2413, 'z133': 2414, 'z153': 2415, 'á': 2416, 'ái': 2417, 'án': 2418, 'áng': 2419, 'ánh': 2420, 'ân': 2421, 'âu': 2422, 'ấp': 2423, 'ích': 2424, 'ô': 2425, 'ôn': 2426, 'ông': 2427, 'úc': 2428, 'út': 2429, 'ý': 2430, 'đa': 2431, 'đai': 2432, 'đan': 2433, 'đang': 2434, 'đào': 2435, 'đại': 2436, 'đạo': 2437, 'đen': 2438, 'đinh': 2439, 'điêu': 2440, 'điện': 2441, 'đình': 2442, 'điếu': 2443, 'điềm': 2444, 'điền': 2445, 'điều': 2446, 'điểm': 2447, 'điển': 2448, 'điện': 2449, 'điệp': 2450, 'điệt': 2451, 'điệu': 2452, 'đoàn': 2453, 'đò': 2454, 'đt': 2455, 'đuống': 2456, 'đà': 2457, 'đài': 2458, 'đàm': 2459, 'đàn': 2460, 'đành': 2461, 'đào': 2462, 'đá': 2463, 'đán': 2464, 'đáng': 2465, 'đáo': 2466, 'đáy': 2467, 'đèn': 2468, 'đê': 2469, 'đìa-nam': 2470, 'đình': 2471, 'đò': 2472, 'đô': 2473, 'đôi': 2474, 'đôn': 2475, 'đông': 2476, 'đồng': 2477, 'đống': 2478, 'độ': 2479, 'đội': 2480, 'đúc': 2481, 'đăm': 2482, 'đăng': 2483, 'đằng': 2484, 'đĩnh': 2485, 'đơ': 2486, 'đường': 2487, 'đại': 2488, 'đạm': 2489, 'đạo': 2490, 'đạt': 2491, 'đản': 2492, 'đảo': 2493, 'đất': 2494, 'đầm': 2495, 'đầy': 2496, 'đẩu': 2497, 'đẫy': 2498, 'đậu': 2499, 'đắc': 2500, 'đằng': 2501, 'đặc': 2502, 'đặng': 2503, 'đế': 2504, 'đề': 2505, 'đền': 2506, 'đệm': 2507, 'đỉnh': 2508, 'địa': 2509, 'định': 2510, 'đốc': 2511, 'đối': 2512, 'đống': 2513, 'đồ': 2514, 'đồi': 2515, 'đồn': 2516, 'đồng': 2517, 'đổ': 2518, 'đổng': 2519, 'đỗ': 2520, 'độ': 2521, 'độc': 2522, 'đội': 2523, 'động': 2524, 'đục': 2525, 'đức': 2526, 'đừng': 2527, 'ơn': 2528, 'ư': 2529, 'ưu': 2530, 'ương': 2531, 'ước': 2532, 'ảnh': 2533, 'ấm': 2534, 'ấp': 2535, 'ấu': 2536, 'ống': 2537, 'ụ': 2538, 'ứng': 2539, 'ỷ': 2540}\n",
            "72 15\n",
            "5946864 5946864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJvJRluuoq0U"
      },
      "source": [
        "from numpy import save\n",
        "save('/content/drive/MyDrive/AI_COLAB/Viettel/input_tensor.npy', input_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obYcHQzmp-P0"
      },
      "source": [
        "from numpy import load\n",
        "input_tensor = load('/content/drive/MyDrive/AI_COLAB/Viettel/input_tensor.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPwEioqzDVl"
      },
      "source": [
        "from numpy import load\n",
        "target_tensor = load('/content/drive/MyDrive/AI_COLAB/Viettel/target_tensor.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7twbMHR-cal",
        "outputId": "ded89943-7a78-4634-debe-c88c17a0c0eb"
      },
      "source": [
        "input_tensor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5946864, 72)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqruAvRoCbUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d730c4-a326-4258-9e2b-18096c1100e4"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4757491, 4757491, 1189373, 1189373)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edsbHyICCdEg"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lai9JhvadAk7",
        "outputId": "01e84920-73ea-4a7f-8b32-f9d5db47c553"
      },
      "source": [
        "vocab_tar_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DZQ_fkhc8J7",
        "outputId": "3d2ae1a0-ab22-4a6a-eac6-3a1b9f11b332"
      },
      "source": [
        "print(vocab_inp_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbLmdJzTCfeG"
      },
      "source": [
        "def gru(units,is_backward):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform',\n",
        "                                    go_backwards = is_backward)\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform',\n",
        "                               go_backwards = is_backward)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGwXLs_FChzs"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru_forward = gru(self.enc_units,is_backward= False)\n",
        "        self.gru_backward = gru(self.enc_units,is_backward = True)\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        # print('after embeding',x.shape)\n",
        "        output_f, state_f = self.gru_forward(x, initial_state = hidden)     \n",
        "        output_b, state_b = self.gru_backward(x, initial_state = hidden)\n",
        "        # print(output_f.shape) \n",
        "        # print(state_f.shape) \n",
        "        return  tf.keras.layers.Concatenate()([output_f, output_b]), state_f\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-fs8D-w4zFp"
      },
      "source": [
        "def split_list(x,key):\n",
        "  result = []\n",
        "  sub_list = []\n",
        "  for item in x :\n",
        "    if item != key:\n",
        "      sub_list.append(item)\n",
        "    else:\n",
        "      result.append(np.asarray(sub_list))\n",
        "      sub_list = []\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1NIQyEL7ZdO",
        "outputId": "bf1b82f0-86fe-4305-e175-fa1bc5b18c87"
      },
      "source": [
        "split_list(np.array([1,2,3,4,5,3,4,3,7]),3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([4, 5]), array([4])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkQZdDg19Daf"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru_char_forward = gru(self.enc_units,is_backward= False)\n",
        "        self.gru_char_backward = gru(self.enc_units,is_backward = True)\n",
        "        self.gru_word_forward = gru(self.enc_units,is_backward= False)\n",
        "        self.gru_word_backward = gru(self.enc_units,is_backward = True)\n",
        "    def call(self, x, hidden):\n",
        "        word_split = split_list(x,inp_lang.word2idx[' '])\n",
        "        print(word_split.shape)\n",
        "        x = self.embedding(x)\n",
        "        print(x.shape)\n",
        "        senten = []\n",
        "        for word in word_split:\n",
        "          x =  self.embedding(word)\n",
        "          print(x.shape)\n",
        "          output_char_f, state_char_f = self.gru_char_forward(x, initial_state = hidden)     \n",
        "          output_char_b, state_char_b = self.gru_char_backward(x, initial_state = hidden)  \n",
        "          senten.append(Concatenate()([output_char_f, output_char_b]))\n",
        "\n",
        "        senten = np.asarray(senten)\n",
        "        print(senten.shape)\n",
        "        output_word_f, state_word_f = self.gru_word_forward(senten, initial_state = hidden)     \n",
        "        output_word_b, state_word_b = self.gru_word_backward(senten, initial_state = hidden) \n",
        "        print(output_word_f.shape)\n",
        "        print(state_word_f)\n",
        "        return  Concatenate()([output_word_f, output_word_b]), state_word_f\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOlBUEG5Cj2T"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units, is_backward=False)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw3VY6XPCmE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722b8e76-a958-4af0-cf9e-bbd5ec798585"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-ab0bb6c42301>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWKz1_yYCn37"
      },
      "source": [
        "optimizer =  tf.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUeFf8MlCpoZ"
      },
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints'\n",
        "checkpoint_dir2 = '/content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints2'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir2, \"ckpt\")\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer,  iterator=iterator)\n",
        "manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2dck4GvdLj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4510619-97e8-4d1b-b477-b5aa5de4af71"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f31d36db400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqIRWXX7Cr4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7cc4d0b-0135-482e-e9f8-58c3f28ad9c6"
      },
      "source": [
        "EPOCHS = 20\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "    \n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    # for (batch, (inp, targ)) in enumerate(dataset):\n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "    \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)  \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        ckpt.step.assign_add(1)\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                          batch,\n",
        "                                                          batch_loss.numpy()))\n",
        "            save_path = manager.save()\n",
        "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
        "            print(\"loss {:1.2f}\".format(loss.numpy()))\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    # if (epoch + 1) % 2 == 0:\n",
        "    # checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restored from /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-3\n",
            "Epoch 1 Batch 0 Loss 1.1895\n",
            "Saved checkpoint for step 103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-4\n",
            "loss 17.84\n",
            "Epoch 1 Batch 100 Loss 1.2005\n",
            "Saved checkpoint for step 203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-5\n",
            "loss 18.01\n",
            "Epoch 1 Batch 200 Loss 1.1126\n",
            "Saved checkpoint for step 303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-6\n",
            "loss 16.69\n",
            "Epoch 1 Batch 300 Loss 1.0610\n",
            "Saved checkpoint for step 403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-7\n",
            "loss 15.91\n",
            "Epoch 1 Batch 400 Loss 1.0775\n",
            "Saved checkpoint for step 503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-8\n",
            "loss 16.16\n",
            "Epoch 1 Batch 500 Loss 1.0009\n",
            "Saved checkpoint for step 603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-9\n",
            "loss 15.01\n",
            "Epoch 1 Batch 600 Loss 1.0994\n",
            "Saved checkpoint for step 703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-10\n",
            "loss 16.49\n",
            "Epoch 1 Batch 700 Loss 1.0321\n",
            "Saved checkpoint for step 803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-11\n",
            "loss 15.48\n",
            "Epoch 1 Batch 800 Loss 1.0194\n",
            "Saved checkpoint for step 903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-12\n",
            "loss 15.29\n",
            "Epoch 1 Batch 900 Loss 1.0030\n",
            "Saved checkpoint for step 1003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-13\n",
            "loss 15.05\n",
            "Epoch 1 Batch 1000 Loss 0.9941\n",
            "Saved checkpoint for step 1103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-14\n",
            "loss 14.91\n",
            "Epoch 1 Batch 1100 Loss 0.9368\n",
            "Saved checkpoint for step 1203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-15\n",
            "loss 14.05\n",
            "Epoch 1 Batch 1200 Loss 0.9904\n",
            "Saved checkpoint for step 1303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-16\n",
            "loss 14.86\n",
            "Epoch 1 Batch 1300 Loss 0.9992\n",
            "Saved checkpoint for step 1403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-17\n",
            "loss 14.99\n",
            "Epoch 1 Batch 1400 Loss 0.9869\n",
            "Saved checkpoint for step 1503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-18\n",
            "loss 14.80\n",
            "Epoch 1 Batch 1500 Loss 0.9994\n",
            "Saved checkpoint for step 1603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-19\n",
            "loss 14.99\n",
            "Epoch 1 Batch 1600 Loss 0.9459\n",
            "Saved checkpoint for step 1703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-20\n",
            "loss 14.19\n",
            "Epoch 1 Batch 1700 Loss 0.9733\n",
            "Saved checkpoint for step 1803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-21\n",
            "loss 14.60\n",
            "Epoch 1 Batch 1800 Loss 0.9584\n",
            "Saved checkpoint for step 1903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-22\n",
            "loss 14.38\n",
            "Epoch 1 Batch 1900 Loss 0.9606\n",
            "Saved checkpoint for step 2003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-23\n",
            "loss 14.41\n",
            "Epoch 1 Batch 2000 Loss 0.8883\n",
            "Saved checkpoint for step 2103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-24\n",
            "loss 13.32\n",
            "Epoch 1 Batch 2100 Loss 0.8522\n",
            "Saved checkpoint for step 2203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-25\n",
            "loss 12.78\n",
            "Epoch 1 Batch 2200 Loss 0.7831\n",
            "Saved checkpoint for step 2303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-26\n",
            "loss 11.75\n",
            "Epoch 1 Batch 2300 Loss 0.7197\n",
            "Saved checkpoint for step 2403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-27\n",
            "loss 10.80\n",
            "Epoch 1 Batch 2400 Loss 0.7478\n",
            "Saved checkpoint for step 2503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-28\n",
            "loss 11.22\n",
            "Epoch 1 Batch 2500 Loss 0.7361\n",
            "Saved checkpoint for step 2603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-29\n",
            "loss 11.04\n",
            "Epoch 1 Batch 2600 Loss 0.7015\n",
            "Saved checkpoint for step 2703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-30\n",
            "loss 10.52\n",
            "Epoch 1 Batch 2700 Loss 0.6991\n",
            "Saved checkpoint for step 2803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-31\n",
            "loss 10.49\n",
            "Epoch 1 Batch 2800 Loss 0.6888\n",
            "Saved checkpoint for step 2903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-32\n",
            "loss 10.33\n",
            "Epoch 1 Batch 2900 Loss 0.6767\n",
            "Saved checkpoint for step 3003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-33\n",
            "loss 10.15\n",
            "Epoch 1 Batch 3000 Loss 0.6435\n",
            "Saved checkpoint for step 3103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-34\n",
            "loss 9.65\n",
            "Epoch 1 Batch 3100 Loss 0.6839\n",
            "Saved checkpoint for step 3203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-35\n",
            "loss 10.26\n",
            "Epoch 1 Batch 3200 Loss 0.6694\n",
            "Saved checkpoint for step 3303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-36\n",
            "loss 10.04\n",
            "Epoch 1 Batch 3300 Loss 0.6482\n",
            "Saved checkpoint for step 3403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-37\n",
            "loss 9.72\n",
            "Epoch 1 Batch 3400 Loss 0.5785\n",
            "Saved checkpoint for step 3503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-38\n",
            "loss 8.68\n",
            "Epoch 1 Batch 3500 Loss 0.6183\n",
            "Saved checkpoint for step 3603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-39\n",
            "loss 9.27\n",
            "Epoch 1 Batch 3600 Loss 0.6076\n",
            "Saved checkpoint for step 3703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-40\n",
            "loss 9.11\n",
            "Epoch 1 Batch 3700 Loss 0.6319\n",
            "Saved checkpoint for step 3803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-41\n",
            "loss 9.48\n",
            "Epoch 1 Batch 3800 Loss 0.5997\n",
            "Saved checkpoint for step 3903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-42\n",
            "loss 9.00\n",
            "Epoch 1 Batch 3900 Loss 0.5898\n",
            "Saved checkpoint for step 4003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-43\n",
            "loss 8.85\n",
            "Epoch 1 Batch 4000 Loss 0.5756\n",
            "Saved checkpoint for step 4103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-44\n",
            "loss 8.63\n",
            "Epoch 1 Batch 4100 Loss 0.5253\n",
            "Saved checkpoint for step 4203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-45\n",
            "loss 7.88\n",
            "Epoch 1 Batch 4200 Loss 0.5630\n",
            "Saved checkpoint for step 4303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-46\n",
            "loss 8.45\n",
            "Epoch 1 Batch 4300 Loss 0.5397\n",
            "Saved checkpoint for step 4403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-47\n",
            "loss 8.10\n",
            "Epoch 1 Batch 4400 Loss 0.5717\n",
            "Saved checkpoint for step 4503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-48\n",
            "loss 8.58\n",
            "Epoch 1 Batch 4500 Loss 0.5127\n",
            "Saved checkpoint for step 4603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-49\n",
            "loss 7.69\n",
            "Epoch 1 Batch 4600 Loss 0.4933\n",
            "Saved checkpoint for step 4703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-50\n",
            "loss 7.40\n",
            "Epoch 1 Batch 4700 Loss 0.5385\n",
            "Saved checkpoint for step 4803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-51\n",
            "loss 8.08\n",
            "Epoch 1 Batch 4800 Loss 0.4990\n",
            "Saved checkpoint for step 4903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-52\n",
            "loss 7.49\n",
            "Epoch 1 Batch 4900 Loss 0.5098\n",
            "Saved checkpoint for step 5003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-53\n",
            "loss 7.65\n",
            "Epoch 1 Batch 5000 Loss 0.4687\n",
            "Saved checkpoint for step 5103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-54\n",
            "loss 7.03\n",
            "Epoch 1 Batch 5100 Loss 0.4662\n",
            "Saved checkpoint for step 5203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-55\n",
            "loss 6.99\n",
            "Epoch 1 Batch 5200 Loss 0.4749\n",
            "Saved checkpoint for step 5303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-56\n",
            "loss 7.12\n",
            "Epoch 1 Batch 5300 Loss 0.4418\n",
            "Saved checkpoint for step 5403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-57\n",
            "loss 6.63\n",
            "Epoch 1 Batch 5400 Loss 0.4642\n",
            "Saved checkpoint for step 5503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-58\n",
            "loss 6.96\n",
            "Epoch 1 Batch 5500 Loss 0.4352\n",
            "Saved checkpoint for step 5603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-59\n",
            "loss 6.53\n",
            "Epoch 1 Batch 5600 Loss 0.4519\n",
            "Saved checkpoint for step 5703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-60\n",
            "loss 6.78\n",
            "Epoch 1 Batch 5700 Loss 0.4221\n",
            "Saved checkpoint for step 5803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-61\n",
            "loss 6.33\n",
            "Epoch 1 Batch 5800 Loss 0.4181\n",
            "Saved checkpoint for step 5903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-62\n",
            "loss 6.27\n",
            "Epoch 1 Batch 5900 Loss 0.4164\n",
            "Saved checkpoint for step 6003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-63\n",
            "loss 6.25\n",
            "Epoch 1 Batch 6000 Loss 0.3781\n",
            "Saved checkpoint for step 6103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-64\n",
            "loss 5.67\n",
            "Epoch 1 Batch 6100 Loss 0.3869\n",
            "Saved checkpoint for step 6203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-65\n",
            "loss 5.80\n",
            "Epoch 1 Batch 6200 Loss 0.4046\n",
            "Saved checkpoint for step 6303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-66\n",
            "loss 6.07\n",
            "Epoch 1 Batch 6300 Loss 0.3579\n",
            "Saved checkpoint for step 6403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-67\n",
            "loss 5.37\n",
            "Epoch 1 Batch 6400 Loss 0.3683\n",
            "Saved checkpoint for step 6503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-68\n",
            "loss 5.52\n",
            "Epoch 1 Batch 6500 Loss 0.3593\n",
            "Saved checkpoint for step 6603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-69\n",
            "loss 5.39\n",
            "Epoch 1 Batch 6600 Loss 0.3079\n",
            "Saved checkpoint for step 6703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-70\n",
            "loss 4.62\n",
            "Epoch 1 Batch 6700 Loss 0.3043\n",
            "Saved checkpoint for step 6803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-71\n",
            "loss 4.57\n",
            "Epoch 1 Batch 6800 Loss 0.3074\n",
            "Saved checkpoint for step 6903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-72\n",
            "loss 4.61\n",
            "Epoch 1 Batch 6900 Loss 0.2978\n",
            "Saved checkpoint for step 7003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-73\n",
            "loss 4.47\n",
            "Epoch 1 Batch 7000 Loss 0.2760\n",
            "Saved checkpoint for step 7103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-74\n",
            "loss 4.14\n",
            "Epoch 1 Batch 7100 Loss 0.2720\n",
            "Saved checkpoint for step 7203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-75\n",
            "loss 4.08\n",
            "Epoch 1 Batch 7200 Loss 0.2639\n",
            "Saved checkpoint for step 7303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-76\n",
            "loss 3.96\n",
            "Epoch 1 Batch 7300 Loss 0.2629\n",
            "Saved checkpoint for step 7403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-77\n",
            "loss 3.94\n",
            "Epoch 1 Batch 7400 Loss 0.2380\n",
            "Saved checkpoint for step 7503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-78\n",
            "loss 3.57\n",
            "Epoch 1 Batch 7500 Loss 0.2340\n",
            "Saved checkpoint for step 7603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-79\n",
            "loss 3.51\n",
            "Epoch 1 Batch 7600 Loss 0.2325\n",
            "Saved checkpoint for step 7703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-80\n",
            "loss 3.49\n",
            "Epoch 1 Batch 7700 Loss 0.2275\n",
            "Saved checkpoint for step 7803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-81\n",
            "loss 3.41\n",
            "Epoch 1 Batch 7800 Loss 0.2051\n",
            "Saved checkpoint for step 7903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-82\n",
            "loss 3.08\n",
            "Epoch 1 Batch 7900 Loss 0.1627\n",
            "Saved checkpoint for step 8003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-83\n",
            "loss 2.44\n",
            "Epoch 1 Batch 8000 Loss 0.1915\n",
            "Saved checkpoint for step 8103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-84\n",
            "loss 2.87\n",
            "Epoch 1 Batch 8100 Loss 0.1981\n",
            "Saved checkpoint for step 8203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-85\n",
            "loss 2.97\n",
            "Epoch 1 Batch 8200 Loss 0.1724\n",
            "Saved checkpoint for step 8303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-86\n",
            "loss 2.59\n",
            "Epoch 1 Batch 8300 Loss 0.1769\n",
            "Saved checkpoint for step 8403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-87\n",
            "loss 2.65\n",
            "Epoch 1 Batch 8400 Loss 0.1815\n",
            "Saved checkpoint for step 8503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-88\n",
            "loss 2.72\n",
            "Epoch 1 Batch 8500 Loss 0.1346\n",
            "Saved checkpoint for step 8603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-89\n",
            "loss 2.02\n",
            "Epoch 1 Batch 8600 Loss 0.1449\n",
            "Saved checkpoint for step 8703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-90\n",
            "loss 2.17\n",
            "Epoch 1 Batch 8700 Loss 0.1265\n",
            "Saved checkpoint for step 8803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-91\n",
            "loss 1.90\n",
            "Epoch 1 Batch 8800 Loss 0.1279\n",
            "Saved checkpoint for step 8903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-92\n",
            "loss 1.92\n",
            "Epoch 1 Batch 8900 Loss 0.1339\n",
            "Saved checkpoint for step 9003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-93\n",
            "loss 2.01\n",
            "Epoch 1 Batch 9000 Loss 0.1164\n",
            "Saved checkpoint for step 9103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-94\n",
            "loss 1.75\n",
            "Epoch 1 Batch 9100 Loss 0.1102\n",
            "Saved checkpoint for step 9203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-95\n",
            "loss 1.65\n",
            "Epoch 1 Batch 9200 Loss 0.1119\n",
            "Saved checkpoint for step 9303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-96\n",
            "loss 1.68\n",
            "Epoch 1 Batch 9300 Loss 0.0979\n",
            "Saved checkpoint for step 9403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-97\n",
            "loss 1.47\n",
            "Epoch 1 Batch 9400 Loss 0.1176\n",
            "Saved checkpoint for step 9503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-98\n",
            "loss 1.76\n",
            "Epoch 1 Batch 9500 Loss 0.1035\n",
            "Saved checkpoint for step 9603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-99\n",
            "loss 1.55\n",
            "Epoch 1 Batch 9600 Loss 0.0907\n",
            "Saved checkpoint for step 9703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-100\n",
            "loss 1.36\n",
            "Epoch 1 Batch 9700 Loss 0.0937\n",
            "Saved checkpoint for step 9803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-101\n",
            "loss 1.41\n",
            "Epoch 1 Batch 9800 Loss 0.0804\n",
            "Saved checkpoint for step 9903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-102\n",
            "loss 1.21\n",
            "Epoch 1 Batch 9900 Loss 0.0671\n",
            "Saved checkpoint for step 10003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-103\n",
            "loss 1.01\n",
            "Epoch 1 Batch 10000 Loss 0.0757\n",
            "Saved checkpoint for step 10103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-104\n",
            "loss 1.14\n",
            "Epoch 1 Batch 10100 Loss 0.0610\n",
            "Saved checkpoint for step 10203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-105\n",
            "loss 0.91\n",
            "Epoch 1 Batch 10200 Loss 0.0488\n",
            "Saved checkpoint for step 10303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-106\n",
            "loss 0.73\n",
            "Epoch 1 Batch 10300 Loss 0.0624\n",
            "Saved checkpoint for step 10403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-107\n",
            "loss 0.94\n",
            "Epoch 1 Batch 10400 Loss 0.0595\n",
            "Saved checkpoint for step 10503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-108\n",
            "loss 0.89\n",
            "Epoch 1 Batch 10500 Loss 0.0542\n",
            "Saved checkpoint for step 10603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-109\n",
            "loss 0.81\n",
            "Epoch 1 Batch 10600 Loss 0.0562\n",
            "Saved checkpoint for step 10703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-110\n",
            "loss 0.84\n",
            "Epoch 1 Batch 10700 Loss 0.0516\n",
            "Saved checkpoint for step 10803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-111\n",
            "loss 0.77\n",
            "Epoch 1 Batch 10800 Loss 0.0488\n",
            "Saved checkpoint for step 10903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-112\n",
            "loss 0.73\n",
            "Epoch 1 Batch 10900 Loss 0.0449\n",
            "Saved checkpoint for step 11003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-113\n",
            "loss 0.67\n",
            "Epoch 1 Batch 11000 Loss 0.0377\n",
            "Saved checkpoint for step 11103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-114\n",
            "loss 0.57\n",
            "Epoch 1 Batch 11100 Loss 0.0510\n",
            "Saved checkpoint for step 11203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-115\n",
            "loss 0.77\n",
            "Epoch 1 Batch 11200 Loss 0.0331\n",
            "Saved checkpoint for step 11303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-116\n",
            "loss 0.50\n",
            "Epoch 1 Batch 11300 Loss 0.0389\n",
            "Saved checkpoint for step 11403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-117\n",
            "loss 0.58\n",
            "Epoch 1 Batch 11400 Loss 0.0274\n",
            "Saved checkpoint for step 11503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-118\n",
            "loss 0.41\n",
            "Epoch 1 Batch 11500 Loss 0.0347\n",
            "Saved checkpoint for step 11603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-119\n",
            "loss 0.52\n",
            "Epoch 1 Batch 11600 Loss 0.0286\n",
            "Saved checkpoint for step 11703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-120\n",
            "loss 0.43\n",
            "Epoch 1 Batch 11700 Loss 0.0301\n",
            "Saved checkpoint for step 11803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-121\n",
            "loss 0.45\n",
            "Epoch 1 Batch 11800 Loss 0.0308\n",
            "Saved checkpoint for step 11903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-122\n",
            "loss 0.46\n",
            "Epoch 1 Batch 11900 Loss 0.0245\n",
            "Saved checkpoint for step 12003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-123\n",
            "loss 0.37\n",
            "Epoch 1 Batch 12000 Loss 0.0359\n",
            "Saved checkpoint for step 12103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-124\n",
            "loss 0.54\n",
            "Epoch 1 Batch 12100 Loss 0.0120\n",
            "Saved checkpoint for step 12203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-125\n",
            "loss 0.18\n",
            "Epoch 1 Batch 12200 Loss 0.0291\n",
            "Saved checkpoint for step 12303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-126\n",
            "loss 0.44\n",
            "Epoch 1 Batch 12300 Loss 0.0213\n",
            "Saved checkpoint for step 12403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-127\n",
            "loss 0.32\n",
            "Epoch 1 Batch 12400 Loss 0.0310\n",
            "Saved checkpoint for step 12503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-128\n",
            "loss 0.46\n",
            "Epoch 1 Batch 12500 Loss 0.0199\n",
            "Saved checkpoint for step 12603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-129\n",
            "loss 0.30\n",
            "Epoch 1 Batch 12600 Loss 0.0170\n",
            "Saved checkpoint for step 12703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-130\n",
            "loss 0.25\n",
            "Epoch 1 Batch 12700 Loss 0.0223\n",
            "Saved checkpoint for step 12803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-131\n",
            "loss 0.33\n",
            "Epoch 1 Batch 12800 Loss 0.0147\n",
            "Saved checkpoint for step 12903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-132\n",
            "loss 0.22\n",
            "Epoch 1 Batch 12900 Loss 0.0274\n",
            "Saved checkpoint for step 13003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-133\n",
            "loss 0.41\n",
            "Epoch 1 Batch 13000 Loss 0.0164\n",
            "Saved checkpoint for step 13103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-134\n",
            "loss 0.25\n",
            "Epoch 1 Batch 13100 Loss 0.0298\n",
            "Saved checkpoint for step 13203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-135\n",
            "loss 0.45\n",
            "Epoch 1 Batch 13200 Loss 0.0173\n",
            "Saved checkpoint for step 13303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-136\n",
            "loss 0.26\n",
            "Epoch 1 Batch 13300 Loss 0.0201\n",
            "Saved checkpoint for step 13403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-137\n",
            "loss 0.30\n",
            "Epoch 1 Batch 13400 Loss 0.0181\n",
            "Saved checkpoint for step 13503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-138\n",
            "loss 0.27\n",
            "Epoch 1 Batch 13500 Loss 0.0207\n",
            "Saved checkpoint for step 13603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-139\n",
            "loss 0.31\n",
            "Epoch 1 Batch 13600 Loss 0.0219\n",
            "Saved checkpoint for step 13703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-140\n",
            "loss 0.33\n",
            "Epoch 1 Batch 13700 Loss 0.0129\n",
            "Saved checkpoint for step 13803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-141\n",
            "loss 0.19\n",
            "Epoch 1 Batch 13800 Loss 0.0160\n",
            "Saved checkpoint for step 13903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-142\n",
            "loss 0.24\n",
            "Epoch 1 Batch 13900 Loss 0.0125\n",
            "Saved checkpoint for step 14003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-143\n",
            "loss 0.19\n",
            "Epoch 1 Batch 14000 Loss 0.0094\n",
            "Saved checkpoint for step 14103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-144\n",
            "loss 0.14\n",
            "Epoch 1 Batch 14100 Loss 0.0174\n",
            "Saved checkpoint for step 14203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-145\n",
            "loss 0.26\n",
            "Epoch 1 Batch 14200 Loss 0.0172\n",
            "Saved checkpoint for step 14303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-146\n",
            "loss 0.26\n",
            "Epoch 1 Batch 14300 Loss 0.0126\n",
            "Saved checkpoint for step 14403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-147\n",
            "loss 0.19\n",
            "Epoch 1 Batch 14400 Loss 0.0151\n",
            "Saved checkpoint for step 14503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-148\n",
            "loss 0.23\n",
            "Epoch 1 Batch 14500 Loss 0.0098\n",
            "Saved checkpoint for step 14603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-149\n",
            "loss 0.15\n",
            "Epoch 1 Batch 14600 Loss 0.0108\n",
            "Saved checkpoint for step 14703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-150\n",
            "loss 0.16\n",
            "Epoch 1 Batch 14700 Loss 0.0089\n",
            "Saved checkpoint for step 14803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-151\n",
            "loss 0.13\n",
            "Epoch 1 Batch 14800 Loss 0.0086\n",
            "Saved checkpoint for step 14903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-152\n",
            "loss 0.13\n",
            "Epoch 1 Batch 14900 Loss 0.0108\n",
            "Saved checkpoint for step 15003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-153\n",
            "loss 0.16\n",
            "Epoch 1 Batch 15000 Loss 0.0116\n",
            "Saved checkpoint for step 15103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-154\n",
            "loss 0.17\n",
            "Epoch 1 Batch 15100 Loss 0.0085\n",
            "Saved checkpoint for step 15203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-155\n",
            "loss 0.13\n",
            "Epoch 1 Batch 15200 Loss 0.0128\n",
            "Saved checkpoint for step 15303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-156\n",
            "loss 0.19\n",
            "Epoch 1 Batch 15300 Loss 0.0138\n",
            "Saved checkpoint for step 15403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-157\n",
            "loss 0.21\n",
            "Epoch 1 Batch 15400 Loss 0.0252\n",
            "Saved checkpoint for step 15503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-158\n",
            "loss 0.38\n",
            "Epoch 1 Batch 15500 Loss 0.0107\n",
            "Saved checkpoint for step 15603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-159\n",
            "loss 0.16\n",
            "Epoch 1 Batch 15600 Loss 0.0085\n",
            "Saved checkpoint for step 15703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-160\n",
            "loss 0.13\n",
            "Epoch 1 Batch 15700 Loss 0.0086\n",
            "Saved checkpoint for step 15803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-161\n",
            "loss 0.13\n",
            "Epoch 1 Batch 15800 Loss 0.0141\n",
            "Saved checkpoint for step 15903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-162\n",
            "loss 0.21\n",
            "Epoch 1 Batch 15900 Loss 0.0127\n",
            "Saved checkpoint for step 16003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-163\n",
            "loss 0.19\n",
            "Epoch 1 Batch 16000 Loss 0.0106\n",
            "Saved checkpoint for step 16103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-164\n",
            "loss 0.16\n",
            "Epoch 1 Batch 16100 Loss 0.0099\n",
            "Saved checkpoint for step 16203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-165\n",
            "loss 0.15\n",
            "Epoch 1 Batch 16200 Loss 0.0056\n",
            "Saved checkpoint for step 16303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-166\n",
            "loss 0.08\n",
            "Epoch 1 Batch 16300 Loss 0.0066\n",
            "Saved checkpoint for step 16403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-167\n",
            "loss 0.10\n",
            "Epoch 1 Batch 16400 Loss 0.0133\n",
            "Saved checkpoint for step 16503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-168\n",
            "loss 0.20\n",
            "Epoch 1 Batch 16500 Loss 0.0106\n",
            "Saved checkpoint for step 16603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-169\n",
            "loss 0.16\n",
            "Epoch 1 Batch 16600 Loss 0.0086\n",
            "Saved checkpoint for step 16703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-170\n",
            "loss 0.13\n",
            "Epoch 1 Batch 16700 Loss 0.0091\n",
            "Saved checkpoint for step 16803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-171\n",
            "loss 0.14\n",
            "Epoch 1 Batch 16800 Loss 0.0049\n",
            "Saved checkpoint for step 16903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-172\n",
            "loss 0.07\n",
            "Epoch 1 Batch 16900 Loss 0.0056\n",
            "Saved checkpoint for step 17003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-173\n",
            "loss 0.08\n",
            "Epoch 1 Batch 17000 Loss 0.0100\n",
            "Saved checkpoint for step 17103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-174\n",
            "loss 0.15\n",
            "Epoch 1 Batch 17100 Loss 0.0065\n",
            "Saved checkpoint for step 17203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-175\n",
            "loss 0.10\n",
            "Epoch 1 Batch 17200 Loss 0.0066\n",
            "Saved checkpoint for step 17303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-176\n",
            "loss 0.10\n",
            "Epoch 1 Batch 17300 Loss 0.0169\n",
            "Saved checkpoint for step 17403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-177\n",
            "loss 0.25\n",
            "Epoch 1 Batch 17400 Loss 0.0074\n",
            "Saved checkpoint for step 17503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-178\n",
            "loss 0.11\n",
            "Epoch 1 Batch 17500 Loss 0.0068\n",
            "Saved checkpoint for step 17603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-179\n",
            "loss 0.10\n",
            "Epoch 1 Batch 17600 Loss 0.0049\n",
            "Saved checkpoint for step 17703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-180\n",
            "loss 0.07\n",
            "Epoch 1 Batch 17700 Loss 0.0047\n",
            "Saved checkpoint for step 17803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-181\n",
            "loss 0.07\n",
            "Epoch 1 Batch 17800 Loss 0.0054\n",
            "Saved checkpoint for step 17903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-182\n",
            "loss 0.08\n",
            "Epoch 1 Batch 17900 Loss 0.0113\n",
            "Saved checkpoint for step 18003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-183\n",
            "loss 0.17\n",
            "Epoch 1 Batch 18000 Loss 0.0099\n",
            "Saved checkpoint for step 18103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-184\n",
            "loss 0.15\n",
            "Epoch 1 Batch 18100 Loss 0.0065\n",
            "Saved checkpoint for step 18203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-185\n",
            "loss 0.10\n",
            "Epoch 1 Batch 18200 Loss 0.0057\n",
            "Saved checkpoint for step 18303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-186\n",
            "loss 0.09\n",
            "Epoch 1 Batch 18300 Loss 0.0046\n",
            "Saved checkpoint for step 18403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-187\n",
            "loss 0.07\n",
            "Epoch 1 Batch 18400 Loss 0.0063\n",
            "Saved checkpoint for step 18503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-188\n",
            "loss 0.09\n",
            "Epoch 1 Batch 18500 Loss 0.0042\n",
            "Saved checkpoint for step 18603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-189\n",
            "loss 0.06\n",
            "Epoch 1 Batch 18600 Loss 0.0031\n",
            "Saved checkpoint for step 18703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-190\n",
            "loss 0.05\n",
            "Epoch 1 Batch 18700 Loss 0.0039\n",
            "Saved checkpoint for step 18803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-191\n",
            "loss 0.06\n",
            "Epoch 1 Batch 18800 Loss 0.0083\n",
            "Saved checkpoint for step 18903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-192\n",
            "loss 0.12\n",
            "Epoch 1 Batch 18900 Loss 0.0069\n",
            "Saved checkpoint for step 19003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-193\n",
            "loss 0.10\n",
            "Epoch 1 Batch 19000 Loss 0.0041\n",
            "Saved checkpoint for step 19103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-194\n",
            "loss 0.06\n",
            "Epoch 1 Batch 19100 Loss 0.0031\n",
            "Saved checkpoint for step 19203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-195\n",
            "loss 0.05\n",
            "Epoch 1 Batch 19200 Loss 0.0140\n",
            "Saved checkpoint for step 19303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-196\n",
            "loss 0.21\n",
            "Epoch 1 Batch 19300 Loss 0.0127\n",
            "Saved checkpoint for step 19403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-197\n",
            "loss 0.19\n",
            "Epoch 1 Batch 19400 Loss 0.0071\n",
            "Saved checkpoint for step 19503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-198\n",
            "loss 0.11\n",
            "Epoch 1 Batch 19500 Loss 0.0114\n",
            "Saved checkpoint for step 19603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-199\n",
            "loss 0.17\n",
            "Epoch 1 Batch 19600 Loss 0.0046\n",
            "Saved checkpoint for step 19703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-200\n",
            "loss 0.07\n",
            "Epoch 1 Batch 19700 Loss 0.0077\n",
            "Saved checkpoint for step 19803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-201\n",
            "loss 0.12\n",
            "Epoch 1 Batch 19800 Loss 0.0061\n",
            "Saved checkpoint for step 19903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-202\n",
            "loss 0.09\n",
            "Epoch 1 Batch 19900 Loss 0.0055\n",
            "Saved checkpoint for step 20003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-203\n",
            "loss 0.08\n",
            "Epoch 1 Batch 20000 Loss 0.0103\n",
            "Saved checkpoint for step 20103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-204\n",
            "loss 0.15\n",
            "Epoch 1 Batch 20100 Loss 0.0045\n",
            "Saved checkpoint for step 20203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-205\n",
            "loss 0.07\n",
            "Epoch 1 Batch 20200 Loss 0.0065\n",
            "Saved checkpoint for step 20303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-206\n",
            "loss 0.10\n",
            "Epoch 1 Batch 20300 Loss 0.0076\n",
            "Saved checkpoint for step 20403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-207\n",
            "loss 0.11\n",
            "Epoch 1 Batch 20400 Loss 0.0062\n",
            "Saved checkpoint for step 20503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-208\n",
            "loss 0.09\n",
            "Epoch 1 Batch 20500 Loss 0.0099\n",
            "Saved checkpoint for step 20603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-209\n",
            "loss 0.15\n",
            "Epoch 1 Batch 20600 Loss 0.0048\n",
            "Saved checkpoint for step 20703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-210\n",
            "loss 0.07\n",
            "Epoch 1 Batch 20700 Loss 0.0056\n",
            "Saved checkpoint for step 20803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-211\n",
            "loss 0.08\n",
            "Epoch 1 Batch 20800 Loss 0.0050\n",
            "Saved checkpoint for step 20903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-212\n",
            "loss 0.08\n",
            "Epoch 1 Batch 20900 Loss 0.0026\n",
            "Saved checkpoint for step 21003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-213\n",
            "loss 0.04\n",
            "Epoch 1 Batch 21000 Loss 0.0025\n",
            "Saved checkpoint for step 21103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-214\n",
            "loss 0.04\n",
            "Epoch 1 Batch 21100 Loss 0.0036\n",
            "Saved checkpoint for step 21203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-215\n",
            "loss 0.05\n",
            "Epoch 1 Batch 21200 Loss 0.0076\n",
            "Saved checkpoint for step 21303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-216\n",
            "loss 0.11\n",
            "Epoch 1 Batch 21300 Loss 0.0088\n",
            "Saved checkpoint for step 21403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-217\n",
            "loss 0.13\n",
            "Epoch 1 Batch 21400 Loss 0.0077\n",
            "Saved checkpoint for step 21503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-218\n",
            "loss 0.12\n",
            "Epoch 1 Batch 21500 Loss 0.0031\n",
            "Saved checkpoint for step 21603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-219\n",
            "loss 0.05\n",
            "Epoch 1 Batch 21600 Loss 0.0026\n",
            "Saved checkpoint for step 21703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-220\n",
            "loss 0.04\n",
            "Epoch 1 Batch 21700 Loss 0.0028\n",
            "Saved checkpoint for step 21803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-221\n",
            "loss 0.04\n",
            "Epoch 1 Batch 21800 Loss 0.0059\n",
            "Saved checkpoint for step 21903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-222\n",
            "loss 0.09\n",
            "Epoch 1 Batch 21900 Loss 0.0080\n",
            "Saved checkpoint for step 22003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-223\n",
            "loss 0.12\n",
            "Epoch 1 Batch 22000 Loss 0.0091\n",
            "Saved checkpoint for step 22103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-224\n",
            "loss 0.14\n",
            "Epoch 1 Batch 22100 Loss 0.0064\n",
            "Saved checkpoint for step 22203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-225\n",
            "loss 0.10\n",
            "Epoch 1 Batch 22200 Loss 0.0016\n",
            "Saved checkpoint for step 22303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-226\n",
            "loss 0.02\n",
            "Epoch 1 Batch 22300 Loss 0.0160\n",
            "Saved checkpoint for step 22403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-227\n",
            "loss 0.24\n",
            "Epoch 1 Batch 22400 Loss 0.0081\n",
            "Saved checkpoint for step 22503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-228\n",
            "loss 0.12\n",
            "Epoch 1 Batch 22500 Loss 0.0080\n",
            "Saved checkpoint for step 22603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-229\n",
            "loss 0.12\n",
            "Epoch 1 Batch 22600 Loss 0.0060\n",
            "Saved checkpoint for step 22703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-230\n",
            "loss 0.09\n",
            "Epoch 1 Batch 22700 Loss 0.0070\n",
            "Saved checkpoint for step 22803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-231\n",
            "loss 0.10\n",
            "Epoch 1 Batch 22800 Loss 0.0012\n",
            "Saved checkpoint for step 22903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-232\n",
            "loss 0.02\n",
            "Epoch 1 Batch 22900 Loss 0.0070\n",
            "Saved checkpoint for step 23003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-233\n",
            "loss 0.10\n",
            "Epoch 1 Batch 23000 Loss 0.0080\n",
            "Saved checkpoint for step 23103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-234\n",
            "loss 0.12\n",
            "Epoch 1 Batch 23100 Loss 0.0043\n",
            "Saved checkpoint for step 23203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-235\n",
            "loss 0.06\n",
            "Epoch 1 Batch 23200 Loss 0.0044\n",
            "Saved checkpoint for step 23303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-236\n",
            "loss 0.07\n",
            "Epoch 1 Batch 23300 Loss 0.0041\n",
            "Saved checkpoint for step 23403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-237\n",
            "loss 0.06\n",
            "Epoch 1 Batch 23400 Loss 0.0065\n",
            "Saved checkpoint for step 23503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-238\n",
            "loss 0.10\n",
            "Epoch 1 Batch 23500 Loss 0.0046\n",
            "Saved checkpoint for step 23603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-239\n",
            "loss 0.07\n",
            "Epoch 1 Batch 23600 Loss 0.0123\n",
            "Saved checkpoint for step 23703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-240\n",
            "loss 0.19\n",
            "Epoch 1 Batch 23700 Loss 0.0121\n",
            "Saved checkpoint for step 23803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-241\n",
            "loss 0.18\n",
            "Epoch 1 Batch 23800 Loss 0.0039\n",
            "Saved checkpoint for step 23903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-242\n",
            "loss 0.06\n",
            "Epoch 1 Batch 23900 Loss 0.0041\n",
            "Saved checkpoint for step 24003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-243\n",
            "loss 0.06\n",
            "Epoch 1 Batch 24000 Loss 0.0016\n",
            "Saved checkpoint for step 24103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-244\n",
            "loss 0.02\n",
            "Epoch 1 Batch 24100 Loss 0.0067\n",
            "Saved checkpoint for step 24203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-245\n",
            "loss 0.10\n",
            "Epoch 1 Batch 24200 Loss 0.0012\n",
            "Saved checkpoint for step 24303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-246\n",
            "loss 0.02\n",
            "Epoch 1 Batch 24300 Loss 0.0037\n",
            "Saved checkpoint for step 24403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-247\n",
            "loss 0.06\n",
            "Epoch 1 Batch 24400 Loss 0.0105\n",
            "Saved checkpoint for step 24503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-248\n",
            "loss 0.16\n",
            "Epoch 1 Batch 24500 Loss 0.0048\n",
            "Saved checkpoint for step 24603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-249\n",
            "loss 0.07\n",
            "Epoch 1 Batch 24600 Loss 0.0041\n",
            "Saved checkpoint for step 24703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-250\n",
            "loss 0.06\n",
            "Epoch 1 Batch 24700 Loss 0.0101\n",
            "Saved checkpoint for step 24803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-251\n",
            "loss 0.15\n",
            "Epoch 1 Batch 24800 Loss 0.0030\n",
            "Saved checkpoint for step 24903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-252\n",
            "loss 0.05\n",
            "Epoch 1 Batch 24900 Loss 0.0044\n",
            "Saved checkpoint for step 25003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-253\n",
            "loss 0.07\n",
            "Epoch 1 Batch 25000 Loss 0.0083\n",
            "Saved checkpoint for step 25103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-254\n",
            "loss 0.13\n",
            "Epoch 1 Batch 25100 Loss 0.0092\n",
            "Saved checkpoint for step 25203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-255\n",
            "loss 0.14\n",
            "Epoch 1 Batch 25200 Loss 0.0043\n",
            "Saved checkpoint for step 25303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-256\n",
            "loss 0.06\n",
            "Epoch 1 Batch 25300 Loss 0.0050\n",
            "Saved checkpoint for step 25403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-257\n",
            "loss 0.08\n",
            "Epoch 1 Batch 25400 Loss 0.0058\n",
            "Saved checkpoint for step 25503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-258\n",
            "loss 0.09\n",
            "Epoch 1 Batch 25500 Loss 0.0039\n",
            "Saved checkpoint for step 25603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-259\n",
            "loss 0.06\n",
            "Epoch 1 Batch 25600 Loss 0.0035\n",
            "Saved checkpoint for step 25703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-260\n",
            "loss 0.05\n",
            "Epoch 1 Batch 25700 Loss 0.0037\n",
            "Saved checkpoint for step 25803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-261\n",
            "loss 0.06\n",
            "Epoch 1 Batch 25800 Loss 0.0029\n",
            "Saved checkpoint for step 25903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-262\n",
            "loss 0.04\n",
            "Epoch 1 Batch 25900 Loss 0.0012\n",
            "Saved checkpoint for step 26003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-263\n",
            "loss 0.02\n",
            "Epoch 1 Batch 26000 Loss 0.0063\n",
            "Saved checkpoint for step 26103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-264\n",
            "loss 0.09\n",
            "Epoch 1 Batch 26100 Loss 0.0082\n",
            "Saved checkpoint for step 26203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-265\n",
            "loss 0.12\n",
            "Epoch 1 Batch 26200 Loss 0.0082\n",
            "Saved checkpoint for step 26303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-266\n",
            "loss 0.12\n",
            "Epoch 1 Batch 26300 Loss 0.0065\n",
            "Saved checkpoint for step 26403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-267\n",
            "loss 0.10\n",
            "Epoch 1 Batch 26400 Loss 0.0013\n",
            "Saved checkpoint for step 26503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-268\n",
            "loss 0.02\n",
            "Epoch 1 Batch 26500 Loss 0.0021\n",
            "Saved checkpoint for step 26603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-269\n",
            "loss 0.03\n",
            "Epoch 1 Batch 26600 Loss 0.0089\n",
            "Saved checkpoint for step 26703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-270\n",
            "loss 0.13\n",
            "Epoch 1 Batch 26700 Loss 0.0053\n",
            "Saved checkpoint for step 26803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-271\n",
            "loss 0.08\n",
            "Epoch 1 Batch 26800 Loss 0.0016\n",
            "Saved checkpoint for step 26903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-272\n",
            "loss 0.02\n",
            "Epoch 1 Batch 26900 Loss 0.0067\n",
            "Saved checkpoint for step 27003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-273\n",
            "loss 0.10\n",
            "Epoch 1 Batch 27000 Loss 0.0132\n",
            "Saved checkpoint for step 27103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-274\n",
            "loss 0.20\n",
            "Epoch 1 Batch 27100 Loss 0.0043\n",
            "Saved checkpoint for step 27203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-275\n",
            "loss 0.07\n",
            "Epoch 1 Batch 27200 Loss 0.0040\n",
            "Saved checkpoint for step 27303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-276\n",
            "loss 0.06\n",
            "Epoch 1 Batch 27300 Loss 0.0015\n",
            "Saved checkpoint for step 27403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-277\n",
            "loss 0.02\n",
            "Epoch 1 Batch 27400 Loss 0.0017\n",
            "Saved checkpoint for step 27503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-278\n",
            "loss 0.02\n",
            "Epoch 1 Batch 27500 Loss 0.0007\n",
            "Saved checkpoint for step 27603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-279\n",
            "loss 0.01\n",
            "Epoch 1 Batch 27600 Loss 0.0026\n",
            "Saved checkpoint for step 27703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-280\n",
            "loss 0.04\n",
            "Epoch 1 Batch 27700 Loss 0.0055\n",
            "Saved checkpoint for step 27803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-281\n",
            "loss 0.08\n",
            "Epoch 1 Batch 27800 Loss 0.0087\n",
            "Saved checkpoint for step 27903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-282\n",
            "loss 0.13\n",
            "Epoch 1 Batch 27900 Loss 0.0030\n",
            "Saved checkpoint for step 28003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-283\n",
            "loss 0.05\n",
            "Epoch 1 Batch 28000 Loss 0.0067\n",
            "Saved checkpoint for step 28103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-284\n",
            "loss 0.10\n",
            "Epoch 1 Batch 28100 Loss 0.0046\n",
            "Saved checkpoint for step 28203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-285\n",
            "loss 0.07\n",
            "Epoch 1 Batch 28200 Loss 0.0022\n",
            "Saved checkpoint for step 28303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-286\n",
            "loss 0.03\n",
            "Epoch 1 Batch 28300 Loss 0.0056\n",
            "Saved checkpoint for step 28403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-287\n",
            "loss 0.08\n",
            "Epoch 1 Batch 28400 Loss 0.0029\n",
            "Saved checkpoint for step 28503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-288\n",
            "loss 0.04\n",
            "Epoch 1 Batch 28500 Loss 0.0077\n",
            "Saved checkpoint for step 28603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-289\n",
            "loss 0.12\n",
            "Epoch 1 Batch 28600 Loss 0.0095\n",
            "Saved checkpoint for step 28703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-290\n",
            "loss 0.14\n",
            "Epoch 1 Batch 28700 Loss 0.0010\n",
            "Saved checkpoint for step 28803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-291\n",
            "loss 0.02\n",
            "Epoch 1 Batch 28800 Loss 0.0068\n",
            "Saved checkpoint for step 28903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-292\n",
            "loss 0.10\n",
            "Epoch 1 Batch 28900 Loss 0.0009\n",
            "Saved checkpoint for step 29003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-293\n",
            "loss 0.01\n",
            "Epoch 1 Batch 29000 Loss 0.0016\n",
            "Saved checkpoint for step 29103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-294\n",
            "loss 0.02\n",
            "Epoch 1 Batch 29100 Loss 0.0131\n",
            "Saved checkpoint for step 29203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-295\n",
            "loss 0.20\n",
            "Epoch 1 Batch 29200 Loss 0.0025\n",
            "Saved checkpoint for step 29303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-296\n",
            "loss 0.04\n",
            "Epoch 1 Batch 29300 Loss 0.0073\n",
            "Saved checkpoint for step 29403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-297\n",
            "loss 0.11\n",
            "Epoch 1 Batch 29400 Loss 0.0068\n",
            "Saved checkpoint for step 29503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-298\n",
            "loss 0.10\n",
            "Epoch 1 Batch 29500 Loss 0.0040\n",
            "Saved checkpoint for step 29603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-299\n",
            "loss 0.06\n",
            "Epoch 1 Batch 29600 Loss 0.0108\n",
            "Saved checkpoint for step 29703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-300\n",
            "loss 0.16\n",
            "Epoch 1 Batch 29700 Loss 0.0119\n",
            "Saved checkpoint for step 29803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-301\n",
            "loss 0.18\n",
            "Epoch 1 Batch 29800 Loss 0.0023\n",
            "Saved checkpoint for step 29903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-302\n",
            "loss 0.03\n",
            "Epoch 1 Batch 29900 Loss 0.0008\n",
            "Saved checkpoint for step 30003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-303\n",
            "loss 0.01\n",
            "Epoch 1 Batch 30000 Loss 0.0019\n",
            "Saved checkpoint for step 30103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-304\n",
            "loss 0.03\n",
            "Epoch 1 Batch 30100 Loss 0.0010\n",
            "Saved checkpoint for step 30203: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-305\n",
            "loss 0.01\n",
            "Epoch 1 Batch 30200 Loss 0.0013\n",
            "Saved checkpoint for step 30303: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-306\n",
            "loss 0.02\n",
            "Epoch 1 Batch 30300 Loss 0.0068\n",
            "Saved checkpoint for step 30403: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-307\n",
            "loss 0.10\n",
            "Epoch 1 Batch 30400 Loss 0.0019\n",
            "Saved checkpoint for step 30503: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-308\n",
            "loss 0.03\n",
            "Epoch 1 Batch 30500 Loss 0.0019\n",
            "Saved checkpoint for step 30603: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-309\n",
            "loss 0.03\n",
            "Epoch 1 Batch 30600 Loss 0.0009\n",
            "Saved checkpoint for step 30703: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-310\n",
            "loss 0.01\n",
            "Epoch 1 Batch 30700 Loss 0.0048\n",
            "Saved checkpoint for step 30803: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-311\n",
            "loss 0.07\n",
            "Epoch 1 Batch 30800 Loss 0.0037\n",
            "Saved checkpoint for step 30903: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-312\n",
            "loss 0.06\n",
            "Epoch 1 Batch 30900 Loss 0.0077\n",
            "Saved checkpoint for step 31003: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-313\n",
            "loss 0.12\n",
            "Epoch 1 Batch 31000 Loss 0.0085\n",
            "Saved checkpoint for step 31103: /content/drive/MyDrive/AI_COLAB/Viettel/training_checkpoints/ckpt-314\n",
            "loss 0.13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-52f26271645e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \"ParameterServerStrategy and CentralStorageStrategy\")\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m       \u001b[0mapply_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mreduced_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0mapply_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_prepare_local\u001b[0;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mlocal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare_local\u001b[0;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"learning_rate\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m       \u001b[0mlr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decayed_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m       \u001b[0mapply_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr_t\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3987\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3988\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Identity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3989\u001b[0;31m         tld.op_callbacks, input)\n\u001b[0m\u001b[1;32m   3990\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3991\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvHxMd2AHHCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9bb331-cd47-4bad-defe-e9d330deb105"
      },
      "source": [
        "(next(iterator)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 72), dtype=int32, numpy=\n",
              "array([[ 1, 23, 28, ...,  0,  0,  0],\n",
              "       [ 1, 23, 23, ...,  0,  0,  0],\n",
              "       [ 1, 23, 40, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 1, 23, 40, ...,  0,  0,  0],\n",
              "       [ 1, 23, 23, ...,  0,  0,  0],\n",
              "       [ 1, 23, 23, ...,  0,  0,  0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHmCTA8PHHad"
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # inputs = [inp_lang.word2idx[i] for i in sentence]\n",
        "    inputs = [inp_lang.word2idx['<start>']]\n",
        "    for s in sentence:\n",
        "      inputs.append(inp_lang.word2idx[s])\n",
        "    inputs.append(inp_lang.word2idx['<end>'])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(len(sentence.split(' '))):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weigths to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        print(predictions.shape)\n",
        "        arr = np.array(predictions[0])\n",
        "        print(arr[1725])\n",
        "        a = arr.argsort()[-10:][::-1]\n",
        "        print(a,arr[a])\n",
        "        # a = tf.make_ndarray(predictions)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHs2RCWMHK_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5d94ec-33a4-4489-f587-39bbd22c6d4a"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f31d37d9a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbOq_D5mHT4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc66cb8b-b71e-4038-bb87-5cbf21f9c469"
      },
      "source": [
        "translate(unfold_vnese('Traanf hueng dao'), encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after embeding (1, 72, 256)\n",
            "(1, 72, 1024)\n",
            "(1, 1024)\n",
            "(1, 2541)\n",
            "-1.23448\n",
            "[2485 2393 1308 2100 1632 1398 1526 1861 1409 1369] [20.935604 18.038221 12.926941 12.070888 11.143383 10.232903  9.701294\n",
            "  8.755151  8.75013   8.615571]\n",
            "(1, 2541)\n",
            "-0.5791761\n",
            "[1529 1049 1856 1436 1533 1051  965 1887 1070 1403] [17.338915  14.424824  13.287314  13.0891    13.024799  12.601821\n",
            " 11.8564625 11.616872  11.506042  11.209258 ]\n",
            "(1, 2541)\n",
            "-2.1797726\n",
            "[1258    2  919 1710  867 2405    5 1324 1718 1721] [20.580462 14.682459 14.279002 12.017232 11.483554 11.420092 11.229128\n",
            " 11.067651 10.968297 10.799262]\n",
            "Input: traanf hueng dao\n",
            "Predicted translation: đĩnh liên ghe \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4K5p8r0pRKA",
        "outputId": "5ef20911-d1c0-44d9-f89c-7fe3bc884568"
      },
      "source": [
        "print(targ_lang.word2idx['trần'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VeGBgQRS5oqx",
        "outputId": "99faca60-5f56-48b4-bf71-2e160372ecef"
      },
      "source": [
        "targ_lang.idx2word[2485]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'đĩnh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfL2JUwKPHj"
      },
      "source": [
        "def unfold_vnese(token):\n",
        "    token = token.lower()\n",
        "    accent_dictionary = {\n",
        "    'á':'as', 'à':'af', 'ả':'ar', 'ã':'ax', 'ạ':'aj', 'â':'aa', 'ấ':'aas', 'ầ':'aaf', 'ẩ':'aar', 'ẫ':'aax', 'ậ':'aaj', 'ă':'aw', 'ắ':'aws', 'ằ':'awf', 'ẳ':'awr', 'ẵ':'awx', 'ặ':'awj',\n",
        "    'ó':'os', 'ò':'of', 'ỏ':'or', 'õ':'ox', 'ọ':'oj', 'ô':'oo', 'ố':'oos', 'ồ':'oof', 'ổ':'oor', 'ỗ':'oox', 'ộ':'ooj', 'ơ':'ow', 'ớ':'ows', 'ờ':'owf', 'ở':'owr', 'ỡ':'owx', 'ợ':'owj',\n",
        "    'é':'es', 'è':'ef', 'ẻ':'er', 'ẽ':'ex', 'ẹ':'ej', 'ê':'ee', 'ế':'ees', 'ề':'eef', 'ể':'eer', 'ễ':'eex', 'ệ':'eej',\n",
        "    'ú':'us', 'ù':'uf', 'ủ':'ur', 'ũ':'ux', 'ụ':'uj', 'ư':'uw', 'ứ':'uws', 'ừ':'uwf', 'ử':'uwr', 'ữ':'uwx', 'ự':'uwj',\n",
        "    'í':'is', 'ì':'if', 'ỉ':'ir', 'ĩ':'ix', 'ị':'ij',\n",
        "    'ý':'ys', 'ỳ':'yf', 'ỷ':'yr', 'ỹ':'yx', 'ỵ':'yj',\n",
        "    'đ':'dd'\n",
        "    }\n",
        "    posible = []\n",
        "    unfold = ''\n",
        "    accent = []\n",
        "    for character in token :\n",
        "        if character in accent_dictionary.keys():\n",
        "            unfold += accent_dictionary[character][0]\n",
        "            for char in accent_dictionary[character][1:]:\n",
        "                if char not in accent:\n",
        "                    accent.append(char)\n",
        "        else:\n",
        "            unfold += character\n",
        "    print(accent)\n",
        "    unfold = unfold + ''.join(x for x in accent)\n",
        "    posible.append(unfold)\n",
        "    \n",
        "    unfold = ''\n",
        "    for character in token :\n",
        "        if character in accent_dictionary.keys():\n",
        "            unfold += accent_dictionary[character]\n",
        "        else:\n",
        "            unfold += character\n",
        "    if unfold not in posible:\n",
        "        posible.append(unfold)\n",
        "    return posible"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vssAGlpuKSWJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
        "\n",
        "def noise_maker(sentence, threshold):   \n",
        "    token_list = sentence.split(' ')\n",
        "    unfold_tokens = []\n",
        "    for i in token_list:\n",
        "        unfold_tokens.append(unfold_vnese(i))\n",
        "    print(unfold_tokens)\n",
        "#     1/0\n",
        "    sentences=['']\n",
        "    for tokens in unfold_tokens:\n",
        "        print(sentences)\n",
        "        for token in tokens:\n",
        "            for senten in sentences:\n",
        "                print(sentences)\n",
        "                a = senten+ ' ' + token\n",
        "                buffer.append(a)\n",
        "            sentences = buffer\n",
        "    \n",
        "    1/0\n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    replace_dict = {'q':['w'],'w':['q','e'],'e':['w','r'],'r':['e','t'],'t':['r','y'],'y':['t','u'],'u':['y','i'],'i':['u','o'],'o':['i','p'],'p':['o','['],\n",
        "                    'a':['s'],'s':['a','d'],'d':['s','f'],'f':['d','g'],'g':['f','h'],'h':['g','j'],'j':['h','k'],'k':['j','l'],'l':['k',';'],';':['l','\\''],\n",
        "                    'z':['x'],'x':['z','c'],'c':['x','v'],'v':['c','b'],'b':['v','n'],'n':['b','m'],'m':['n',',']}\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == len(sentence) - 1:\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    if sentence[i] != ' ' and sentence[i+1] != ' ':\n",
        "                        noisy_sentence.append(sentence[i+1])\n",
        "                        noisy_sentence.append(sentence[i])\n",
        "                        i += 1\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                if sentence[i] in replace_dict.keys():\n",
        "                    random_letter = np.random.choice(replace_dict[sentence[i]], 1)[0]\n",
        "                    noisy_sentence.append(random_letter)\n",
        "#                 print(random_letter)\n",
        "                else:\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                if sentence[i] == ' ':\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                else:\n",
        "                    pass     \n",
        "        i += 1\n",
        "    return ''.join(x for x in noisy_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "Ee6MaLVGKVG_",
        "outputId": "a00f57d9-487b-49ff-8d11-c0c85335a0a9"
      },
      "source": [
        "print(noise_maker('hồ chúng minh',0.9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-223-fc8fea5abd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hồ chúng minh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-222-fd9af5371322>\u001b[0m in \u001b[0;36mnoise_maker\u001b[0;34m(sentence, threshold)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msenten\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenten\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}