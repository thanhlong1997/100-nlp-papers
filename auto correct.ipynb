{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5UKXQOfqKplJKEtzsTAWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/auto%20correct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He6c59ylCBS4",
        "outputId": "145ef0dd-b327-4671-b0ec-f7d7743e2f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AueOdJqVFpFf",
        "outputId": "68533f2e-fef9-4c8c-87c6-c291618fc621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiKrMu26KyH1"
      },
      "source": [
        "with open('/content/data_noise.txt','r',encoding='utf-8') as file:\n",
        "  data_noise = file.read().split('\\n')\n",
        "with open('/content/data_true.txt','r',encoding='utf-8') as file:\n",
        "  data_true = file.read().split('\\n')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzNFcmRzFX_p"
      },
      "source": [
        "def unfold_vnese(token):\n",
        "    accent_dictionary = {\n",
        "    'á':'as', 'à':'af', 'ả':'ar', 'ã':'ax', 'ạ':'aj', 'â':'aa', 'ấ':'aas', 'ầ':'aaf', 'ẩ':'aar', 'ẫ':'aax', 'ậ':'aaj', 'ă':'aw', 'ắ':'aws', 'ằ':'awf', 'ẳ':'awr', 'ẵ':'awx', 'ặ':'awj',\n",
        "    'ó':'os', 'ò':'of', 'ỏ':'or', 'õ':'ox', 'ọ':'oj', 'ô':'oo', 'ố':'oos', 'ồ':'oof', 'ổ':'oor', 'ỗ':'oox', 'ộ':'ooj', 'ơ':'ow', 'ớ':'ows', 'ờ':'owf', 'ở':'owr', 'ỡ':'owx', 'ợ':'owj',\n",
        "    'é':'es', 'è':'ef', 'ẻ':'er', 'ẽ':'ex', 'ẹ':'ej', 'ê':'ee', 'ế':'ees', 'ề':'eef', 'ể':'eer', 'ễ':'eex', 'ệ':'eej',\n",
        "    'ú':'us', 'ù':'uf', 'ủ':'ur', 'ũ':'ux', 'ụ':'uj', 'ư':'uw', 'ứ':'uws', 'ừ':'uwf', 'ử':'uwr', 'ữ':'uwx', 'ự':'uwj',\n",
        "    'í':'is', 'ì':'if', 'ỉ':'ir', 'ĩ':'ix', 'ị':'ij',\n",
        "    'ý':'ys', 'ỳ':'yf', 'ỷ':'yr', 'ỹ':'yx', 'ỵ':'yj',\n",
        "    'đ':'dd'\n",
        "    }\n",
        "    unfold = ''\n",
        "    for character in token :\n",
        "        if character in accent_dictionary.keys():\n",
        "            unfold += accent_dictionary[character]\n",
        "        else:\n",
        "            unfold += character\n",
        "    return unfold"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNfaweKdLEa7"
      },
      "source": [
        "with open('combine_data2.txt','w',encoding='utf-8') as file:\n",
        "  for index in range(len(data_noise)):\n",
        "    file.write(data_noise[index] +'\\t' + unfold_vnese(data_true[index])+'\\n')"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5SccVjiCMsp"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5XNQMcqLxRq"
      },
      "source": [
        "path_to_file = '/content/combine_data2.txt'"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjM8ooFxCRoM"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    # w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    # w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZykxJ_TCThC"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gSE2W9CVZx"
      },
      "source": [
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase)\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    self.word2idx['<start>'] = 1\n",
        "    self.word2idx['<end>'] = 2\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7CGziudKxAW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGu7fNZXCXmc"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    inp_lang = LanguageIndex(noise for noise, true in pairs)\n",
        "    targ_lang = LanguageIndex(true for noise, true in pairs)\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Spanish sentences\n",
        "    input_tensor = []\n",
        "    target_tensor = []\n",
        "    for noise, true in pairs:\n",
        "      noise_senten_tensor = [inp_lang.word2idx['<start>']]\n",
        "      for s in noise:\n",
        "        noise_senten_tensor.append(inp_lang.word2idx[s])\n",
        "      noise_senten_tensor.append(inp_lang.word2idx['<end>'])\n",
        "\n",
        "      true_senten_tensor = [inp_lang.word2idx['<start>']]\n",
        "      for s in true:\n",
        "        true_senten_tensor.append(inp_lang.word2idx[s])\n",
        "      true_senten_tensor.append(inp_lang.word2idx['<end>'])\n",
        "\n",
        "      input_tensor.append(noise_senten_tensor)\n",
        "      target_tensor.append(true_senten_tensor)\n",
        "    # English sentences\n",
        "    \n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    print(max_length_inp)\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Rm5UOyCZtC",
        "outputId": "ed19cbf4-50ea-4dd4-c94d-f50e059d0660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_examples = 300000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqruAvRoCbUJ",
        "outputId": "e25b4320-12b9-4e63-b333-50560877c0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240000, 240000, 60000, 60000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edsbHyICCdEg"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbLmdJzTCfeG"
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')\n"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGwXLs_FChzs"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOlBUEG5Cj2T"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw3VY6XPCmE0"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWKz1_yYCn37"
      },
      "source": [
        "optimizer =  tf.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUeFf8MlCpoZ"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints_correct1'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqIRWXX7Cr4p",
        "outputId": "3e539435-eedf-4940-8fc3-bb1850b7bed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.5278\n",
            "Epoch 1 Batch 100 Loss 0.8628\n",
            "Epoch 1 Batch 200 Loss 0.5946\n",
            "Epoch 1 Batch 300 Loss 0.5006\n",
            "Epoch 1 Batch 400 Loss 0.4693\n",
            "Epoch 1 Batch 500 Loss 0.4238\n",
            "Epoch 1 Batch 600 Loss 0.3644\n",
            "Epoch 1 Batch 700 Loss 0.3417\n",
            "Epoch 1 Batch 800 Loss 0.2834\n",
            "Epoch 1 Batch 900 Loss 0.2569\n",
            "Epoch 1 Batch 1000 Loss 0.2465\n",
            "Epoch 1 Batch 1100 Loss 0.2082\n",
            "Epoch 1 Batch 1200 Loss 0.1647\n",
            "Epoch 1 Batch 1300 Loss 0.1309\n",
            "Epoch 1 Batch 1400 Loss 0.1186\n",
            "Epoch 1 Batch 1500 Loss 0.1068\n",
            "Epoch 1 Batch 1600 Loss 0.1092\n",
            "Epoch 1 Batch 1700 Loss 0.0667\n",
            "Epoch 1 Batch 1800 Loss 0.0610\n",
            "Epoch 1 Batch 1900 Loss 0.0803\n",
            "Epoch 1 Batch 2000 Loss 0.0581\n",
            "Epoch 1 Batch 2100 Loss 0.0557\n",
            "Epoch 1 Batch 2200 Loss 0.0468\n",
            "Epoch 1 Batch 2300 Loss 0.0576\n",
            "Epoch 1 Batch 2400 Loss 0.0450\n",
            "Epoch 1 Batch 2500 Loss 0.0352\n",
            "Epoch 1 Batch 2600 Loss 0.0402\n",
            "Epoch 1 Batch 2700 Loss 0.0435\n",
            "Epoch 1 Batch 2800 Loss 0.0408\n",
            "Epoch 1 Batch 2900 Loss 0.0311\n",
            "Epoch 1 Batch 3000 Loss 0.0302\n",
            "Epoch 1 Batch 3100 Loss 0.0363\n",
            "Epoch 1 Batch 3200 Loss 0.0781\n",
            "Epoch 1 Batch 3300 Loss 0.0306\n",
            "Epoch 1 Batch 3400 Loss 0.0331\n",
            "Epoch 1 Batch 3500 Loss 0.0340\n",
            "Epoch 1 Batch 3600 Loss 0.0310\n",
            "Epoch 1 Batch 3700 Loss 0.0219\n",
            "Epoch 1 Loss 0.1914\n",
            "Time taken for 1 epoch 4514.028482437134 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0224\n",
            "Epoch 2 Batch 100 Loss 0.0211\n",
            "Epoch 2 Batch 200 Loss 0.0332\n",
            "Epoch 2 Batch 300 Loss 0.0270\n",
            "Epoch 2 Batch 400 Loss 0.0173\n",
            "Epoch 2 Batch 500 Loss 0.0182\n",
            "Epoch 2 Batch 600 Loss 0.0142\n",
            "Epoch 2 Batch 700 Loss 0.0214\n",
            "Epoch 2 Batch 800 Loss 0.0195\n",
            "Epoch 2 Batch 900 Loss 0.0214\n",
            "Epoch 2 Batch 1000 Loss 0.0252\n",
            "Epoch 2 Batch 1100 Loss 0.0655\n",
            "Epoch 2 Batch 1200 Loss 0.0281\n",
            "Epoch 2 Batch 1300 Loss 0.0257\n",
            "Epoch 2 Batch 1400 Loss 0.0171\n",
            "Epoch 2 Batch 1500 Loss 0.0383\n",
            "Epoch 2 Batch 1600 Loss 0.0839\n",
            "Epoch 2 Batch 1700 Loss 0.0333\n",
            "Epoch 2 Batch 1800 Loss 0.0274\n",
            "Epoch 2 Batch 1900 Loss 0.0323\n",
            "Epoch 2 Batch 2000 Loss 0.0199\n",
            "Epoch 2 Batch 2100 Loss 0.0743\n",
            "Epoch 2 Batch 2200 Loss 0.0271\n",
            "Epoch 2 Batch 2300 Loss 0.0202\n",
            "Epoch 2 Batch 2400 Loss 0.0160\n",
            "Epoch 2 Batch 2500 Loss 0.0152\n",
            "Epoch 2 Batch 2600 Loss 0.0142\n",
            "Epoch 2 Batch 2700 Loss 0.0189\n",
            "Epoch 2 Batch 2800 Loss 0.0132\n",
            "Epoch 2 Batch 2900 Loss 0.0177\n",
            "Epoch 2 Batch 3000 Loss 0.0300\n",
            "Epoch 2 Batch 3100 Loss 0.0798\n",
            "Epoch 2 Batch 3200 Loss 0.0305\n",
            "Epoch 2 Batch 3300 Loss 0.0283\n",
            "Epoch 2 Batch 3400 Loss 0.0322\n",
            "Epoch 2 Batch 3500 Loss 0.0368\n",
            "Epoch 2 Batch 3600 Loss 0.0286\n",
            "Epoch 2 Batch 3700 Loss 0.0202\n",
            "Epoch 2 Loss 0.0343\n",
            "Time taken for 1 epoch 4535.269375324249 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0143\n",
            "Epoch 3 Batch 100 Loss 0.0201\n",
            "Epoch 3 Batch 200 Loss 0.0170\n",
            "Epoch 3 Batch 300 Loss 0.0201\n",
            "Epoch 3 Batch 400 Loss 0.0187\n",
            "Epoch 3 Batch 500 Loss 0.0125\n",
            "Epoch 3 Batch 600 Loss 0.0405\n",
            "Epoch 3 Batch 700 Loss 0.0387\n",
            "Epoch 3 Batch 800 Loss 0.0230\n",
            "Epoch 3 Batch 900 Loss 0.0214\n",
            "Epoch 3 Batch 1000 Loss 0.0237\n",
            "Epoch 3 Batch 1100 Loss 0.0206\n",
            "Epoch 3 Batch 1200 Loss 0.1545\n",
            "Epoch 3 Batch 1300 Loss 0.1034\n",
            "Epoch 3 Batch 1400 Loss 0.0359\n",
            "Epoch 3 Batch 1500 Loss 0.0432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvHxMd2AHHCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHmCTA8PHHad"
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # inputs = [inp_lang.word2idx[i] for i in sentence]\n",
        "    inputs = [inp_lang.word2idx['<start>']]\n",
        "    for s in sentence:\n",
        "      inputs.append(inp_lang.word2idx[s])\n",
        "    inputs.append(inp_lang.word2idx['<end>'])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "        \n",
        "        # storing the attention weigths to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] \n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    \n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHs2RCWMHK_i",
        "outputId": "891883a0-2859-4388-ccbc-f381368dd660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8b79875e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbOq_D5mHT4u",
        "outputId": "833fed7b-9a13-4226-cbd4-2296eb0d410d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "translate(u'hoof chid minh', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: hoof chid minh\n",
            "Predicted translation: ph minh ng ng ng ng ng ng ng ng ng ng ng ng ng \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}