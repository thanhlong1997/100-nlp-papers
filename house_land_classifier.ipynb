{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRHZ/E7UQObwKc7NlXpZvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhlong1997/100-nlp-papers/blob/master/house_land_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz79TpKNA6Ej",
        "outputId": "74d42f09-1ce8-49b4-e84f-78237e3ab9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-13 08:11:37--  https://s3.amazonaws.com/models.huggingface.co/bert/vinai/phobert-base/vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.141.206\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.141.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895321 (874K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 874.34K   966KB/s    in 0.9s    \n",
            "\n",
            "2020-10-13 08:11:38 (966 KB/s) - ‘vocab.txt’ saved [895321/895321]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fFXvKt4Beo",
        "outputId": "b10b1c33-8eb6-4c51-803e-7a2642e96e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "\n",
        "    pairs = set(pairs)\n",
        "    return pairs\n",
        "print(get_pairs('Hôm_nay'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('n', 'a'), ('m', '_'), ('ô', 'm'), ('a', 'y'), ('_', 'n'), ('H', 'ô')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xgZ30tg4wMj",
        "outputId": "fef9017a-1d21-4e66-ac86-c96d87cf3596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('bpe.codes', encoding=\"utf-8\") as merges_handle:\n",
        "  merges = merges_handle.read().split(\"\\n\")[:-1]\n",
        "\n",
        "merges = [tuple(merge.split()[:-1]) for merge in merges]\n",
        "bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "cache = {}\n",
        "\n",
        "def bpe(token):\n",
        "  if token in cache:\n",
        "    return cache[token]\n",
        "  word = tuple(token)\n",
        "  word = tuple(list(word[:-1]) + [word[-1] + \"</w>\"])\n",
        "  pairs = get_pairs(word)\n",
        "\n",
        "  if not pairs:\n",
        "    return token\n",
        "\n",
        "  while True:\n",
        "    bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
        "    if bigram not in bpe_ranks:\n",
        "      break\n",
        "    first, second = bigram\n",
        "    new_word = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "      try:\n",
        "        j = word.index(first, i)\n",
        "      except ValueError:\n",
        "        new_word.extend(word[i:])\n",
        "        break\n",
        "      else:\n",
        "        new_word.extend(word[i:j])\n",
        "        i = j\n",
        "\n",
        "      if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
        "        new_word.append(first + second)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_word.append(word[i])\n",
        "        i += 1\n",
        "    new_word = tuple(new_word)\n",
        "    word = new_word\n",
        "    if len(word) == 1:\n",
        "      break\n",
        "    else:\n",
        "      pairs = get_pairs(word)\n",
        "  word = \"@@ \".join(word)\n",
        "  word = word[:-4]\n",
        "  cache[token] = word\n",
        "  return word\n",
        "\n",
        "print(bpe('Viblo'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vi@@ b@@ lo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWel1NUh7bxQ",
        "outputId": "7fd5fdd6-e8c2-4ee6-a7c1-fb7c129c478f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "def _tokenize(text):\n",
        "    \"\"\"Tokenize a string.\"\"\"\n",
        "    split_tokens = []\n",
        "\n",
        "    words = re.findall(r\"\\S+\\n?\", text)\n",
        "\n",
        "    for token in words:\n",
        "        split_tokens.extend([t for t in bpe(token).split(\" \")])\n",
        "    return split_tokens\n",
        "\n",
        "print(_tokenize('Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hôm_nay', 'trời', 'nóng', 'quá', 'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6acVhfz9AZrF"
      },
      "source": [
        "encoder = {}\n",
        "bos_token=\"<s>\"\n",
        "eos_token=\"</s>\"\n",
        "sep_token=\"</s>\"\n",
        "cls_token=\"<s>\"\n",
        "unk_token=\"<unk>\"\n",
        "pad_token=\"<pad>\"\n",
        "mask_token=\"<mask>\"\n",
        "\n",
        "\n",
        "encoder[bos_token] = 0\n",
        "encoder[pad_token] = 1\n",
        "encoder[eos_token] = 2\n",
        "encoder[unk_token] = 3\n",
        "\n",
        "def add_from_file(f):\n",
        "  if isinstance(f, str):\n",
        "      try:\n",
        "          with open(f, \"r\", encoding=\"utf-8\") as fd:\n",
        "              add_from_file(fd)\n",
        "      except FileNotFoundError as fnfe:\n",
        "          raise fnfe\n",
        "      except UnicodeError:\n",
        "          raise Exception(\"Incorrect encoding detected in {}, please \" \"rebuild the dataset\".format(f))\n",
        "      return\n",
        "\n",
        "  lines = f.readlines()\n",
        "  for lineTmp in lines:\n",
        "      line = lineTmp.strip()\n",
        "      idx = line.rfind(\" \")\n",
        "      if idx == -1:\n",
        "          raise ValueError(\"Incorrect dictionary format, expected '<token> <cnt>'\")\n",
        "      word = line[:idx]\n",
        "      encoder[word] = len(encoder)\n",
        "\n",
        "add_from_file('vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcz9F29_Sgz",
        "outputId": "01213f3f-7b26-448a-c224-615aa34ab9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def _convert_token_to_id(token):\n",
        "    \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "    return encoder.get(token, encoder.get(unk_token))\n",
        "print(_convert_token_to_id('Hôm_nay'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59YJ4MJF3xu",
        "outputId": "cc5b51c1-a994-4691-8924-4bbd31f36368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = {v: k for k, v in encoder.items()}\n",
        "def _convert_id_to_token(index):\n",
        "    return decoder.get(index, unk_token)\n",
        "\n",
        "print(_convert_id_to_token(7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "của\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSccuIU7rn3",
        "outputId": "c47204ad-4001-4c85-9cfc-4a07bb27f884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def convert_tokens_to_string(tokens):\n",
        "    out_string = \" \".join(tokens)\n",
        "    out_string = out_string.replace(\"@@ \", \"\").strip()\n",
        "    out_string = out_string.replace('@@','').strip()\n",
        "    return out_string\n",
        "  \n",
        "print(convert_tokens_to_string(['Hôm_nay', 'trời', 'nóng', 'quá' ,'nên', 'tôi', 'ở', 'nhà', 'viết', 'Vi@@', 'blo@@']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGKL3T4d0l8J"
      },
      "source": [
        "# Count feature\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZTb_7oezVdQ"
      },
      "source": [
        "!pip install pandas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av0uUbDt0tmE",
        "outputId": "0cb63b0e-805f-419d-e584-0c0f77df4f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! pip install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRIsnIO_00FB",
        "outputId": "f89734d6-71bf-40aa-83c5-fd9219eb1beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "! pip3 install vncorenlp! mkdir -p vncorenlp/models/wordsegmenter\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "! mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "! mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "! mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645934 sha256=853e84916a1da86ee07453e003a770fd50053ba1a4851d5457e0b2eb0897434c\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhN1qnIx08Og",
        "outputId": "046b2cf1-aa9f-41b8-8520-436e666c1ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "! mkdir -p vncorenlp/models/wordsegmenter\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "! wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "! mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "! mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "! mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-14 01:57:42--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  36.2MB/s    in 0.7s    \n",
            "\n",
            "2020-10-14 01:57:51 (36.2 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2020-10-14 01:57:51--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-10-14 01:57:52 (7.35 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2020-10-14 01:57:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-14 01:57:53 (3.09 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjjCGJ5I1Y41"
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "import re\n",
        "\n",
        "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "def document_2_list_token(documents):\n",
        "  vocab = {}\n",
        "  for document in documents:\n",
        "    tokenizer = rdrsegmenter.tokenize(document)\n",
        "    for sentence in tokenizer:\n",
        "      for token in sentence:\n",
        "        token = token.lower()\n",
        "        if not re.search('[a-zA-Z]',token):\n",
        "          continue\n",
        "        if token != '':\n",
        "          if token not in vocab:\n",
        "            vocab[token] = 1\n",
        "          else:\n",
        "            vocab[token] += 1\n",
        "  return vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cbSUDkxzZdM",
        "outputId": "a12f01a1-f6b9-4e87-b24e-4e10cc09d309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from itertools import islice\n",
        "\n",
        "dfs = pd.read_excel('data_classifier.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "data_land = list(dfs['land'])\n",
        "data_house = list(dfs['house'])\n",
        "\n",
        "for index1 in range(len(data_land)):\n",
        "  if isinstance(data_land[index1],float):\n",
        "    break\n",
        "for index2 in range(len(data_house)):\n",
        "  if isinstance(data_house[index2],float):\n",
        "    break\n",
        "\n",
        "data_land = data_land[:index1]\n",
        "count_feature_land = document_2_list_token(data_land)\n",
        "count_feature_land = {k: v for k, v in sorted(count_feature_land.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "data_house = data_house[:index2]\n",
        "count_feature_house = document_2_list_token(data_house)\n",
        "count_feature_house = {k: v for k, v in sorted(count_feature_house.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "print(take(20,count_feature_land.items()))\n",
        "print(take(20,count_feature_house.items()))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('có', 132), ('đất', 129), ('nhà', 56), ('cách', 49), ('đường', 46), ('bán', 44), ('hà_nội', 33), ('giá', 33), ('rất', 33), ('liên_hệ', 31), ('cấp', 30), ('trong', 30), ('và', 28), ('lô', 27), ('lại', 27), ('là', 27), ('khu', 26), ('sổ_đỏ', 26), ('nhà_đất', 24), ('nhu_cầu', 24), ('cần', 23), ('chủ', 23), ('diện_tích', 22), ('chính', 22), ('vuông_vắn', 22), ('làm', 21), ('xã', 20), ('còn', 19), ('được', 19), ('không', 18), ('dt', 18), ('đẹp', 18), ('vị_trí', 17), ('gần', 17), ('chỉ', 17), ('sử_dụng', 17), ('nhiều', 17), ('tư_vấn', 17), ('vườn', 16), ('lh', 16), ('rộng', 15), ('an_ninh', 15), ('khoảng', 15), ('tỷ', 15), ('ô_tô', 15), ('cho', 15), ('giao_thông', 15), ('thêm', 15), ('mảnh', 14), ('đầy_đủ', 14)]\n",
            "[('nhà', 246), ('tầng', 212), ('phòng', 199), ('có', 95), ('gần', 80), ('giá', 78), ('ngủ', 78), ('và', 77), ('tỷ', 77), ('chính', 75), ('cách', 75), ('bán', 74), ('sổ_đỏ', 73), ('bếp', 68), ('chủ', 67), ('khu', 60), ('khách', 58), ('sân', 56), ('đường', 56), ('xây', 51), ('cửa', 50), ('diện_tích', 47), ('pháp_lý', 46), ('chỉ', 45), ('thiết_kế', 44), ('rộng', 43), ('phơi', 43), ('ô_tô', 42), ('liên_hệ', 40), ('thờ', 39), ('gỗ', 39), ('các', 39), ('vị_trí', 38), ('rất', 38), ('mới', 37), ('nội_thất', 37), ('ngõ', 36), ('wc', 36), ('đầy_đủ', 36), ('m', 36), ('thanh_trì', 35), ('thanh_hà', 33), ('đẹp', 31), ('trường', 31), ('đô_thị', 31), ('thoáng', 30), ('hướng', 30), ('mặt_tiền', 30), ('vệ_sinh', 30), ('thương_lượng', 30)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFERuDc77qfk"
      },
      "source": [
        "vocabulary_land = ['đất','nhà','lô','nhà_đất','vuông_vắn','vườn','mảnh','xưởng','kho']\n",
        "vocabulary_house = ['tầng','phòng','ngủ','bếp','khách','sân','cửa','thiết_kế','phơi','thờ','nội_thất','wc','hướng',',mặt_tiền','vệ_sinh']"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBRMeH-9GJK3",
        "outputId": "fd75ca6f-6c1a-4abe-cbcf-8b060dbc2930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = []\n",
        "  document = rdrsegmenter.tokenize(text)\n",
        "  for senten in document:\n",
        "    for token in senten:\n",
        "      tokens.append(token.lower())\n",
        "  return tokens\n",
        "\n",
        "def transform_document_to_vec(text):\n",
        "  vocab = vocabulary_land\n",
        "  for item in vocabulary_house:\n",
        "    if item not in vocab:\n",
        "      vocab.append(item)\n",
        "  \n",
        "  text = text.lower()\n",
        "  vectorizer = CountVectorizer(tokenizer=tokenize,vocabulary=vocab,binary=True)\n",
        "  # print(vectorizer.build_tokenizer())\n",
        "  # 1/0\n",
        "  X = vectorizer.fit_transform([text]).toarray()\n",
        "  return X\n",
        "\n",
        "\n",
        "def convert_labels(y, C = 2):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y.T\n",
        "\n",
        "# a = transform_document_to_vec('Bán nhà 40m2x 3 tầng mới đường oto đỗ cửa giá 1.38 tỷ có TL ... Hướng : Đông Nam ... Diện tích : 40m2 nở hậu ... Vị trí : Nhà có vị trí cực thuận tiện gần trường học các cấp , công an phường , quân đội , nhà văn hóa . Nhà gần bãi gửi xe ngày đêm , đường trước nhà rộng 3 cực thoáng trong dãy phân lô cao tầng ... + Nhà gần trục đường 6 tuyến đi Hà Đông , Ngã Tư Sở , Mỹ Đình hay tuyến Chương Mỹ , Hòa Bình ... + Nhà gần nhiều dự án trọng đểm của quận Hà Đông là khu đang phát triển rất mạnh .. ... + Khu dân cư đông , an ninh tốt , hàng xóm thân thiện .. ... + Nhà gần điểm xe bus cách đường quốc lộ 6 chỉ 500m ... + Với tài chính vừa phải bạn sở hữu căn nhà lý tưởng ... Thiết kế : Theo kiến trúc tân cổ điển , rộng , thoáng , tầng 2 phòng ngủ ... + Tâng 1 : Phòng khách , để xe , bếp ăn riêng , có sân sau ... + Tầng 2 : 2 phòng ngủ rộng , 1 wc ... + Tầng 3 : 1 phòng ngủ , 1 phòng thờ , sân phơi rộng ... + Nhà có sân trước 3m , sân sau 1,5m các phòng đều có cửa sổ rộng , không khí lưu thông ... Nội thất : Hoàn thiện đầy đủ nội thất cơ bản , nhận nhà ngay ... Pháp lý : Giấy phép xây dựng riêng ... Sổ đỏ chính chủ ... Giá : 1.38 tỷ làm việc trực tiếp , bao sang tên ... Liên hệ : Tuấn Anh 0974322298 đi xem miễn phí ... Hỗ trợ thủ tục vay vốn ngân hàng 70% giá trị với lãi xuất thấp , thủ tục nhanh ... Thông tin pháp lý : Sổ đỏ chính chủ.')\n",
        "# print(a)\n",
        "Feature_1 = []\n",
        "label = []\n",
        "for document in data_house:\n",
        "  Feature_1.extend(transform_document_to_vec(document))\n",
        "  label.append(0)\n",
        "for document in data_land:\n",
        "  Feature_1.extend(transform_document_to_vec(document))\n",
        "  label.append(1)\n",
        "Feature_1 = np.asarray(Feature_1)\n",
        "Label     = np.asarray(label)\n",
        "\n",
        "Feature1_train, Feature1_test, y_train, y_test = train_test_split(\n",
        "     Feature_1, Label, test_size=0.33, random_state=42)\n",
        "y_train =  convert_labels(y_train)\n",
        "y_test  =  convert_labels(y_test)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sUkwYqEHm0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktjm5dwVTF7r"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Parameter\n",
        "learning_rate = 0.01\n",
        "epoch = 100\n",
        "batch_size = 128\n",
        "display_step =5\n",
        "\n",
        "#Network Parameters\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 64\n",
        "dimention_1 = 24 # dimention of vocabulary\n",
        "dimention_2 = 8 # dimention of entities\n",
        "connect_dimention = 16 # dimention connect\n",
        "num_class = 2\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "X_1 = tf.placeholder('float',[None,dimention_1])\n",
        "X_2 = tf.placeholder('float',[None,dimention_2])\n",
        "Y   = tf.placeholder('float',[None,num_class])"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vzDYHFE4HON"
      },
      "source": [
        "# Store layers weight and bias\n",
        "\n",
        "weights = {\n",
        "    'concat_1': tf.Variable(tf.random_normal([dimention_1,connect_dimention]),name='concat_1'),\n",
        "    'concat_2': tf.Variable(tf.random_normal([dimention_2,connect_dimention]),name='concat_2'),\n",
        "    'h_1'     : tf.Variable(tf.random_normal([connect_dimention,n_hidden_1]),name='h_1'),\n",
        "    'h_2'     : tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2]),name='h_2'),\n",
        "    'out'     : tf.Variable(tf.random_normal([n_hidden_2,num_class]),name='out')\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b_1' : tf.Variable(tf.random_normal([n_hidden_1]),name='b_1'),\n",
        "    'b_2' : tf.Variable(tf.random_normal([n_hidden_2]),name='b_2'),\n",
        "    'out' : tf.Variable(tf.random_normal([num_class]),name='out')\n",
        "}"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okTxSF0s-JRE"
      },
      "source": [
        "# Create model\n",
        "def neural_net(x1,x2):\n",
        "  # Concat Feature\n",
        "  concat  = tf.add(tf.matmul(x1,weights['concat_1']),tf.matmul(x2,weights['concat_2']))\n",
        "  # hidden layer 1\n",
        "  layer_1 = tf.add(tf.matmul(concat,weights['h_1']),biases['b_1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  # hidden layer 2\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weights['h_2']),biases['b_2'])\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  # out layer\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weights['out']),biases['out'])\n",
        "  return out_layer"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TSyMOgp_v-i"
      },
      "source": [
        "logit = neural_net(X_1, X_2)\n",
        "\n",
        "# loss \n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits = logit, labels = Y\n",
        "))"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9qDsefjAuLC",
        "outputId": "ce8fed4d-5260-4555-bccd-e85320cf2300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "def training(op):\n",
        "  if op == 'Adam':\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  elif op == 'RMSprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "  elif op == 'Momentum':\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "  correct_pred = tf.equal(tf.argmax(logit, 1),tf.argmax(Y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pred , tf.float32))\n",
        "  loss_list = np.zeros(epoch)\n",
        "  init = tf.global_variables_initializer()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for step in range(epoch):\n",
        "      batch_x, batch_y = Feature1_train , y_train\n",
        "      sess.run(train_op, feed_dict={X_1: batch_x,\n",
        "                                    X_2:np.zeros([batch_x.shape[0],dimention_2]),\n",
        "                                    Y :batch_y})\n",
        "      loss , acc = sess.run([loss_op, accuracy],feed_dict={X_1: batch_x,\n",
        "                                                           X_2:np.zeros([batch_x.shape[0],dimention_2]),\n",
        "                                                           Y :batch_y})\n",
        "      loss_list[step] = loss\n",
        "      if step % display_step == 0 or step == 1:\n",
        "        print('Step '+str(step)+' batch loss = '+ '{:.4f}'.format(loss) + ' trainning acc ='+\n",
        "              '{:.3f}'.format(acc))\n",
        "        save_path = saver.save(sess, \"/content/model/model\" + str(step)+ \".ckpt\")\n",
        "        print(\"Model saved in path: %s\" % save_path)\n",
        "      \n",
        "    print('Testing Acc:',sess.run(accuracy,feed_dict={X_1: Feature1_test,\n",
        "                                                      X_2:np.zeros([Feature1_test.shape[0],dimention_2]),\n",
        "                                                      Y  :y_test}))     \n",
        "  return loss_list\n",
        "\n",
        "adam = training('Adam') "
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 batch loss = 78.6708 trainning acc =0.731\n",
            "Model saved in path: /content/model/model0.ckpt\n",
            "Step 1 batch loss = 66.4416 trainning acc =0.628\n",
            "Model saved in path: /content/model/model1.ckpt\n",
            "Step 5 batch loss = 3.8735 trainning acc =0.974\n",
            "Model saved in path: /content/model/model5.ckpt\n",
            "Step 10 batch loss = 0.7315 trainning acc =0.987\n",
            "Model saved in path: /content/model/model10.ckpt\n",
            "Step 15 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model15.ckpt\n",
            "Step 20 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model20.ckpt\n",
            "Step 25 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model25.ckpt\n",
            "Step 30 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model30.ckpt\n",
            "Step 35 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model35.ckpt\n",
            "Step 40 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model40.ckpt\n",
            "Step 45 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model45.ckpt\n",
            "Step 50 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model50.ckpt\n",
            "Step 55 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model55.ckpt\n",
            "Step 60 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model60.ckpt\n",
            "Step 65 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model65.ckpt\n",
            "Step 70 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model70.ckpt\n",
            "Step 75 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model75.ckpt\n",
            "Step 80 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model80.ckpt\n",
            "Step 85 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model85.ckpt\n",
            "Step 90 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model90.ckpt\n",
            "Step 95 batch loss = 0.0000 trainning acc =1.000\n",
            "Model saved in path: /content/model/model95.ckpt\n",
            "Testing Acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYJsj6EgjCWh"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Create some variables.\n",
        "v1 = tf.get_variable(\"v1\", shape=[3])\n",
        "v2 = tf.get_variable(\"v2\", shape=[5])\n",
        "\n",
        "# Add ops to save and restore all the variables.\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Later, launch the model, use the saver to restore variables from disk, and\n",
        "# do some work with the model.\n",
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  saver.restore(sess, \"/tmp/model.ckpt\")\n",
        "  print(\"Model restored.\")\n",
        "  # Check the values of the variables\n",
        "  print(\"v1 : %s\" % v1.eval())\n",
        "  print(\"v2 : %s\" % v2.eval())"
      ],
      "execution_count": 146,
      "outputs": []
    }
  ]
}